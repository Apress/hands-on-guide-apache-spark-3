{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e91184",
   "metadata": {},
   "source": [
    "**Chapter 6. Spark Streaming.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b07516",
   "metadata": {},
   "source": [
    "**1. Basic Sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822e443",
   "metadata": {},
   "source": [
    "1.1.  TCP/IP Sockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab3df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675025622426)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/29 21:53:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "23/01/29 21:53:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/29 21:53:47 WARN BlockManager: Block input-0-1675025626800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/29 21:53:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/29 21:53:47 WARN BlockManager: Block input-0-1675025627000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675025630000 ms\n",
      "-------------------------------------------\n",
      "1009,Julia,20,DNeuro,01-09-2022\n",
      "1010,Javier,30,DEndo,01-09-2022\n",
      "1011,Laura,50,DGineco,01-09-2022\n",
      "1012,Nuria,10,DCardio,01-09-2022\n",
      "1013,Helena,10,DCardio,01-09-2022\n",
      "1014,Nati,10,DCardio,01-09-2022\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675025635000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/01/29 21:53:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/29 21:53:58 WARN BlockManager: Block input-0-1675025638000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/29 21:53:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/29 21:53:58 WARN BlockManager: Block input-0-1675025638200 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675025640000 ms\n",
      "-------------------------------------------\n",
      "1004,Tomás,30,DEndo,01-09-2022\n",
      "1005,Lorena,50,DGineco,01-09-2022\n",
      "1006,Pedro,10,DCardio,01-09-2022\n",
      "1007,Ester,10,DCardio,01-09-2022\n",
      "1008,Marina,10,DCardio,01-09-2022\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675025645000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675025650000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    // Create the context with a 5 seconds batch size\n",
    "    val ssc = new StreamingContext(sc, Seconds(5)) // Read the forlder every 5 seconds\n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    lines.print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c4863",
   "metadata": {},
   "source": [
    "**Chapter 6. Improving our Data Analytics with Spark Streaming Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675203472376)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 23:17:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "23/01/31 23:17:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:17:56 WARN BlockManager: Block input-0-1675203476600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/31 23:17:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:17:57 WARN BlockManager: Block input-0-1675203477600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675203480000 ms\n",
      "-------------------------------------------\n",
      "1009\n",
      "Julia\n",
      "20\n",
      "DNeuro\n",
      "01-09-2022\n",
      "1010\n",
      "Javier\n",
      "30\n",
      "DEndo\n",
      "01-09-2022\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675203485000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675203490000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675203495000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/01/31 23:18:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:18:17 WARN BlockManager: Block input-0-1675203497400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/31 23:18:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:18:18 WARN BlockManager: Block input-0-1675203497800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675203500000 ms\n",
      "-------------------------------------------\n",
      "1005\n",
      "Lorena\n",
      "50\n",
      "DGineco\n",
      "01-09-2022\n",
      "1006\n",
      "Pedro\n",
      "10\n",
      "DCardio\n",
      "01-09-2022\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675203505000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675203510000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5))\n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    lines.flatMap(_.split(\",\")).print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925839cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "-------------------------------------------\n",
      "Time: 1675204460000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "23/01/31 23:34:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:21 WARN BlockManager: Block input-0-1675204461200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/31 23:34:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:21 WARN BlockManager: Block input-0-1675204461600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675204465000 ms\n",
      "-------------------------------------------\n",
      "75\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675204470000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "23/01/31 23:34:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:31 WARN BlockManager: Block input-0-1675204471000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/31 23:34:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:32 WARN BlockManager: Block input-0-1675204472400 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675204475000 ms\n",
      "-------------------------------------------\n",
      "75\n",
      "\n",
      "23/01/31 23:34:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:40 WARN BlockManager: Block input-0-1675204479800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675204480000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "23/01/31 23:34:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:40 WARN BlockManager: Block input-0-1675204480000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675204485000 ms\n",
      "-------------------------------------------\n",
      "35\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675204490000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675204495000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "23/01/31 23:34:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:56 WARN BlockManager: Block input-0-1675204496600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/01/31 23:34:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/01/31 23:34:57 WARN BlockManager: Block input-0-1675204496800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675204500000 ms\n",
      "-------------------------------------------\n",
      "260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5))\n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    lines.flatMap(_.split(\",\")).count().print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00433467",
   "metadata": {},
   "source": [
    "* countByValue() to count the number of times each word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79629021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[2]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5)) \n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    lines.countByValue().print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10565ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675236815329)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 08:33:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "-------------------------------------------\n",
      "Time: 1675236820000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/02/01 08:33:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:33:40 WARN BlockManager: Block input-0-1675236820400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/01 08:33:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:33:41 WARN BlockManager: Block input-0-1675236821000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675236825000 ms\n",
      "-------------------------------------------\n",
      "(01-09-202,1)\n",
      "(01-09-2022,20)\n",
      "(1007,5)\n",
      "(1008,5)\n",
      "(50,4)\n",
      "(DCardio,16)\n",
      "(Lorena,4)\n",
      "(Tomás,1)\n",
      "(Marina,5)\n",
      "(Ester,5)\n",
      "...\n",
      "\n",
      "23/02/01 08:33:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:33:48 WARN BlockManager: Block input-0-1675236828600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/01 08:33:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:33:49 WARN BlockManager: Block input-0-1675236829000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675236830000 ms\n",
      "-------------------------------------------\n",
      "(01-09-202,1)\n",
      "(01-09-2022,13)\n",
      "(1007,3)\n",
      "(1008,3)\n",
      "(50,3)\n",
      "(DCardio,10)\n",
      "(Lorena,3)\n",
      "(Tomás,1)\n",
      "(Marina,3)\n",
      "(Ester,3)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675236835000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675236840000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675236845000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/02/01 08:34:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:34:06 WARN BlockManager: Block input-0-1675236846600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/01 08:34:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 08:34:07 WARN BlockManager: Block input-0-1675236846800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675236850000 ms\n",
      "-------------------------------------------\n",
      "(01-09-202,1)\n",
      "(01-09-2022,6)\n",
      "(1007,2)\n",
      "(1008,1)\n",
      "(50,2)\n",
      "(DCardio,5)\n",
      "(Lorena,2)\n",
      "(Marina,1)\n",
      "(Ester,2)\n",
      "(DGineco,2)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5)) \n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    lines.flatMap(_.split(\",\")).countByValue().print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9985d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.10.45:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675241765921)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 09:56:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "-------------------------------------------\n",
      "Time: 1675241770000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/02/01 09:56:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 09:56:14 WARN BlockManager: Block input-0-1675241774400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/01 09:56:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 09:56:14 WARN BlockManager: Block input-0-1675241774600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675241775000 ms\n",
      "-------------------------------------------\n",
      "(01-09-202,2)\n",
      "(01-09-2022,22)\n",
      "(1007,6)\n",
      "(1008,5)\n",
      "(50,5)\n",
      "(DCardio,19)\n",
      "(Lorena,5)\n",
      "(Marina,5)\n",
      "(Ester,6)\n",
      "(DGineco,5)\n",
      "...\n",
      "\n",
      "23/02/01 09:56:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 09:56:20 WARN BlockManager: Block input-0-1675241779800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675241780000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/02/01 09:56:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 09:56:20 WARN BlockManager: Block input-0-1675241780200 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675241785000 ms\n",
      "-------------------------------------------\n",
      "(01-09-2022,9)\n",
      "(1007,3)\n",
      "(1008,2)\n",
      "(50,1)\n",
      "(DCardio,8)\n",
      "(Lorena,1)\n",
      "(Marina,2)\n",
      "(Ester,3)\n",
      "(DGineco,1)\n",
      "(Pedro,3)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241790000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241795000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241800000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241805000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241810000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241815000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675241820000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5)) \n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "\n",
    "    val words = lines.flatMap(_.split(\",\")).map(x => (x, 1)).reduceByKey(_+_)\n",
    "    words.print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da0dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675284919607)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 21:55:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is listening on port 9999 and ready...\n",
      "23/02/01 21:55:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 21:55:23 WARN BlockManager: Block input-0-1675284923000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675284925000 ms\n",
      "-------------------------------------------\n",
      "(DCardio,3)\n",
      "(DGineco,1)\n",
      "(DEndo,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675284930000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675284935000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "23/02/01 21:55:36 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 21:55:36 WARN BlockManager: Block input-0-1675284936200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/01 21:55:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/01 21:55:37 WARN BlockManager: Block input-0-1675284936800 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675284940000 ms\n",
      "-------------------------------------------\n",
      "(DCardio,3)\n",
      "(DGineco,1)\n",
      "(DEndo,1)\n",
      "(DNeuro,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675284945000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675284950000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Hands-On_Spark3_socketTextStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5)) \n",
    "\n",
    "    val lines = ssc.socketTextStream(host, port)\n",
    "    \n",
    "    printf(\"\\n Spark is listening on port 9999 and ready...\\n\")\n",
    "    \n",
    "    \n",
    "    val filterHeaders = lines.filter(!_.matches(\"[^0-9]+\"))\n",
    "    val selectedRecords = filterHeaders.map{ row =>\n",
    "        val rowArray = row.split(\",\")\n",
    "        (rowArray(3))\n",
    "    }\n",
    "    selectedRecords.map(x => (x, 1)).reduceByKey(_+_).print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831436b",
   "metadata": {},
   "source": [
    "**Chapter 6. Spark File Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 18:57:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Spark is monitoring folder /tmp/patient_streaming and ready... \n",
      "-------------------------------------------\n",
      "Time: 1675447065000 ms\n",
      "-------------------------------------------\n",
      "(DCardio,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675447070000 ms\n",
      "-------------------------------------------\n",
      "(DEndo,1)\n",
      "(DNeuro,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675447075000 ms\n",
      "-------------------------------------------\n",
      "(DGastro,1)\n",
      "(DCardio,3)\n",
      "(DGineco,1)\n",
      "(DNeuro,2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675447080000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675447085000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675447090000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import java.io.IOException\n",
    "\n",
    "val folder=\"/tmp/patient_streaming\"\n",
    "\n",
    "try{\n",
    "    val spark = SparkSession\n",
    "        .builder()\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"Hand-On-Spark3_textFileStream\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    val ssc = new StreamingContext(sc, Seconds(5))\n",
    "    \n",
    "    val lines = ssc.textFileStream(folder)\n",
    "    \n",
    "    printf(f\"\\n Spark is monitoring folder $folder%s and ready... \\n\")\n",
    "    \n",
    "    \n",
    "    val filterHeaders = lines.filter(!_.matches(\"[^0-9]+\"))\n",
    "    val selectedRecords = filterHeaders.map{ row =>\n",
    "        val rowArray = row.split(\",\")\n",
    "        (rowArray(3))\n",
    "    }\n",
    "    selectedRecords.map(x => (x, 1)).reduceByKey(_+_).print()\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "}catch {\n",
    "      case e: IOException => println(\"IOException occurred\")\n",
    "      case t: Throwable => println(\"Error receiving data\", t)\n",
    "    } finally {\n",
    "      println(\"Finally block\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ac2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945d66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba771e84",
   "metadata": {},
   "source": [
    "**Chapter 6. Spark Streaming Graceful Shutdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75842754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.14:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675631790116)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 22:16:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      " Listening and ready... \n",
      "23/02/05 22:16:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/05 22:16:34 WARN BlockManager: Block input-0-1675631794400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/05 22:16:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/05 22:16:34 WARN BlockManager: Block input-0-1675631794600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1675631795000 ms\n",
      "-------------------------------------------\n",
      "(DCardio,12)\n",
      "(DGineco,4)\n",
      "(DEndo,4)\n",
      "(DNeuro,2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631795000 ms\n",
      "-------------------------------------------\n",
      "(01-09-2022,22)\n",
      "(1007,2)\n",
      "(1008,2)\n",
      "(Laura,2)\n",
      "(Julia,2)\n",
      "(50,4)\n",
      "(Nuria,2)\n",
      "(1009,2)\n",
      "(DCardio,12)\n",
      "(Javier,2)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631800000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631800000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "Streaming in progress. Timeout...\n",
      "\n",
      " Listening and ready... \n",
      "-------------------------------------------\n",
      "Time: 1675631805000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631805000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631810000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631810000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "Streaming in progress. Timeout...\n",
      "Stopping ssc context\n",
      "23/02/05 22:16:53 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.endRead(NioSocketImpl.java:245)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:324)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:347)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:800)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:270)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:313)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:188)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:176)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:162)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:329)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:396)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/02/05 22:16:53 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/02/05 22:16:53 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.endRead(NioSocketImpl.java:245)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:324)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:347)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:800)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:270)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:313)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:188)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:176)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:162)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:329)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:396)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/02/05 22:16:53 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "-------------------------------------------\n",
      "Time: 1675631815000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631815000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631820000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1675631820000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "ssc context has been stopped!\n",
      "\n",
      " Listening and ready... \n",
      "Streaming process is not longer active...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.hadoop.conf.Configuration\n",
       "import org.apache.hadoop.fs.{FileSystem, Path}\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5ff5786f\n",
       "import spark.implicits._\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@42a949e7\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@500effb8\n",
       "host: String = localhost\n",
       "port: Int = 9999\n",
       "altFolder: String = /tmp/alt_folder\n",
       "stopFlag: Boolean = true\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@7cf7...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[3]\")\n",
    "    .appName(\"streamingGracefulShutdown\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", true)\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "val ssc = new StreamingContext(sc, Seconds(5))\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "val altFolder = \"/tmp/alt_folder\"\n",
    "var stopFlag:Boolean = false\n",
    "\n",
    "val lines = ssc.socketTextStream(host, port)\n",
    "\n",
    "val groupedRecords =lines.map(record => \n",
    "                              {\n",
    "                                  val arrayRecords=record.split(\",\")\n",
    "                                  (arrayRecords(3))\n",
    "                              }\n",
    "                             )\n",
    "groupedRecords.countByValue().print()\n",
    "\n",
    "val words = lines.flatMap(_.split(\",\"))\n",
    "val wordCounts = words.map(x => (x, 1)).reduceByKey(_+_)\n",
    "wordCounts.print()\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "val timeout = 10000 \n",
    "var wasStopped = false\n",
    "\n",
    "while (! wasStopped) {\n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "\n",
    "    wasStopped = ssc.awaitTerminationOrTimeout(timeout)\n",
    "    \n",
    "    if (wasStopped)\n",
    "        println(\"Streaming process is no longer active...\")\n",
    "    else\n",
    "        println(\"Streaming is in progress...\")\n",
    "    \n",
    "    // Check the existence of altFolder, /tmp/alt_folder\n",
    "    if (!stopFlag) {\n",
    "      val fs = FileSystem.get(new Configuration())\n",
    "      stopFlag = fs.exists(new Path(altFolder))\n",
    "    }\n",
    "    \n",
    "    if (!wasStopped && stopFlag) {\n",
    "        println(\"Stopping ssc context\")\n",
    "        ssc.stop(stopSparkContext = true, stopGracefully = true)\n",
    "        println(\"ssc context has been stopped!\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9149975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"streaming\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", true)\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "val ssc = new StreamingContext(sc, Seconds(5))\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "val altFolder = \"/tmp/alt_folder\"\n",
    "var stopFlag:Boolean = false\n",
    "\n",
    "val lines = ssc.socketTextStream(host, port)\n",
    "\n",
    "val groupedRecords =lines.map(record => \n",
    "                              {\n",
    "                                  val arrayRecords=record.split(\",\")\n",
    "                                  (arrayRecords(3))\n",
    "                              }\n",
    "                             )\n",
    "groupedRecords.countByValue().print()\n",
    "\n",
    "val words = lines.flatMap(_.split(\",\"))\n",
    "val wordCounts = words.map(x => (x, 1)).reduceByKey(_+_)\n",
    "wordCounts.print()\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "val timeout = 10000\n",
    "var isStopped = false\n",
    "\n",
    "while (! isStopped) {\n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "\n",
    "    isStopped = ssc.awaitTerminationOrTimeout(timeout)\n",
    "    \n",
    "    if (isStopped)\n",
    "        println(\"Stopped streaming. Leaving the application...\")\n",
    "    else\n",
    "        println(\"Streaming App is still running. Timeout...\")\n",
    "    \n",
    "    // Check the existence of altFolder, /tmp/alt_folder\n",
    "    if (!stopFlag) {\n",
    "      val fs = FileSystem.get(new Configuration())\n",
    "      stopFlag = fs.exists(new Path(altFolder))\n",
    "    }\n",
    "    \n",
    "    if (!isStopped && stopFlag) {\n",
    "        println(\"stopping ssc right now\")\n",
    "        ssc.stop(stopSparkContext = true, stopGracefully = true)\n",
    "        println(\"ssc is stopped!!!!!!!\")\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
