{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66851e7d-6962-470c-8987-cb4b19030039",
   "metadata": {},
   "source": [
    "#### Reading and splitting strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f03892-dad2-4afc-a72d-5b2338bb755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reading and splitting strings\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,DateType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "import scala.concurrent.duration._\n",
    "import org.apache.spark.sql.streaming._\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark\n",
    "        .readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "\n",
    "    val selectDF = PatientDS\n",
    "    .select('value cast \"string\")\n",
    "    .withColumn(\"fields\", split('value, \",\"))\n",
    "    .withColumn(\"NSS\", 'fields(0) cast \"string\")\n",
    "    .withColumn(\"Nom\", 'fields(1) cast \"string\")\n",
    "    .withColumn(\"DID\", 'fields(2) cast \"int\")\n",
    "    .withColumn(\"DNom\", 'fields(3) cast \"string\")\n",
    "    .withColumn(\"Fecha\", to_timestamp('fields(4)))\n",
    "    .select(\"DID\",\"DNom\",\"Fecha\")\n",
    "    \n",
    "    selectDF.printSchema()\n",
    "\n",
    "    val counts = selectDF\n",
    "        .groupBy(window(col(\"Fecha\"), \"10 seconds\"))\n",
    "        .count()\n",
    "    \n",
    "    counts\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .option(\"truncate\", false)\n",
    "        .option(\"numRows\", 10)\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "        .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In final block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a0da6-bfb8-472f-b182-bdd4b57a8710",
   "metadata": {},
   "source": [
    "#### Stateful operations single column. Reading and splitting Input Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f644c-b078-4ad3-aa62-50aec3437f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stateful operations single column. Reading and splitting Input Strings\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,DateType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "import scala.concurrent.duration._\n",
    "import org.apache.spark.sql.streaming._\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark\n",
    "        .readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "\n",
    "    val selectDF = PatientDS\n",
    "    .select('value cast \"string\")\n",
    "    .withColumn(\"fields\", split('value, \",\"))\n",
    "    .withColumn(\"NSS\", 'fields(0) cast \"string\")\n",
    "    .withColumn(\"Nom\", 'fields(1) cast \"string\")\n",
    "    .withColumn(\"DID\", 'fields(2) cast \"int\")\n",
    "    .withColumn(\"DNom\", 'fields(3) cast \"string\")\n",
    "    .withColumn(\"Fecha\", to_timestamp('fields(4)))\n",
    "    .select(\"DID\",\"DNom\",\"Fecha\")\n",
    "    \n",
    "    selectDF.printSchema()\n",
    "\n",
    "    val counts = selectDF\n",
    "        .groupBy(col(\"DID\"))\n",
    "        .count()\n",
    "    \n",
    "    counts.printSchema()\n",
    "    \n",
    "    counts\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .option(\"truncate\", false)\n",
    "        .option(\"numRows\", 10)\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "        .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In final block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd6c28-c01d-40fe-9f14-d04639793fd7",
   "metadata": {},
   "source": [
    "#### Stateful operations multiple columns. With JSON Format and User Defined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b77598-d836-462e-b2c1-fcab6bdf4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Stateful operations multiple columns\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "\n",
    "    //val selectDF = PatientDS.select(\"DNom\")\n",
    "    \n",
    "    val counts = PatientDS\n",
    "        .groupBy(col(\"DID\"),col(\"DNom\"))\n",
    "        .count()\n",
    "    \n",
    "\n",
    "    counts.writeStream\n",
    "      .format(\"update\")\n",
    "      .format(\"console\")\n",
    "      //.option(\"checkpointLocation\", \"/tmp/stateful\")\n",
    "      .outputMode(\"complete\")\n",
    "      .option(\"truncate\",false)\n",
    "      .option(\"newRows\",30)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92af31-5a3b-41af-bf35-58524af6e2d1",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021482c1-1dda-458e-b161-b438c55898a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Aggregations\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "    \n",
    "    val counts = PatientDS\n",
    "        .groupBy(col(\"DID\"),col(\"DNom\"))\n",
    "        .agg(count(\"*\").alias(\"countDID\"),\n",
    "             sum(\"DID\").alias(\"sumDID\"),\n",
    "             mean(\"DID\").alias(\"meanDID\"),\n",
    "             stddev(\"DID\").alias(\"stddevDID\"),\n",
    "             approx_count_distinct(\"DID\").alias(\"distinctDID\"),\n",
    "             //collect_set(\"DID\").alias(\"collect_setDID\"),\n",
    "             collect_list(\"DID\").alias(\"collect_listDID\")\n",
    "            )\n",
    "    \n",
    "\n",
    "    counts.writeStream\n",
    "      .format(\"update\")\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"complete\")\n",
    "      .option(\"truncate\",false)\n",
    "      .option(\"newRows\",30)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbefbf1-cf07-44c5-a3fe-902d7446488d",
   "metadata": {},
   "source": [
    "#### Structured Streaming with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525be981-ce2e-456b-b20b-592e1d5bc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Structured Streaming with Checkpointing\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "val checkpointDir = \"/tmp/streaming_checkpoint\"\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "    \n",
    "    val counts = PatientDS\n",
    "        .groupBy(col(\"DID\"),col(\"DNom\"))\n",
    "        .count()\n",
    "    \n",
    "    counts.writeStream\n",
    "      .format(\"update\")\n",
    "      .format(\"console\")\n",
    "      .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "      .option(\"checkpointLocation\", checkpointDir)\n",
    "      .outputMode(\"complete\")\n",
    "      .option(\"truncate\",false)\n",
    "      .option(\"newRows\",30)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb073f-c1a3-4d79-bbb2-28b468f16642",
   "metadata": {},
   "source": [
    "#### Spark Streaming Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5512-292b-4c2b-9f95-2e4192b5f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// File Sink to CSV\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "val checkpointDir = \"/tmp/streaming_checkpoint\"\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "    \n",
    "    val PatientDF = PatientDS.select(\"*\")\n",
    "    \n",
    "    PatientDF.writeStream\n",
    "      .format(\"csv\")\n",
    "      .option(\"path\", \"/tmp/streaming_output/csv\")\n",
    "      .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "      .option(\"checkpointLocation\", checkpointDir)\n",
    "      .outputMode(\"append\")\n",
    "      .option(\"truncate\",false)\n",
    "      .option(\"newRows\",30)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad379ad9-1df3-4c63-9048-565f3fe02ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "// File Sink to Parquet\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "val checkpointDir = \"/tmp/streaming_checkpoint\"\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "    \n",
    "    val PatientDF = PatientDS.select(\"*\")\n",
    "    \n",
    "    PatientDF.writeStream\n",
    "      .format(\"parquet\")\n",
    "      .option(\"path\", \"/tmp/streaming_output/parquet\")\n",
    "      .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "      .option(\"checkpointLocation\", checkpointDir)\n",
    "      .outputMode(\"append\")\n",
    "      .option(\"truncate\",false)\n",
    "      .option(\"newRows\",30)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ea524-aec4-4bb8-8ed5-d3d81e1c1b65",
   "metadata": {},
   "source": [
    "#### foreachBatch File Sink to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321a9f0-dcbb-4827-9efd-64a8b4256f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// foreachBatch File Sink to CSV\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType,DoubleType,LongType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}\n",
    "import java.io.IOException\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.{GroupState,GroupStateTimeout,OutputMode}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "val PatientsSchema = StructType(Array(\n",
    "     StructField(\"NSS\", StringType),\n",
    "     StructField(\"Nom\", StringType),\n",
    "     StructField(\"DID\", IntegerType),\n",
    "     StructField(\"DNom\", StringType),\n",
    "     StructField(\"Fecha\", StringType)\n",
    "         )\n",
    "    )\n",
    "\n",
    "case class Patient(\n",
    "    NSS: String,\n",
    "    Nom: String,\n",
    "    DID: Option[Long],\n",
    "    DNom: String,\n",
    "    Fecha: String\n",
    ")\n",
    "\n",
    "def saveToCSV = (df: DataFrame, timeStamp: Long) => {\n",
    "    df.withColumn(\"timeStamp\", date_format(current_date(),\"yyyyMMdd\"))\n",
    "    .write.format(\"csv\")\n",
    "    .option(\"path\", \"/tmp/streaming_output/foreachBatch\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    "}\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Hand-On-Spark3_Socket_Data_Source\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val host = \"localhost\"\n",
    "val port = 9999\n",
    "val checkpointDir = \"/tmp/streaming_checkpoint\"\n",
    "\n",
    "try {\n",
    "    val PatientDS = spark.readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\",host)\n",
    "        .option(\"port\",port)\n",
    "        .load()\n",
    "        .select(from_json(col(\"value\"), PatientsSchema).as(\"patient\"))\n",
    "        .selectExpr(\"Patient.*\")\n",
    "        .as[Patient]\n",
    "    \n",
    "    printf(\"\\n Listening and ready... \\n\")\n",
    "    \n",
    "    val PatientDF = PatientDS.select(\"*\")\n",
    "    \n",
    "    PatientDF.writeStream\n",
    "      .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "      .option(\"checkpointLocation\", checkpointDir)\n",
    "      .outputMode(\"append\")\n",
    "      .foreachBatch(saveToCSV)\n",
    "      .start()\n",
    "      .awaitTermination()\n",
    "    \n",
    "} catch {\n",
    "    case e: java.net.ConnectException => println(\"Error establishing connection to \" + host + \":\" + port)\n",
    "    case e: IOException => println(\"IOException occurred\")\n",
    "    case t: Throwable => println(\"Error receiving data\", t)\n",
    "}finally {\n",
    "    println(\"In finally block\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097e818-f5fa-4dfd-bece-f78fecd8d94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
