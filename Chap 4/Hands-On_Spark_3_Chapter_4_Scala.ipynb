{"cells":[{"cell_type":"code","execution_count":null,"id":"c46a114c","metadata":{"id":"c46a114c","outputId":"22629122-f12a-4441-ed0e-bea9c4d3e882"},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Spark Web UI available at http://192.168.0.14:4043\n","SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1666128146808)\n","SparkSession available as 'spark'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["import org.apache.spark.sql.SparkSession\n"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["// import org.apache.spark.sql.SparkSession"]},{"cell_type":"code","execution_count":null,"id":"851bb5c0","metadata":{"id":"851bb5c0","outputId":"4faf4968-1e05-40a5-ac87-5140d6a514b4"},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Spark Web UI available at http://192.168.0.14:4041\n","SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1670349031543)\n","SparkSession available as 'spark'\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["22/12/06 18:50:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]},{"data":{"text/plain":["import org.apache.spark.sql.SparkSession\n","spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4389c820\n"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","\n","val spark = SparkSession.builder()\n","   .appName(\"Hands-On Spark 3\")\n","   .master(\"local[*]\")\n","   .config(\"spark.memory.offHeap.enabled\",true)\n","   .config(\"spark.executor.memory\", \"4g\")\n","   .config(\"spark.driver.cores\", \"4\")\n","   .config(\"spark.executor.cores\", \"2\")\n","   .config(\"spark.driver.memory\",\"2g\")\n","   .getOrCreate()\n"]},{"cell_type":"code","execution_count":null,"id":"3ec1a2bf","metadata":{"id":"3ec1a2bf"},"outputs":[],"source":["sc.setLogLevel(\"ERROR\")"]},{"cell_type":"code","execution_count":null,"id":"47c9ba3f","metadata":{"id":"47c9ba3f"},"outputs":[],"source":["val rdd = spark.sparkContext.parallelize(\n","      List(\"Apple Orange Banana\",\"Banana Orange Kiwi\",\"Orange Mango Papaya China\")\n","    )"]},{"cell_type":"code","execution_count":null,"id":"75065c83","metadata":{"id":"75065c83","outputId":"0d98ff05-1739-445d-880b-147210295412"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Orange,1)\n","(Mango,1)\n","(Papaya,1)\n","(China,1)\n","(Banana,1)\n","(Orange,1)\n","(Kiwi,1)\n","(Apple,1)\n","(Orange,1)\n","(Banana,1)\n"]},{"data":{"text/plain":["wordsRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at flatMap at <console>:25\n","pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at <console>:26\n"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":[" val wordsRdd = rdd.flatMap(_.split(\" \"))\n"," val pairRDD = wordsRdd.map(f=>(f,1))\n"," pairRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"742480b7","metadata":{"id":"742480b7","outputId":"8c2c09df-fef9-43f4-df11-00591b750f21"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Russia,1)\n","(India,1)\n","(USA,1)\n","(Germany,1)\n","(China,1)\n","(Canada,1)\n","(Brazil,1)\n"]}],"source":["//distinct – Returns distinct keys.\n","\n","\n","pairRDD.distinct().foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"05d523e3","metadata":{"id":"05d523e3","outputId":"79ce7abf-557d-4ec9-bcf9-0617a46cdca0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sort by Key ==>\n","(India,1)\n","(India,1)\n","(India,1)\n","(Canada,1)\n","(Brazil,1)\n","(Russia,1)\n","(China,1)\n","(USA,1)\n","(Germany,1)\n","(USA,1)\n"]},{"data":{"text/plain":["sortRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at sortByKey at <console>:29\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["//sortByKey – Transformation returns an RDD after sorting by key\n","\n","val sortRDD = pairRDD.sortByKey()\n","\n","sortRDD.foreach(println)\n"]},{"cell_type":"code","execution_count":null,"id":"69291df3","metadata":{"id":"69291df3"},"outputs":[],"source":["// Example data sets\n","USD flag\tUS Dollar\tDollar\t$\tUSD\tUnited States\n","EUR flag\tEuro\tEuro\t€\tEUR\tEuropean Union\n","GBP flag\tPound sterling\tPound\t£\tGBP\tUnited Kingdom\n","AUD flag\tAustralian dollar\tDollar\tA$\tAUD\tAustralia\n","CAD flag\tCanadian dollar\tDollar\tC$\tCAD\tCanada\n","CHF flag\tSwiss franc\tFranc\tFr\tCHF\tSwitzerland\n","CNY flag\tChinese Renminbi\tYuan\t¥\tCNY\tChina\n","JPY flag\tJapanese yen\tYen\t¥\tJPY\tJapan\n","HKD flag\tHong Kong dollar\tDollar\t$\tHKD\tHong Kong SAR China\n","NZD flag\tNew Zealand dollar\tDollar\t$\tNZD\tNew Zealand\n","KRW flag\tKorean won\tWon\t₩\tKRW\tSouth Korea\n","SEK flag\tSwedish krona\tKrona\tkr\tSEK\tSweden\n","MXN flag\tMexican peso\tPeso\t$\tMXN\tMexico\n","INR flag\tIndian rupee\tRupee\t₹\tINR\tIndia\n","RUB flag\tRussian ruble\tRuble\t₽\tRUB\tRussia\n"]},{"cell_type":"code","execution_count":null,"id":"e927119b","metadata":{"id":"e927119b","outputId":"0539fdaa-2ac9-4949-88de-690579a777ca"},"outputs":[{"data":{"text/plain":["currencyListRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:25\n"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\n","val currencyListRdd = spark.sparkContext.parallelize(\n","    List(\"USD;Euro;GBP;CHF\",\"CHF;JPY;CNY;KRW\",\"CNY;KRW;Euro;USD\",\"CAD;NZD;SEK;MXN\")\n",")"]},{"cell_type":"code","execution_count":null,"id":"548562af","metadata":{"id":"548562af","outputId":"b14f0399-b188-40b0-f75d-bc901b73c6f3"},"outputs":[{"data":{"text/plain":["currenciesRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:25\n"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["val currenciesRdd = currencyListRdd.flatMap(_.split(\";\"))\n"]},{"cell_type":"code","execution_count":null,"id":"9140ce2a","metadata":{"id":"9140ce2a","outputId":"6965e4d1-73f6-41ed-895c-e943e07c43b1"},"outputs":[{"data":{"text/plain":["pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:25\n"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["val pairRDD = currenciesRdd.map(c=>(c,1))"]},{"cell_type":"code","execution_count":null,"id":"58e62db2","metadata":{"id":"58e62db2","outputId":"8bbe5274-a40e-4cdd-aca3-6878d8d29ac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(USD,1)\n","(Euro,1)\n","(GBP,1)\n","(CHF,1)\n","(CHF,1)\n","(JPY,1)\n","(CNY,1)\n","(KRW,1)\n","(CNY,1)\n","(KRW,1)\n","(Euro,1)\n","(USD,1)\n","(CAD,1)\n","(NZD,1)\n","(SEK,1)\n","(MXN,1)\n"]}],"source":["pairRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"b95eda5c","metadata":{"id":"b95eda5c","outputId":"56165685-64b5-4480-e1aa-7c4c8289fbb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["(MXN,1)\n","(GBP,1)\n","(CHF,1)\n","(CNY,1)\n","(KRW,1)\n","(SEK,1)\n","(USD,1)\n","(JPY,1)\n","(Euro,1)\n","(NZD,1)\n","(CAD,1)\n"]}],"source":["pairRDD.distinct().foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"018254f8","metadata":{"id":"018254f8","outputId":"19edbf28-1d8d-4ede-f3f1-b3f26a49fd43"},"outputs":[{"name":"stdout","output_type":"stream","text":["(KRW,1)\n","(KRW,1)\n","(CNY,1)\n","(CNY,1)\n","(GBP,1)\n","(NZD,1)\n","(JPY,1)\n","(MXN,1)\n","(Euro,1)\n","(Euro,1)\n","(SEK,1)\n","(USD,1)\n","(USD,1)\n","(CAD,1)\n","(CHF,1)\n","(CHF,1)\n"]}],"source":["//sortByKey – Transformation returns an RDD after sorting by key\n","\n","pairRDD.sortByKey().foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"a4eab4d5","metadata":{"id":"a4eab4d5","outputId":"64a2fe06-991f-461e-b4bc-1c67a8e147b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(JPY,1)\n","(GBP,1)\n","(KRW,1)\n","(KRW,1)\n","(SEK,1)\n","(USD,1)\n","(USD,1)\n","(NZD,1)\n","(CNY,1)\n","(CNY,1)\n","(MXN,1)\n","(Euro,1)\n","(Euro,1)\n","(CAD,1)\n","(CHF,1)\n","(CHF,1)\n"]},{"data":{"text/plain":["sortRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[67] at sortByKey at <console>:27\n"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["// https://sparkbyexamples.com/apache-spark-rdd/spark-sortbykey-with-rdd-example/\n","val sortRDD = pairRDD.sortByKey(true)\n","sortRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"e8356c11","metadata":{"id":"e8356c11","outputId":"2fa3e04f-8ead-4c69-cebc-67164cf7c24f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(CAD,1)\n","(CHF,1)\n","(CHF,1)\n","(CNY,1)\n","(CNY,1)\n","(Euro,1)\n","(Euro,1)\n","(GBP,1)\n","(JPY,1)\n","(KRW,1)\n"]},{"data":{"text/plain":["sortRDD: Array[(String, Int)] = Array((CAD,1), (CHF,1), (CHF,1), (CNY,1), (CNY,1), (Euro,1), (Euro,1), (GBP,1), (JPY,1), (KRW,1))\n"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["// https://stackoverflow.com/questions/23838614/how-to-sort-an-rdd-in-scala-spark\n","// rdd.takeOrdered(10)\n","\n","// In ascending order:\n","val sortRDD = pairRDD.sortByKey().takeOrdered(10)\n","sortRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"05683c6d","metadata":{"id":"05683c6d","outputId":"42580ef5-4d8e-4e49-ae54-7252d36f0c6c"},"outputs":[{"ename":"<console>","evalue":"4: error: identifier expected but ')' found.","output_type":"error","traceback":["<console>:4: error: identifier expected but ')' found.","       val sortRDD = pairRDD.sortByKey().takeOrdered(10, lambda (k, v): -v)","                                                                          ^",""]}],"source":["// In descending order:\n","\n","val sortRDD = pairRDD.sortByKey().takeOrdered(10, lambda (k, v): -v)\n","sortRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"ccd374f2","metadata":{"id":"ccd374f2","outputId":"a0b9efbe-1471-406d-a87e-7ae8cf0a8c59"},"outputs":[{"name":"stdout","output_type":"stream","text":["(CAD,1)\n","(CHF,2)\n","(GBP,1)\n","(SEK,1)\n","(JPY,1)\n","(MXN,1)\n","(Euro,2)\n","(KRW,2)\n","(CNY,2)\n","(NZD,1)\n","(USD,2)\n"]},{"data":{"text/plain":["forexCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[69] at reduceByKey at <console>:26\n"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["val forexCount = pairRDD.reduceByKey((k,v)=>k+v)\n","wordCount.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"2b2371e0","metadata":{"id":"2b2371e0","outputId":"4bf2aa66-5b6f-4ef5-d161-3af040960a51"},"outputs":[{"name":"stdout","output_type":"stream","text":["(CHF,2)\n","(Euro,2)\n","(GBP,1)\n","(KRW,2)\n","(CAD,1)\n","(NZD,1)\n","(USD,2)\n","(CNY,2)\n","(SEK,1)\n","(JPY,1)\n","(MXN,1)\n"]},{"data":{"text/plain":["forexCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[70] at reduceByKey at <console>:26\n"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["val forexCount = pairRDD.reduceByKey(_ + _)\n","wordCount.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"f1d3791d","metadata":{"id":"f1d3791d","outputId":"e4fddd74-2d73-44e8-aaf3-aaba200ab65c"},"outputs":[{"data":{"text/plain":["DonQuixoteRdd: org.apache.spark.rdd.RDD[String] = DonQuixote.txt MapPartitionsRDD[232] at textFile at <console>:25\n"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["val DonQuixoteRdd = spark.sparkContext.textFile(\"DonQuixote.txt\")"]},{"cell_type":"code","execution_count":null,"id":"19eb42d9","metadata":{"id":"19eb42d9","outputId":"e163ffb7-a535-494c-bceb-de3d9abafc0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["saddle the hack as well as handle the bill-hook. The age of this\n","In a village of La Mancha, the name of which I have no desire to call\n","gentleman of ours was bordering on fifty; he was of a hardy habit,\n","spare, gaunt-featured, a very early riser and a great sportsman. They\n","to mind, there lived not long since one of those gentlemen that keep a\n","will have it his surname was Quixada or Quesada (for here there is some\n","lance in the lance-rack, an old buckler, a lean hack, and a greyhound\n","difference of opinion among the authors who write on the subject),\n","for coursing. An olla of rather more beef than mutton, a salad on most\n","although from reasonable conjectures it seems plain that he was called\n","nights, scraps on Saturdays, lentils on Fridays, and a pigeon or so\n","Quexana. This, however, is of but little importance to our tale; it\n","extra on Sundays, made away with three-quarters of his income. The rest\n","of it went in a doublet of fine cloth and velvet breeches and shoes to\n","match for holidays, while on week-days he made a brave figure in his\n","best homespun. He had in his house a housekeeper past forty, a niece\n","will be enough not to stray a hair’s breadth from the truth in the\n","telling of it.\n","under twenty, and a lad for the field and market-place, who used to\n"]}],"source":["DonQuixoteRdd.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"ee80af38","metadata":{"id":"ee80af38","outputId":"c396bb5c-5b97-4c3c-b826-9968c6de6e35"},"outputs":[{"data":{"text/plain":["wordsDonQuixoteRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[233] at flatMap at <console>:25\n"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["val wordsDonQuixoteRdd = DonQuixoteRdd.flatMap(_.split(\" \"))"]},{"cell_type":"code","execution_count":null,"id":"b0a72b2b","metadata":{"id":"b0a72b2b","outputId":"a4d7c548-274a-4126-c8e7-9e85623e5a53"},"outputs":[{"name":"stdout","output_type":"stream","text":["saddle\n","In\n","a\n","village\n","of\n","La\n","Mancha,\n","the\n","name\n","the\n","of\n","which\n","I\n","have\n","hack\n","no\n","desire\n","as\n","well\n","to\n","call\n","as\n","handle\n","the\n","to\n","mind,\n","bill-hook.\n","The\n","there\n","lived\n","age\n","of\n","not\n","long\n","this\n","since\n","one\n","of\n","those\n","gentlemen\n","gentleman\n","of\n","that\n","keep\n","ours\n","was\n","a\n","bordering\n","lance\n","in\n","on\n","fifty;\n","he\n","the\n","lance-rack,\n","was\n","of\n","a\n","hardy\n","an\n","old\n","habit,\n","buckler,\n","a\n","spare,\n","gaunt-featured,\n","lean\n","hack,\n","a\n","very\n","and\n","a\n","early\n","riser\n","greyhound\n","and\n","a\n","great\n","sportsman.\n","They\n","for\n","will\n","have\n","coursing.\n","An\n","olla\n","of\n","rather\n","more\n","beef\n","than\n","mutton,\n","a\n","salad\n","on\n","most\n","it\n","his\n","surname\n","was\n","Quixada\n","nights,\n","scraps\n","on\n","Saturdays,\n","lentils\n","on\n","Fridays,\n","and\n","a\n","pigeon\n","or\n","so\n","extra\n","on\n","Sundays,\n","made\n","or\n","Quesada\n","(for\n","here\n","away\n","there\n","is\n","some\n","with\n","three-quarters\n","of\n","his\n","difference\n","of\n","income.\n","The\n","rest\n","of\n","opinion\n","it\n","among\n","the\n","went\n","in\n","authors\n","who\n","write\n","on\n","the\n","subject),\n","although\n","from\n","reasonable\n","conjectures\n","it\n","seems\n","plain\n","that\n","he\n","was\n","called\n","a\n","doublet\n","of\n","fine\n","cloth\n","and\n","velvet\n","breeches\n","and\n","shoes\n","to\n","Quexana.\n","This,\n","match\n","for\n","however,\n","is\n","holidays,\n","while\n","on\n","week-days\n","he\n","made\n","a\n","brave\n","figure\n","in\n","his\n","best\n","homespun.\n","of\n","but\n","little\n","importance\n","He\n","had\n","to\n","our\n","in\n","his\n","tale;\n","it\n","house\n","a\n","housekeeper\n","past\n","forty,\n","a\n","niece\n","will\n","be\n","enough\n","not\n","to\n","stray\n","a\n","hair’s\n","breadth\n","from\n","under\n","twenty,\n","and\n","the\n","truth\n","a\n","lad\n","for\n","the\n","field\n","and\n","market-place,\n","who\n","used\n","to\n","in\n","the\n","telling\n","of\n","it.\n"]}],"source":["wordsDonQuixoteRdd.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"a2472440","metadata":{"id":"a2472440","outputId":"9f312f2f-87e4-4442-db21-9fcf5665ef3e"},"outputs":[{"data":{"text/plain":["tupleDonQuixoteRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[74] at map at <console>:25\n"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["val tupleDonQuixoteRdd = wordsDonQuixoteRdd.map(w => (w,1))"]},{"cell_type":"code","execution_count":null,"id":"8c963214","metadata":{"id":"8c963214","outputId":"dfdde2d5-38dc-42d5-9373-c19ac8bb386e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(saddle,1)\n","(In,1)\n","(the,1)\n","(hack,1)\n","(as,1)\n","(well,1)\n","(as,1)\n","(handle,1)\n","(the,1)\n","(bill-hook.,1)\n","(The,1)\n","(age,1)\n","(of,1)\n","(this,1)\n","(a,1)\n","(village,1)\n","(of,1)\n","(La,1)\n","(gentleman,1)\n","(of,1)\n","(ours,1)\n","(was,1)\n","(bordering,1)\n","(on,1)\n","(fifty;,1)\n","(he,1)\n","(was,1)\n","(of,1)\n","(a,1)\n","(hardy,1)\n","(habit,,1)\n","(Mancha,,1)\n","(the,1)\n","(name,1)\n","(spare,,1)\n","(gaunt-featured,,1)\n","(a,1)\n","(very,1)\n","(early,1)\n","(riser,1)\n","(and,1)\n","(a,1)\n","(great,1)\n","(sportsman.,1)\n","(They,1)\n","(of,1)\n","(which,1)\n","(I,1)\n","(have,1)\n","(will,1)\n","(have,1)\n","(it,1)\n","(his,1)\n","(surname,1)\n","(was,1)\n","(Quixada,1)\n","(or,1)\n","(Quesada,1)\n","((for,1)\n","(here,1)\n","(there,1)\n","(is,1)\n","(some,1)\n","(no,1)\n","(desire,1)\n","(to,1)\n","(call,1)\n","(difference,1)\n","(of,1)\n","(opinion,1)\n","(to,1)\n","(mind,,1)\n","(there,1)\n","(lived,1)\n","(not,1)\n","(long,1)\n","(since,1)\n","(one,1)\n","(among,1)\n","(of,1)\n","(the,1)\n","(authors,1)\n","(those,1)\n","(gentlemen,1)\n","(who,1)\n","(write,1)\n","(that,1)\n","(keep,1)\n","(on,1)\n","(the,1)\n","(subject),,1)\n","(a,1)\n","(although,1)\n","(from,1)\n","(lance,1)\n","(in,1)\n","(the,1)\n","(lance-rack,,1)\n","(an,1)\n","(old,1)\n","(reasonable,1)\n","(conjectures,1)\n","(buckler,,1)\n","(a,1)\n","(it,1)\n","(seems,1)\n","(lean,1)\n","(hack,,1)\n","(plain,1)\n","(that,1)\n","(and,1)\n","(a,1)\n","(he,1)\n","(was,1)\n","(greyhound,1)\n","(called,1)\n","(for,1)\n","(coursing.,1)\n","(An,1)\n","(olla,1)\n","(Quexana.,1)\n","(This,,1)\n","(of,1)\n","(however,,1)\n","(is,1)\n","(of,1)\n","(rather,1)\n","(more,1)\n","(beef,1)\n","(but,1)\n","(than,1)\n","(mutton,,1)\n","(little,1)\n","(importance,1)\n","(to,1)\n","(our,1)\n","(tale;,1)\n","(it,1)\n","(a,1)\n","(salad,1)\n","(on,1)\n","(most,1)\n","(nights,,1)\n","(scraps,1)\n","(on,1)\n","(Saturdays,,1)\n","(lentils,1)\n","(will,1)\n","(be,1)\n","(enough,1)\n","(on,1)\n","(Fridays,,1)\n","(and,1)\n","(not,1)\n","(a,1)\n","(pigeon,1)\n","(or,1)\n","(to,1)\n","(stray,1)\n","(a,1)\n","(hair’s,1)\n","(breadth,1)\n","(from,1)\n","(the,1)\n","(truth,1)\n","(in,1)\n","(the,1)\n","(so,1)\n","(telling,1)\n","(of,1)\n","(it.,1)\n","(extra,1)\n","(on,1)\n","(Sundays,,1)\n","(made,1)\n","(away,1)\n","(with,1)\n","(three-quarters,1)\n","(of,1)\n","(his,1)\n","(income.,1)\n","(The,1)\n","(rest,1)\n","(of,1)\n","(it,1)\n","(went,1)\n","(in,1)\n","(a,1)\n","(doublet,1)\n","(of,1)\n","(fine,1)\n","(cloth,1)\n","(and,1)\n","(velvet,1)\n","(breeches,1)\n","(and,1)\n","(shoes,1)\n","(to,1)\n","(match,1)\n","(for,1)\n","(holidays,,1)\n","(while,1)\n","(on,1)\n","(week-days,1)\n","(he,1)\n","(made,1)\n","(a,1)\n","(brave,1)\n","(figure,1)\n","(in,1)\n","(his,1)\n","(best,1)\n","(homespun.,1)\n","(He,1)\n","(had,1)\n","(in,1)\n","(his,1)\n","(house,1)\n","(a,1)\n","(housekeeper,1)\n","(past,1)\n","(forty,,1)\n","(a,1)\n","(niece,1)\n","(under,1)\n","(twenty,,1)\n","(and,1)\n","(a,1)\n","(lad,1)\n","(for,1)\n","(the,1)\n","(field,1)\n","(and,1)\n","(market-place,,1)\n","(who,1)\n","(used,1)\n","(to,1)\n"]}],"source":["tupleDonQuixoteRdd.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"2a6eb1ba","metadata":{"id":"2a6eb1ba","outputId":"ef7b4821-3d02-49ff-a6f9-4187001f7bed"},"outputs":[{"data":{"text/plain":["reduceByKeyDonQuixoteRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[75] at reduceByKey at <console>:25\n"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["val reduceByKeyDonQuixoteRdd = tupleDonQuixoteRdd.reduceByKey((a,b)=>a+b)"]},{"cell_type":"code","execution_count":null,"id":"46458ebc","metadata":{"id":"46458ebc","outputId":"d57f748b-34c6-4358-b4ed-de5e9853aa96"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Quesada,1)\n","(went,1)\n","(under,1)\n","(call,1)\n","(this,1)\n","(hardy,1)\n","(long,1)\n","(authors,1)\n","(past,1)\n","(lean,1)\n","(Saturdays,,1)\n","(Fridays,,1)\n","(olla,1)\n","(have,2)\n","(scraps,1)\n","(plain,1)\n","(field,1)\n","(one,1)\n","(rather,1)\n","(with,1)\n","(extra,1)\n","(some,1)\n","(best,1)\n","(here,1)\n","(pigeon,1)\n","(house,1)\n","(importance,1)\n","(who,2)\n","(niece,1)\n","(buckler,,1)\n","(opinion,1)\n","(This,,1)\n","(week-days,1)\n","(They,1)\n","(coursing.,1)\n","(rest,1)\n","(gaunt-featured,,1)\n","(Mancha,,1)\n","(forty,,1)\n","(is,2)\n","(bordering,1)\n","(from,2)\n","(well,1)\n","(hair’s,1)\n","(three-quarters,1)\n","(habit,,1)\n","(little,1)\n","(his,4)\n","(market-place,,1)\n","(handle,1)\n","(among,1)\n","(housekeeper,1)\n","(fine,1)\n","(however,,1)\n","(away,1)\n","(enough,1)\n","(velvet,1)\n","(breadth,1)\n","(although,1)\n","(will,2)\n","(our,1)\n","(as,2)\n","(there,2)\n","(since,1)\n","(very,1)\n","(figure,1)\n","(cloth,1)\n","(beef,1)\n","(desire,1)\n","(so,1)\n","(brave,1)\n","(village,1)\n","(reasonable,1)\n","(mind,,1)\n","(lived,1)\n","(shoes,1)\n","(Quexana.,1)\n","(made,2)\n","(sportsman.,1)\n","(conjectures,1)\n","(it,4)\n","(The,2)\n","(than,1)\n","(lance-rack,,1)\n","(had,1)\n","(it.,1)\n","(on,7)\n","(old,1)\n","(hack,,1)\n","(truth,1)\n","(stray,1)\n","(while,1)\n","(twenty,,1)\n","(lentils,1)\n","(spare,,1)\n","(difference,1)\n","(he,3)\n","(in,5)\n","(salad,1)\n","(income.,1)\n","(tale;,1)\n","(which,1)\n","(In,1)\n","(those,1)\n","(for,3)\n","(used,1)\n","(great,1)\n","(surname,1)\n","(Sundays,,1)\n","(lance,1)\n","(riser,1)\n","(called,1)\n","(the,9)\n","(gentleman,1)\n","(ours,1)\n","(homespun.,1)\n","(greyhound,1)\n","(He,1)\n","(not,2)\n","(keep,1)\n","(doublet,1)\n","(bill-hook.,1)\n","(most,1)\n","(name,1)\n","(that,2)\n","(a,15)\n","(or,2)\n","(mutton,,1)\n","(be,1)\n","(was,4)\n","(match,1)\n","(no,1)\n","(saddle,1)\n","(I,1)\n","(to,6)\n","(breeches,1)\n","(more,1)\n","(of,13)\n","(Quixada,1)\n","(nights,,1)\n","(An,1)\n","(La,1)\n","(gentlemen,1)\n","(holidays,,1)\n","(age,1)\n","(hack,1)\n","(an,1)\n","(telling,1)\n","(but,1)\n","(and,7)\n","(subject),,1)\n","((for,1)\n","(early,1)\n","(write,1)\n","(fifty;,1)\n","(lad,1)\n","(seems,1)\n"]}],"source":["reduceByKeyDonQuixoteRdd.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"ddb410cb","metadata":{"id":"ddb410cb","outputId":"bd2b4e68-985b-4095-d036-ad705a29f781"},"outputs":[{"name":"stdout","output_type":"stream","text":["Count : 157\n"]}],"source":["println(\"Count : \"+reduceByKeyDonQuixoteRdd.count())"]},{"cell_type":"markdown","id":"8e43eaa0","metadata":{"id":"8e43eaa0"},"source":["Saving a RDD to a text file"]},{"cell_type":"code","execution_count":null,"id":"c55730af","metadata":{"id":"c55730af","outputId":"287bc9e0-39b5-44fd-e528-a0c15aa7ae47"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/09/27 03:31:27 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 971147 ms exceeds timeout 120000 ms\n","22/09/27 03:31:27 WARN SparkContext: Killing executors is not supported by current scheduler.\n"]}],"source":["reduceByKeyDonQuixoteRdd.saveAsTextFile(\"RDDDonQuixote\")"]},{"cell_type":"markdown","id":"0a3d1ad1","metadata":{"id":"0a3d1ad1"},"source":["**How does Spark aggregate function - aggregateByKey work?**\n"]},{"cell_type":"code","execution_count":null,"id":"b473b4e2","metadata":{"id":"b473b4e2","outputId":"605e7044-5ffd-4a37-b29c-3937efb1da00"},"outputs":[{"data":{"text/plain":["pairs: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[82] at parallelize at <console>:25\n"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["val pairs = sc.parallelize(Array((\"a\", 1), (\"a\", 3), (\"b\", 2), (\"a\", 5), (\"b\", 4), (\"a\", 7), (\"b\", 6)))"]},{"cell_type":"code","execution_count":null,"id":"670ca3a5","metadata":{"id":"670ca3a5","outputId":"4848f6f5-4d19-4bfa-daa5-649d27b832ec"},"outputs":[{"data":{"text/plain":["outputReduceByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[87] at reduceByKey at <console>:28\n","res49: Array[(String, Int)] = Array((a,16), (b,12))\n"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["// Now you want to \"combine\" them by key producing a sum. In this case reduceByKey and aggregateByKey are the same:\n","\n","val outputReduceByKey = pairs.reduceByKey(_ + _) //the same operation for everything\n","outputReduceByKey.collect"]},{"cell_type":"code","execution_count":null,"id":"2e39eabe","metadata":{"id":"2e39eabe","outputId":"b5b48166-7d1d-4952-ff35-0e620ae656f1"},"outputs":[{"data":{"text/plain":["outputAggregateByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[88] at aggregateByKey at <console>:27\n","res50: Array[(String, Int)] = Array((a,16), (b,12))\n"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["//0 is initial value, _+_ inside partition, _+_ between partitions\n","val outputAggregateByKey = pairs.aggregateByKey(0)(_+_,_+_)\n","outputAggregateByKey.collect"]},{"cell_type":"markdown","id":"0b9aa22e","metadata":{"id":"0b9aa22e"},"source":["Now, imagine that you want the aggregation to be a Set of the values, that is a different type that the values, that are integers (the sum of integers is also integers):"]},{"cell_type":"code","execution_count":null,"id":"24cd08b2","metadata":{"id":"24cd08b2","outputId":"95df3e1a-c65b-42ba-b167-ca02f537dc7c"},"outputs":[{"data":{"text/plain":["import scala.collection.mutable.HashSet\n","outcomeSets: org.apache.spark.rdd.RDD[(String, scala.collection.mutable.HashSet[Int])] = ShuffledRDD[89] at aggregateByKey at <console>:28\n","res52: Array[(String, scala.collection.mutable.HashSet[Int])] = Array((a,Set(1, 5, 3, 7)), (b,Set(2, 6, 4)))\n"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["import scala.collection.mutable.HashSet\n","//the initial value is a void Set. Adding an element to a set is the first\n","//_+_ Join two sets is the  _++_\n","val outcomeSets = pairs.aggregateByKey(new HashSet[Int])(_+_, _++_)\n","outcomeSets.collect"]},{"cell_type":"markdown","id":"3edf4116","metadata":{"id":"3edf4116"},"source":["pyspark.RDD.foldByKey"]},{"cell_type":"code","execution_count":null,"id":"74700153","metadata":{"id":"74700153","outputId":"7e1a6a16-095d-4ff6-be39-10b1ec51b6c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Partitions : 10\n"]}],"source":["println(\"Partitions : \"+pairs.getNumPartitions)"]},{"cell_type":"code","execution_count":null,"id":"e87d3f9a","metadata":{"id":"e87d3f9a","outputId":"aaaddd43-f6ff-4b41-ac88-18ed48c94909"},"outputs":[{"data":{"text/plain":["res56: Array[(String, Int)] = Array((a,16), (b,12))\n"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["pairs.foldByKey(0)(_+_).collect"]},{"cell_type":"code","execution_count":null,"id":"7abf1d29","metadata":{"id":"7abf1d29","outputId":"015fe465-e30b-4aec-b844-8b864f57ea77"},"outputs":[{"data":{"text/plain":["res68: Array[(String, Int)] = Array((a,20), (b,15))\n"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["pairs.foldByKey(1)(_+_).collect"]},{"cell_type":"code","execution_count":null,"id":"13c11a94","metadata":{"id":"13c11a94","outputId":"94e4f6ea-56fd-4105-ec57-faa55021c9d3"},"outputs":[{"data":{"text/plain":["res66: Array[(String, Int)] = Array((a,24), (b,18))\n"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["pairs.foldByKey(2)(_+_).collect"]},{"cell_type":"code","execution_count":null,"id":"f942c569","metadata":{"id":"f942c569"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b3b759ed","metadata":{"id":"b3b759ed","outputId":"7f0cf230-df66-4c30-b4d9-bb6be7ad031a"},"outputs":[{"ename":"<console>","evalue":"2: error: ':' expected but ')' found.","output_type":"error","traceback":["<console>:2: error: ':' expected but ')' found.","       def to_list(a):","                    ^",""]}],"source":["\n","\n","def to_list(a):\n","\n","    return [a]\n","\n","\n","def append(a, b):\n","\n","    a.append(b)\n","\n","    return a\n","\n","\n","def extend(a, b):\n","\n","    a.extend(b)\n","\n","    return a\n","\n","\n","pairs.combineByKey(to_list, append, extend).collect()"]},{"cell_type":"markdown","id":"3680c60d","metadata":{"id":"3680c60d"},"source":["Grouping of Data, groupByKey()"]},{"cell_type":"code","execution_count":null,"id":"a823b029","metadata":{"id":"a823b029","outputId":"e9a42ab6-4975-4b28-edbd-ab0e35a481e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|      value|\n","+-----------+\n","|     España|\n","|Afghanistan|\n","|    Denmark|\n","|  Guatemala|\n","| Kazakhstan|\n","+-----------+\n","\n"]},{"data":{"text/plain":["countriesDs: org.apache.spark.sql.Dataset[String] = [value: string]\n"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["val countriesDs = List(\"España\", \"Afghanistan\", \"Denmark\", \"Guatemala\", \"Kazakhstan\").toDS\n","countriesDs.show"]},{"cell_type":"code","execution_count":null,"id":"91addb4a","metadata":{"id":"91addb4a","outputId":"7a51e696-2910-4f4e-94bd-492fee19400a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+\n","|value|\n","+-----+\n","|  Esp|\n","|  Afg|\n","|  Den|\n","|  Gua|\n","|  Kaz|\n","+-----+\n","\n"]}],"source":["countriesDs.groupByKey(w => w.substring(0,3)).keys.show"]},{"cell_type":"code","execution_count":null,"id":"175d7836","metadata":{"id":"175d7836","outputId":"1aee4161-8702-4623-eb17-d8d08972692f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------+\n","|key|count(1)|\n","+---+--------+\n","|Esp|       1|\n","|Afg|       1|\n","|Den|       1|\n","|Gua|       1|\n","|Kaz|       1|\n","+---+--------+\n","\n"]}],"source":["countriesDs.groupByKey(l => l.substring(0,3)).count.show"]},{"cell_type":"code","execution_count":null,"id":"43bb0646","metadata":{"id":"43bb0646","outputId":"168670b7-2af8-46d5-c192-0164f9bc8786"},"outputs":[{"data":{"text/plain":["countriesTuples: Seq[(String, Int)] = List((España,1), (Kazakhstan,1), (Denmark,1), (España,1), (España,1), (Kazakhstan,1), (Kazakhstan,1))\n","countriesDs: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[37] at parallelize at <console>:29\n"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["// Scala collection containing tuples Key-Value pairs\n","val countriesTuples = Seq((\"España\",1),(\"Kazakhstan\",1), (\"Denmark\", 1),(\"España\",1),(\"España\",1),(\"Kazakhstan\",1),(\"Kazakhstan\",1))\n","\n","// Converting the collection to a RDD.\n","val countriesDs = spark.sparkContext.parallelize(countriesTuples)"]},{"cell_type":"code","execution_count":null,"id":"5b086d34","metadata":{"id":"5b086d34","outputId":"abf884a2-f14f-4a4d-cb70-9270e0fc6cee"},"outputs":[{"name":"stdout","output_type":"stream","text":["(España,1)\n","(Kazakhstan,1)\n","(Denmark,1)\n","(España,1)\n","(España,1)\n","(Kazakhstan,1)\n","(Kazakhstan,1)\n"]}],"source":["dataRDD.collect.foreach(println)"]},{"cell_type":"markdown","id":"7d4907a0","metadata":{"id":"7d4907a0"},"source":["After creating a pair RDD, we will collect all values corresponding to the same key using groupByKey method."]},{"cell_type":"code","execution_count":null,"id":"1ec14fe9","metadata":{"id":"1ec14fe9","outputId":"a0e0de48-ac68-48c4-b6ea-38ca35a9fb4f"},"outputs":[{"data":{"text/plain":["groupRDDByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[34] at groupByKey at <console>:26\n"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["// Applying transformation on Pair RDD.\n","val groupRDDByKey = dataRDD.groupByKey()"]},{"cell_type":"code","execution_count":null,"id":"07443753","metadata":{"id":"07443753","outputId":"cf7b10d4-8f2b-4861-e350-dfce62a8877e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(España,CompactBuffer(1, 1, 1))\n","(Kazakhstan,CompactBuffer(1, 1, 1))\n","(Denmark,CompactBuffer(1))\n","22/10/08 00:53:05 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 939936 ms exceeds timeout 120000 ms\n","22/10/08 00:53:05 WARN SparkContext: Killing executors is not supported by current scheduler.\n"]}],"source":["groupRDDByKey.collect.foreach(println)"]},{"cell_type":"markdown","id":"5861f142","metadata":{"id":"5861f142"},"source":["For example, we will sum all occurrences of an individual word. We will convert the CompactBuffer to a List as shown below.\n"]},{"cell_type":"code","execution_count":null,"id":"70c69da2","metadata":{"id":"70c69da2","outputId":"cba323bd-daa2-45ed-8dc5-e14ee337f4c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(España,3)\n","(Kazakhstan,3)\n","(Denmark,1)\n"]},{"data":{"text/plain":["countryCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[35] at map at <console>:26\n"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["\n","// Converting a CompactBuffer to a List to calculate the sum.\n","val countryCountRDD = groupRDDByKey.map(tuple => (tuple._1, tuple._2.toList.sum))\n","\n","countryCountRDD.collect.foreach(println)"]},{"cell_type":"markdown","id":"51d1f37b","metadata":{"id":"51d1f37b"},"source":["Avoid GroupByKey"]},{"cell_type":"code","execution_count":null,"id":"831b0864","metadata":{"id":"831b0864","outputId":"ec04945f-acfe-4b0a-fd45-d11cfe1ea749"},"outputs":[{"data":{"text/plain":["countriesList: List[String] = List(España, Kazakhstan, Denmark, España, España, Kazakhstan, Kazakhstan)\n","countriesDs: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[39] at parallelize at <console>:28\n"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["val countriesList = List(\"España\",\"Kazakhstan\", \"Denmark\",\"España\",\"España\",\"Kazakhstan\",\"Kazakhstan\")\n","\n","val countriesDs = spark.sparkContext.parallelize(countriesList)\n"]},{"cell_type":"code","execution_count":null,"id":"1138fe3b","metadata":{"id":"1138fe3b","outputId":"6a336666-e9fa-4fc6-b771-f2cd7f132fe6"},"outputs":[{"data":{"text/plain":["countryPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[51] at map at <console>:27\n","countryCountsWithReduce: Array[(String, Int)] = Array((España,3), (Kazakhstan,3), (Denmark,1))\n","countryCountsWithGroup: Array[(String, Int)] = Array((España,3), (Kazakhstan,3), (Denmark,1))\n"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["val countryPairsRDD = sc.parallelize(countriesList).map(country => (country, 1))\n","\n","\n","val countryCountsWithReduce = countryPairsRDD\n","  .reduceByKey(_ + _)\n","  .collect()\n","\n","val countryCountsWithGroup = countryPairsRDD\n","  .groupByKey()\n","  .map(t => (t._1, t._2.sum))\n","  .collect()"]},{"cell_type":"markdown","id":"d6cbfce6","metadata":{"id":"d6cbfce6"},"source":["Joins"]},{"cell_type":"code","execution_count":null,"id":"41a94c44","metadata":{"id":"41a94c44","outputId":"0a74399c-da49-40c2-8ecb-ebbd0adc8671"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Scala,(15,11))\n","(Scala,(15,20))\n","(PySpark,(10,75))\n","(PySpark,(10,35))\n"]},{"data":{"text/plain":["rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at parallelize at <console>:28\n","rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[87] at parallelize at <console>:29\n","joinedRDD: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[90] at join at <console>:31\n"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["\n","val rdd1 = sc.parallelize(Array((\"PySpark\",10),(\"Scala\",15),(\"R\",100)))\n","val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","val joinedRDD = rdd1.join(rdd2)\n","\n","joinedRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"de9e0930","metadata":{"id":"de9e0930","outputId":"a29bac0c-d6fe-42de-965d-08fca387bcf7"},"outputs":[{"ename":"<console>","evalue":"27: error: value innerJoin is not a member of org.apache.spark.rdd.RDD[(String, Int)]","output_type":"error","traceback":["<console>:27: error: value innerJoin is not a member of org.apache.spark.rdd.RDD[(String, Int)]","       val innerJoinedRDD = rdd1.innerJoin(rdd2)","                                 ^",""]}],"source":["val innerJoinedRDD = rdd1.innerJoin(rdd2)\n","\n","innerJoinedRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"187bcdd1","metadata":{"id":"187bcdd1","outputId":"d34318de-2ebf-4b3e-a730-f7946c8e34b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(R,(100,None))\n","(Scala,(15,Some(11)))\n","(Scala,(15,Some(20)))\n","(PySpark,(10,Some(75)))\n","(PySpark,(10,Some(35)))\n"]},{"data":{"text/plain":["leftJoinedRDD: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[96] at leftOuterJoin at <console>:26\n"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["// val rdd1 = sc.parallelize(Array((\"PySpark\",10),(\"Scala\",15),(\"R\",100)))\n","// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","val leftJoinedRDD = rdd1.leftOuterJoin(rdd2)\n","\n","leftJoinedRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"dc57ef7d","metadata":{"id":"dc57ef7d","outputId":"5835ea95-60c9-4c65-80be-6164cefed8c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(R,(Some(100),None))\n","(Scala,(Some(15),Some(11)))\n","(PySpark,(Some(10),Some(75)))\n","(PySpark,(Some(10),Some(35)))\n","(Scala,(Some(15),Some(20)))\n"]},{"data":{"text/plain":["leftJoinedRDD: org.apache.spark.rdd.RDD[(String, (Option[Int], Option[Int]))] = MapPartitionsRDD[99] at fullOuterJoin at <console>:27\n"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["val leftJoinedRDD = rdd1.fullOuterJoin(rdd2)\n","\n","leftJoinedRDD.foreach(println)"]},{"cell_type":"markdown","id":"dbde9bc2","metadata":{"id":"dbde9bc2"},"source":["**Sorting Data**"]},{"cell_type":"code","execution_count":null,"id":"71a0dca1","metadata":{"id":"71a0dca1","outputId":"9c900a8f-9123-4ffd-f689-c5f97dabbd72"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Scala,15)\n","(R,100)\n","(PySpark,10)\n"]}],"source":["// val rdd1 = sc.parallelize(Array((\"PySpark\",10),(\"Scala\",15),(\"R\",100)))\n","// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","rdd1.sortBy(_._2).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"ed1c1b74","metadata":{"id":"ed1c1b74","outputId":"a637062e-9b77-44f2-b8b3-1dedc47f8929"},"outputs":[{"name":"stdout","output_type":"stream","text":["(Scala,20)\n","(PySpark,35)\n","(Scala,11)\n","(PySpark,75)\n"]}],"source":["rdd2.sortBy(_._2).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"08ab789c","metadata":{"id":"08ab789c","outputId":"9caca924-26d3-4a13-b3ac-c21ae9c056c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(R,100)\n","(PySpark,10)\n","(Scala,15)\n"]}],"source":["// val rdd1 = sc.parallelize(Array((\"PySpark\",10),(\"Scala\",15),(\"R\",100)))\n","// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","rdd1.sortByKey(true).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"c2118f55","metadata":{"id":"c2118f55","outputId":"60f3fb56-a85e-46b7-ad59-20cbe6cf3e55"},"outputs":[{"name":"stdout","output_type":"stream","text":["(b,2)\n","(d,3)\n","(a,1)\n","(c,1)\n"]},{"data":{"text/plain":["rdd3: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[196] at parallelize at <console>:26\n"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["val rdd3 = sc.parallelize(Array((\"a\",1), (\"b\",2), (\"c\",1), (\"d\",3)) )\n","rdd3.sortBy(_._2).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"d416fe10","metadata":{"id":"d416fe10","outputId":"29245de8-cfc5-40c8-fafc-9e6a4aa15641"},"outputs":[{"name":"stdout","output_type":"stream","text":["(PySpark,10)\n","(R,100)\n","(Scala,15)\n"]}],"source":["rdd1.sortByKey(false).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"30382c1f","metadata":{"id":"30382c1f","outputId":"0966f8ab-1791-4d11-e1e9-d6c925adc746"},"outputs":[{"name":"stdout","output_type":"stream","text":["(PySpark,10)\n","(R,100)\n","(Scala,15)\n"]}],"source":["rdd1.sortBy(x => x._1).collect().foreach(println)\n","//rdd1.sortBy(x => x._2).collect().foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"8007030a","metadata":{"id":"8007030a","outputId":"2b57a05c-9757-4148-e0c8-e7fad3994619"},"outputs":[{"name":"stdout","output_type":"stream","text":["(PySpark,10)\n","(Scala,15)\n","(R,100)\n"]}],"source":["rdd1.sortBy(x => x._2).collect().foreach(println)"]},{"cell_type":"markdown","id":"c76e5226","metadata":{"id":"c76e5226"},"source":["countByKey() method\n"]},{"cell_type":"code","execution_count":null,"id":"85d1e322","metadata":{"id":"85d1e322","outputId":"b30490da-8b17-4a75-d734-0771b66a5060"},"outputs":[{"data":{"text/plain":["res48: scala.collection.Map[String,Long] = Map(PySpark -> 2, Scala -> 2)\n"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","              \n","rdd2.countByKey()"]},{"cell_type":"code","execution_count":null,"id":"49aba421","metadata":{"id":"49aba421","outputId":"e16b7c36-f620-463f-e7dd-ada1672dfce2"},"outputs":[{"name":"stdout","output_type":"stream","text":["(PySpark,2)\n","(Scala,2)\n"]}],"source":["rdd2.countByKey().foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"dddf11dc","metadata":{"id":"dddf11dc","outputId":"8fb52824-b32c-464c-80b5-ee714c834158"},"outputs":[{"name":"stdout","output_type":"stream","text":["(PySpark,2)\n","(Scala,2)\n"]},{"data":{"text/plain":["elementsCount: scala.collection.Map[String,Long] = Map(PySpark -> 2, Scala -> 2)\n"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["val elementsCount = rdd2.countByKey()\n","elementsCount.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"36214f98","metadata":{"id":"36214f98","outputId":"b560d016-74e8-40bd-d66e-1cc994f865aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]},{"data":{"text/plain":["ajay: Long = 2\n"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["// val ajay = elementsCount(\"Scala\") \n","// println(ajay)"]},{"cell_type":"code","execution_count":null,"id":"020a8164","metadata":{"id":"020a8164","outputId":"87e5d5b7-bd19-4917-8419-446abb82ae28"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["println(elementsCount(\"Scala\"))"]},{"cell_type":"code","execution_count":null,"id":"7876219a","metadata":{"id":"7876219a","outputId":"d7a5f529-dd98-431c-8bca-06ab52e06122"},"outputs":[{"ename":"java.util.NoSuchElementException","evalue":" key not found: SQL","output_type":"error","traceback":["java.util.NoSuchElementException: key not found: SQL","  at scala.collection.immutable.Map$Map2.apply(Map.scala:227)","  ... 34 elided",""]}],"source":["println(elementsCount(\"SQL\"))"]},{"cell_type":"markdown","id":"d1d0dd6c","metadata":{"id":"d1d0dd6c"},"source":["**countByValue()**\n","\n","RDD.countByValue() → Dict[K, int][source]\n","\n","* Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."]},{"cell_type":"code","execution_count":null,"id":"a8138d45","metadata":{"id":"a8138d45","outputId":"1b87bcda-532e-4cfa-ae1a-4651b2c863a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Map((PySpark,35) -> 1, (Scala,11) -> 1, (Scala,20) -> 1, (PySpark,75) -> 1)\n"]}],"source":["println(rdd2.countByValue())\n","//Output:"]},{"cell_type":"code","execution_count":null,"id":"accbb290","metadata":{"id":"accbb290"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2eca2ed1","metadata":{"id":"2eca2ed1"},"source":["**RDD collectAsMap() method**\n","* PySpark RDD's collectAsMap(~) method collects all the elements of a pair RDD in the driver node and converts the RDD into a dictionary.\n","\n","* NOTE. A pair RDD is a RDD that contains a list of tuples.\n","* Return Value. A dictionary."]},{"cell_type":"code","execution_count":null,"id":"7470e652","metadata":{"id":"7470e652","outputId":"97cba298-9779-4855-c3ab-c3f411582d66"},"outputs":[{"data":{"text/plain":["res62: scala.collection.Map[String,Int] = Map(R -> 100, Scala -> 15, PySpark -> 10)\n"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["// val rdd1 = sc.parallelize(Array((\"PySpark\",10),(\"Scala\",15),(\"R\",100)))\n","\n","rdd1.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"2677faf5","metadata":{"id":"2677faf5","outputId":"670fcb5d-ac83-42eb-b125-dd5f4dd68d9c"},"outputs":[{"data":{"text/plain":["res63: scala.collection.Map[String,Int] = Map(Scala -> 20, PySpark -> 35)\n"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","rdd2.collectAsMap() // Return (key,value) pairs as a dictionary"]},{"cell_type":"markdown","id":"cb916eb4","metadata":{"id":"cb916eb4"},"source":["**lookup(key)**\n"," \n","* It returns all values associated with the provided key.\n","rdd.lookup()\n"]},{"cell_type":"code","execution_count":null,"id":"d0462309","metadata":{"id":"d0462309","outputId":"8b55545d-ffd5-42f4-db61-77df703c05e7"},"outputs":[{"data":{"text/plain":["res66: Seq[Int] = WrappedArray(75, 35)\n"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["// val rdd2 = sc.parallelize(Array((\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)))\n","\n","rdd2.lookup(\"PySpark\")"]},{"cell_type":"code","execution_count":null,"id":"302750e2","metadata":{"id":"302750e2","outputId":"dda27035-d876-4134-93c7-99c295a68b68"},"outputs":[{"ename":"<console>","evalue":"26: error: reassignment to val","output_type":"error","traceback":["<console>:26: error: reassignment to val","       rdd3 = sc.parallelize(range(100))","            ^","<console>:27: error: value sum is not a member of org.apache.spark.rdd.RDD[(String, Int)]","       val result = rdd3.sum","                         ^",""]}],"source":["rdd3 = sc.parallelize(range(100))\n","val result = rdd3.sum\n","//rdd3.sum() //Sum of RDD elements 4950\n","println(result)"]},{"cell_type":"markdown","id":"7835c2e2","metadata":{"id":"7835c2e2"},"source":["**Broadcast variables in Spark, how and when to use them?**\n"]},{"cell_type":"code","execution_count":null,"id":"bded7f3d","metadata":{"id":"bded7f3d","outputId":"33265283-6dfd-44ef-b590-6ca80780d1ca"},"outputs":[{"data":{"text/plain":["bVariable: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(137)\n"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["val bVariable = sc.broadcast(Array(1, 2, 3, 4, 5, 6, 7, 8, 9))"]},{"cell_type":"code","execution_count":null,"id":"55fa14f8","metadata":{"id":"55fa14f8","outputId":"584ef374-f40b-454b-9506-04c565ad46e2"},"outputs":[{"data":{"text/plain":["res100: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\n"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["bVariable.value"]},{"cell_type":"markdown","id":"d67f1f1a","metadata":{"id":"d67f1f1a"},"source":["**Accumulators**"]},{"cell_type":"markdown","id":"339a752b","metadata":{"id":"339a752b"},"source":["**Accumulators**\n"]},{"cell_type":"code","execution_count":null,"id":"a328bc48","metadata":{"id":"a328bc48","outputId":"20c738e1-6146-47ce-ca25-aa2bb306ebb7"},"outputs":[{"data":{"text/plain":["acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 3707, name: Some(myAccumulator), value: 0)\n"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["val acc = sc.longAccumulator(\"myAccumulator\")"]},{"cell_type":"code","execution_count":null,"id":"72e20878","metadata":{"scrolled":true,"id":"72e20878","outputId":"2f7f580a-acfe-401e-f61f-6c60c29125a5"},"outputs":[{"ename":"java.lang.InternalError","evalue":" java.lang.IllegalAccessException: final field has no write access: $Lambda$5982/0x0000000802117450.arg$1/putField, from class java.lang.Object (module java.base)","output_type":"error","traceback":["java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$5982/0x0000000802117450.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)","  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:176)","  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)","  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)","  at java.base/java.lang.reflect.Field.set(Field.java:820)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)","  at org.apache.spark.SparkContext.clean(SparkContext.scala:2491)","  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1002)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)","  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1001)","  ... 34 elided","Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$5982/0x0000000802117450.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3494)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3485)","  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1637)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)","  ... 46 more",""]}],"source":["sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8, 9)).foreach(x => acc.add(x))"]},{"cell_type":"code","execution_count":null,"id":"e5e58265","metadata":{"id":"e5e58265","outputId":"951f8caa-1d6d-411e-a2ce-01a6eb27409a"},"outputs":[{"data":{"text/plain":["res106: Long = 0\n"]},"execution_count":181,"metadata":{},"output_type":"execute_result"}],"source":["acc.value"]},{"cell_type":"code","execution_count":null,"id":"c5f71614","metadata":{"id":"c5f71614","outputId":"6d0cb97a-f2d2-480e-cb12-38edadce4ea3"},"outputs":[{"data":{"text/plain":["accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 3709, name: Some(My Accumulator), value: 0)\n"]},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":["val accum = sc.longAccumulator(\"My Accumulator\")"]},{"cell_type":"code","execution_count":null,"id":"6ddcb2c5","metadata":{"id":"6ddcb2c5","outputId":"79d12889-8e00-41ff-e63b-90f08e2a3f28"},"outputs":[{"ename":"java.lang.InternalError","evalue":" java.lang.IllegalAccessException: final field has no write access: $Lambda$6070/0x0000000802136060.arg$1/putField, from class java.lang.Object (module java.base)","output_type":"error","traceback":["java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$6070/0x0000000802136060.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)","  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:176)","  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)","  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)","  at java.base/java.lang.reflect.Field.set(Field.java:820)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)","  at org.apache.spark.SparkContext.clean(SparkContext.scala:2491)","  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1002)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)","  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1001)","  ... 34 elided","Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$6070/0x0000000802136060.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3494)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3485)","  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1637)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)","  ... 46 more",""]}],"source":["spark.sparkContext.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))"]},{"cell_type":"code","execution_count":null,"id":"4899e29a","metadata":{"id":"4899e29a","outputId":"164db01e-e8ed-46ba-d837-580dc5cf3aee"},"outputs":[{"data":{"text/plain":["multiplier: Double = 0.25\n","dataRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[257] at parallelize at <console>:26\n"]},"execution_count":204,"metadata":{},"output_type":"execute_result"}],"source":["val multiplier = 0.25\n","val dataRDD = sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8, 9))"]},{"cell_type":"code","execution_count":null,"id":"e5c2d20d","metadata":{"id":"e5c2d20d","outputId":"e2c13c82-9202-4d7a-c164-12558564f27e"},"outputs":[{"ename":"java.lang.InternalError","evalue":" java.lang.IllegalAccessException: final field has no write access: $Lambda$6122/0x0000000802160c38.arg$1/putField, from class java.lang.Object (module java.base)","output_type":"error","traceback":["java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$6122/0x0000000802160c38.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)","  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:176)","  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)","  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)","  at java.base/java.lang.reflect.Field.set(Field.java:820)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)","  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)","  at org.apache.spark.SparkContext.clean(SparkContext.scala:2491)","  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:414)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)","  at org.apache.spark.rdd.RDD.map(RDD.scala:413)","  ... 34 elided","Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$6122/0x0000000802160c38.arg$1/putField, from class java.lang.Object (module java.base)","  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3494)","  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3485)","  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1637)","  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)","  ... 46 more",""]}],"source":["dataRDD.map(x => x * multiplier)"]},{"cell_type":"markdown","id":"64029f3c","metadata":{"id":"64029f3c"},"source":["**Datasets CHAPTER IV**"]},{"cell_type":"code","execution_count":null,"id":"12fa131b","metadata":{"id":"12fa131b","outputId":"9055d2e9-0edb-4f3a-cb59-566423a0c0ba"},"outputs":[{"data":{"text/plain":["defined class Personas\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["// case class University(name: String, numStudents: Long, yearFounded: Long)\n","\n","case class Personas(Nombre: String, Primer_Apellido: String, Segundo_Apellido: String, Edad: Int, Sexo:String)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4d3c3249","metadata":{"id":"4d3c3249","outputId":"afb256b6-05c5-4f48-907f-b67579591bf9"},"outputs":[{"ename":"<console>","evalue":"29: error: overloaded method value options with alternatives:","output_type":"error","traceback":["<console>:29: error: overloaded method value options with alternatives:","  (options: java.util.Map[String,String])org.apache.spark.sql.DataFrameReader <and>","  (options: scala.collection.Map[String,String])org.apache.spark.sql.DataFrameReader"," cannot be applied to (String, Boolean)","       val personas = spark.read.options(\"header\",true).csv(\"/Users/aantolinez/Downloads/personas.csv\").as[Personas]","                                 ^",""]}],"source":["// val schools = sqlContext.read.json(\"/schools.json\").as[University]\n","\n","val personas = spark.read.options(\"header\",true).csv(\"/Users/aantolinez/Downloads/personas.csv\").as[Personas]"]},{"cell_type":"code","execution_count":null,"id":"19c42b2c","metadata":{"id":"19c42b2c","outputId":"d3b088fc-9d96-4ef1-8dbf-a757fcda0e9d"},"outputs":[{"data":{"text/plain":["personas: org.apache.spark.sql.Dataset[Personas] = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["val personas = spark.read.format(\"com.databricks.spark.csv\")\n","    .option(\"header\", \"true\")\n","    .option(\"inferSchema\", \"true\")\n","    .load(\"/Users/aantolinez/Downloads/personas.csv\")\n","    .as[Personas]"]},{"cell_type":"code","execution_count":null,"id":"73043a8e","metadata":{"id":"73043a8e","outputId":"cff10c1c-266f-4eac-b516-37eaa5c5545c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]}],"source":["personas.show()"]},{"cell_type":"code","execution_count":null,"id":"75f7c43c","metadata":{"id":"75f7c43c","outputId":"47a6eede-d789-429c-c252-ad88a7780735"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Nombre: string (nullable = true)\n"," |-- Primer_Apellido: string (nullable = true)\n"," |-- Segundo_Apellido: string (nullable = true)\n"," |-- Edad: integer (nullable = true)\n"," |-- Sexo: string (nullable = true)\n","\n"]}],"source":["personas.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"b5f82d5d","metadata":{"id":"b5f82d5d","outputId":"0a6a8b40-2da8-4dd0-dcec-f91db398acb6"},"outputs":[{"data":{"text/plain":["df_tmp: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["val df_tmp = spark.read.format(\"com.databricks.spark.csv\")\n","    .option(\"header\", \"true\")\n","    .option(\"inferSchema\", \"true\")\n","    .load(\"/Users/aantolinez/Downloads/personas.csv\")"]},{"cell_type":"code","execution_count":null,"id":"5a63ce8b","metadata":{"id":"5a63ce8b","outputId":"e984d133-4a97-46b7-e33a-cf7c135c840e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]}],"source":["df_tmp.show()"]},{"cell_type":"code","execution_count":null,"id":"1b3549f3","metadata":{"id":"1b3549f3","outputId":"19c39305-242f-4150-f1c6-879442ab2370"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Nombre: string (nullable = true)\n"," |-- Primer_Apellido: string (nullable = true)\n"," |-- Segundo_Apellido: string (nullable = true)\n"," |-- Edad: integer (nullable = true)\n"," |-- Sexo: string (nullable = true)\n","\n"]}],"source":["df_tmp.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"e4c3b1c8","metadata":{"id":"e4c3b1c8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1114aaa7","metadata":{"id":"1114aaa7","outputId":"e4d8bc0d-c5f0-45c7-fba5-27820d64fe4a"},"outputs":[{"data":{"text/plain":["textRDD: org.apache.spark.rdd.RDD[String] = /Users/aantolinez/Downloads/personas.csv MapPartitionsRDD[55] at textFile at <console>:25\n"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["val textRDD = sc.textFile(\"/Users/aantolinez/Downloads/personas.csv\")\n"]},{"cell_type":"code","execution_count":null,"id":"bcd205de","metadata":{"id":"bcd205de","outputId":"21a1e2f1-0af0-40ce-a566-da33cbd9c80d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nombre,Primer_Apellido,Segundo_Apellido,Edad,Sexo\n","Miguel,de Cervantes,Saavedra,50,M\n","Fancisco,Quevedo,Santibáñez Villegas,55,M\n","Luis,de Góngora,y Argote,65,M\n","Teresa,Sánchez de Cepeda,y Ahumada,70,F\n"]}],"source":["textRDD.foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"b9ea4fe5","metadata":{"id":"b9ea4fe5","outputId":"59f1a19d-56b9-417a-f126-2a715223a78a"},"outputs":[{"data":{"text/plain":["empDF: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["val empDF= spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Users/aantolinez/Downloads/personas.csv\")\n"]},{"cell_type":"code","execution_count":null,"id":"77180f16","metadata":{"id":"77180f16"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bef5eb3d","metadata":{"id":"bef5eb3d","outputId":"f70ff6f9-7ea6-4577-b1ad-169f3288f4b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]}],"source":["empDF.show()"]},{"cell_type":"code","execution_count":null,"id":"106897fb","metadata":{"id":"106897fb","outputId":"b31cad98-0567-4e64-e50b-d0715245124f"},"outputs":[{"ename":"<console>","evalue":"26: error: overloaded method value createDataset with alternatives:","output_type":"error","traceback":["<console>:26: error: overloaded method value createDataset with alternatives:","  [T](data: java.util.List[T])(implicit evidence$6: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T] <and>","  [T](data: org.apache.spark.rdd.RDD[T])(implicit evidence$5: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T] <and>","  [T](data: Seq[T])(implicit evidence$4: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T]"," cannot be applied to (org.apache.spark.sql.DataFrame)","       val ds3=spark.createDataset(empDF)","                     ^",""]}],"source":["val ds3=spark.createDataset(empDF)\n","ds3.printSchema()"]},{"cell_type":"markdown","id":"3b193a2a","metadata":{"id":"3b193a2a"},"source":["**Applying lambda function on DataFrame and Dataset**"]},{"cell_type":"code","execution_count":null,"id":"18d73962","metadata":{"id":"18d73962"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fc5497a5","metadata":{"id":"fc5497a5","outputId":"8d25be64-79de-4ba8-e3ef-e08406fdf568"},"outputs":[{"data":{"text/plain":["import spark.sqlContext.implicits._\n","defined class SpanishWriters\n"]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["//import spark.sparkContext.implicits._ grants access to toDF() method\n","import spark.sqlContext.implicits._\n","\n","case class SpanishWriters(Nombre: String, Apellido: String, Edad: Int, Sexo:String)\n"]},{"cell_type":"code","execution_count":null,"id":"f9eb69c8","metadata":{"id":"f9eb69c8","outputId":"025ab7ba-78f0-43c9-8057-ebfbe72cb297"},"outputs":[{"data":{"text/plain":["SpanishWritersData: Seq[SpanishWriters] = List(SpanishWriters(Miguel,Cervantes,50,M), SpanishWriters(Fancisco,Quevedo,55,M), SpanishWriters(Luis,Góngora,65,M))\n"]},"execution_count":281,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersData = Seq(SpanishWriters(\"Miguel\", \"Cervantes\", 50, \"M\"), SpanishWriters(\"Fancisco\", \"Quevedo\", 55, \"M\"), SpanishWriters(\"Luis\", \"Góngora\", 65, \"M\"))\n"]},{"cell_type":"code","execution_count":null,"id":"661af089","metadata":{"id":"661af089","outputId":"3d73e796-1fa7-45ec-fd6f-79141066a3f1"},"outputs":[{"data":{"text/plain":["SpanishWritersRDD: org.apache.spark.rdd.RDD[SpanishWriters] = ParallelCollectionRDD[241] at parallelize at <console>:76\n"]},"execution_count":303,"metadata":{},"output_type":"execute_result"}],"source":["//val SpanishWritersRDD = spark.sparkContext.makeRDD(SpanishWritersData)\n","val SpanishWritersRDD = spark.sparkContext.parallelize(SpanishWritersData)"]},{"cell_type":"code","execution_count":null,"id":"6a3fba36","metadata":{"id":"6a3fba36","outputId":"0c7e6222-8067-4d1c-d33a-a03c69db1061"},"outputs":[{"data":{"text/plain":["writersDF: org.apache.spark.sql.DataFrame = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":304,"metadata":{},"output_type":"execute_result"}],"source":["val writersDF = SpanishWritersRDD.toDF()"]},{"cell_type":"code","execution_count":null,"id":"90fca4c5","metadata":{"id":"90fca4c5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bcbbea3a","metadata":{"id":"bcbbea3a","outputId":"06b99e0d-4a25-4220-ecdd-c5e855a7e27d"},"outputs":[{"data":{"text/plain":["writersDS: org.apache.spark.sql.Dataset[SpanishWriters] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":305,"metadata":{},"output_type":"execute_result"}],"source":["val writersDS = SpanishWritersRDD.toDS()"]},{"cell_type":"code","execution_count":null,"id":"4b4687c5","metadata":{"id":"4b4687c5","outputId":"2dc6c3cf-5541-42d0-dafd-49927b3916d6"},"outputs":[{"ename":"<console>","evalue":"77: error: value Edad is not a member of org.apache.spark.sql.Row","output_type":"error","traceback":["<console>:77: error: value Edad is not a member of org.apache.spark.sql.Row","       val writersDFResult = writersDF.filter(writer => writer.Edad > 53)","                                                               ^",""]}],"source":["// Dataframe\n","\n","val writersDFResult = writersDF.filter(writer => writer.Edad > 53)\n"]},{"cell_type":"code","execution_count":null,"id":"492601ef","metadata":{"id":"492601ef","outputId":"bf19f26a-8849-41e1-ee94-32fab50cd8b8"},"outputs":[{"data":{"text/plain":["writersDSResult: org.apache.spark.sql.Dataset[SpanishWriters] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":298,"metadata":{},"output_type":"execute_result"}],"source":["//Dataset\n","\n","val writersDSResult = writersDS.filter(writer => writer.Edad > 53)"]},{"cell_type":"code","execution_count":null,"id":"3463ccd4","metadata":{"id":"3463ccd4","outputId":"024cc5a7-9f42-4237-9c3f-04432b945b8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["SpanishWriters(Luis,Góngora,65,M)\n","SpanishWriters(Fancisco,Quevedo,55,M)\n"]}],"source":["writersDSResult.foreach(x => println(x))"]},{"cell_type":"code","execution_count":null,"id":"8aa1a4fe","metadata":{"id":"8aa1a4fe","outputId":"d5ac0e99-8bbe-46ae-9905-060255d4b3b3"},"outputs":[{"data":{"text/plain":["writersDFResult: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":300,"metadata":{},"output_type":"execute_result"}],"source":["val writersDFResult = writersDF.filter(writer => writer.getAs[Int](\"Edad\") > 53)"]},{"cell_type":"code","execution_count":null,"id":"edbef8ae","metadata":{"id":"edbef8ae","outputId":"4157ca70-b821-4c58-af01-28d73ad2d18d"},"outputs":[{"name":"stdout","output_type":"stream","text":["class org.apache.spark.sql.Dataset\n"]}],"source":["System.out.println(writersDFResult.getClass)"]},{"cell_type":"code","execution_count":null,"id":"0a23c147","metadata":{"id":"0a23c147","outputId":"e50bb183-50e5-4077-819c-5f8577fd4300"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/10/22 16:31:43 ERROR Executor: Exception in task 2.0 in stage 106.0 (TID 398)\n","java.lang.ClassCastException: class $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters cannot be cast to class $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters ($line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n","\tat java.base/java.lang.Thread.run(Thread.java:833)\n","22/10/22 16:31:43 ERROR TaskSetManager: Task 2 in stage 106.0 failed 1 times; aborting job\n"]},{"ename":"org.apache.spark.SparkException","evalue":" Job aborted due to stage failure: Task 2 in stage 106.0 failed 1 times, most recent failure: Lost task 2.0 in stage 106.0 (TID 398) (192.168.51.14 executor driver): java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","output_type":"error","traceback":["org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 106.0 failed 1 times, most recent failure: Lost task 2.0 in stage 106.0 (TID 398) (192.168.51.14 executor driver): java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","\tat java.base/java.lang.Thread.run(Thread.java:833)","","Driver stacktrace:","  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)","  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)","  at scala.Option.foreach(Option.scala:407)","  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)","  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)","  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)","  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)","  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)","  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)","  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)","  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)","  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)","  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)","  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)","  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)","  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)","  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)","  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)","  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:776)","  ... 65 elided","Caused by: java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","  at org.apache.spark.scheduler.Task.run(Task.scala:136)","  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","  ... 1 more",""]}],"source":["\n","\n","writersDFResult.show()"]},{"cell_type":"markdown","id":"c2c457c6","metadata":{"id":"c2c457c6"},"source":["**Querying on non existing column**\n"]},{"cell_type":"code","execution_count":null,"id":"2a2db215","metadata":{"id":"2a2db215"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ea72967f","metadata":{"id":"ea72967f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d4c1902b","metadata":{"id":"d4c1902b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d2217a6e","metadata":{"id":"d2217a6e"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"3b0156f6","metadata":{"id":"3b0156f6"},"source":["**Dataset**"]},{"cell_type":"code","execution_count":null,"id":"5491bbe1","metadata":{"id":"5491bbe1","outputId":"9a9fcf19-492a-42fe-b404-c720a3f12b6b"},"outputs":[{"data":{"text/plain":["import spark.sqlContext.implicits._\n","defined class SpanishWriters\n"]},"execution_count":205,"metadata":{},"output_type":"execute_result"}],"source":["import spark.sqlContext.implicits._\n","\n","case class SpanishWriters(Nombre: String, Apellido: String, Edad: Int, Sexo:String)"]},{"cell_type":"code","execution_count":null,"id":"fb45aa1a","metadata":{"id":"fb45aa1a","outputId":"28fa4864-0fb1-4c75-d33b-5da1f1b05863"},"outputs":[{"data":{"text/plain":["SpanishWritersData: Seq[SpanishWriters] = List(SpanishWriters(Miguel,Cervantes,50,M), SpanishWriters(Fancisco,Quevedo,55,M), SpanishWriters(Luis,Góngora,65,M))\n"]},"execution_count":206,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersData = Seq(SpanishWriters(\"Miguel\", \"Cervantes\", 50, \"M\"), SpanishWriters(\"Fancisco\", \"Quevedo\", 55, \"M\"), SpanishWriters(\"Luis\", \"Góngora\", 65, \"M\"))\n"]},{"cell_type":"code","execution_count":null,"id":"8ede0ad7","metadata":{"id":"8ede0ad7","outputId":"577d8262-a830-43bb-c0cf-1dfda42e1ac8"},"outputs":[{"data":{"text/plain":["writersDS: org.apache.spark.sql.Dataset[SpanishWriters] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":207,"metadata":{},"output_type":"execute_result"}],"source":["val writersDS = SpanishWritersData.toDS()"]},{"cell_type":"code","execution_count":null,"id":"3c2c6774","metadata":{"id":"3c2c6774","outputId":"72297a2a-e159-4353-d08f-dcf3a6cc5931"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+---------+----+----+\n","|  Nombre| Apellido|Edad|Sexo|\n","+--------+---------+----+----+\n","|  Miguel|Cervantes|  50|   M|\n","|Fancisco|  Quevedo|  55|   M|\n","|    Luis|  Góngora|  65|   M|\n","+--------+---------+----+----+\n","\n"]}],"source":["writersDS.show()"]},{"cell_type":"code","execution_count":null,"id":"68b20c90","metadata":{"id":"68b20c90","outputId":"01574b7a-5fe2-46a7-8e58-7e1c6d3ab185"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Nombre: string (nullable = true)\n"," |-- Apellido: string (nullable = true)\n"," |-- Edad: integer (nullable = false)\n"," |-- Sexo: string (nullable = true)\n","\n"]}],"source":["writersDS.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"52d1d81f","metadata":{"id":"52d1d81f","outputId":"f285513b-7f3c-4885-8013-180bbf18a76c"},"outputs":[{"data":{"text/plain":["writersDSResult: org.apache.spark.sql.Dataset[SpanishWriters] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"source":["val writersDSResult = writersDS.filter(writer => writer.Edad > 53)"]},{"cell_type":"code","execution_count":null,"id":"31ab2dc7","metadata":{"id":"31ab2dc7","outputId":"57d54949-b267-44a0-9bdd-3f7be55a8790"},"outputs":[{"name":"stdout","output_type":"stream","text":["SpanishWriters(Luis,Góngora,65,M)\n","SpanishWriters(Fancisco,Quevedo,55,M)\n"]}],"source":["writersDSResult.foreach(x => println(x))"]},{"cell_type":"code","execution_count":null,"id":"b955a4bc","metadata":{"id":"b955a4bc","outputId":"9f6e3baf-efac-4ec2-c53c-6873ac01ea3e"},"outputs":[{"data":{"text/plain":["SpanishWritersSeq: Seq[(String, String, Int, String)] = List((Miguel,Cervantes,50,M), (Fancisco,Quevedo,55,M), (Luis,Góngora,65,M))\n","SpanishWritersRDD: org.apache.spark.rdd.RDD[(String, String, Int, String)] = ParallelCollectionRDD[187] at parallelize at <console>:60\n"]},"execution_count":219,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersSeq = Seq((\"Miguel\", \"Cervantes\", 50, \"M\"), (\"Fancisco\", \"Quevedo\", 55, \"M\"), (\"Luis\", \"Góngora\", 65, \"M\"))\n","\n","val SpanishWritersRDD = spark.sparkContext.parallelize(SpanishWritersSeq)\n"]},{"cell_type":"code","execution_count":null,"id":"dda6e0f8","metadata":{"id":"dda6e0f8","outputId":"1f22e6f1-ac41-4a96-8f7e-501197207578"},"outputs":[{"data":{"text/plain":["writersDS: org.apache.spark.sql.Dataset[(String, String, Int, String)] = [_1: string, _2: string ... 2 more fields]\n"]},"execution_count":220,"metadata":{},"output_type":"execute_result"}],"source":["val writersDS = SpanishWritersRDD.toDS()"]},{"cell_type":"code","execution_count":null,"id":"b689a23c","metadata":{"id":"b689a23c","outputId":"3a90d97d-d099-4f49-fb8d-a66da2a2dc88"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+---------+---+---+\n","|      _1|       _2| _3| _4|\n","+--------+---------+---+---+\n","|  Miguel|Cervantes| 50|  M|\n","|Fancisco|  Quevedo| 55|  M|\n","|    Luis|  Góngora| 65|  M|\n","+--------+---------+---+---+\n","\n"]}],"source":["writersDS.show()"]},{"cell_type":"code","execution_count":null,"id":"3c983fbc","metadata":{"scrolled":true,"id":"3c983fbc","outputId":"83139c88-d066-42bc-b9cd-24ddde9471a4"},"outputs":[{"data":{"text/plain":["writersDSResult: org.apache.spark.sql.Dataset[(String, String, Int, String)] = [_1: string, _2: string ... 2 more fields]\n"]},"execution_count":268,"metadata":{},"output_type":"execute_result"}],"source":["val writersDSResult = writersDS.filter(writer => writer._3 > 53)"]},{"cell_type":"code","execution_count":null,"id":"17d5b3d1","metadata":{"id":"17d5b3d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b41a8c1a","metadata":{"id":"b41a8c1a"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"63dd4d4d","metadata":{"id":"63dd4d4d"},"source":["**Dataframe**"]},{"cell_type":"code","execution_count":null,"id":"eea5fa8b","metadata":{"id":"eea5fa8b","outputId":"02c5d77f-477f-42f5-f1a7-2b2b4d4fcc1e"},"outputs":[{"data":{"text/plain":["SpanishWritersData: Seq[(String, String, Int, String)] = List((Miguel,Cervantes,50,M), (Fancisco,Quevedo,55,M), (Luis,Góngora,65,M))\n"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersData = Seq((\"Miguel\", \"Cervantes\", 50, \"M\"), (\"Fancisco\", \"Quevedo\", 55, \"M\"), (\"Luis\", \"Góngora\", 65, \"M\"))\n"]},{"cell_type":"code","execution_count":null,"id":"9edf32a7","metadata":{"id":"9edf32a7","outputId":"929eb04f-1b03-43e9-b69c-b8e2669db263"},"outputs":[{"data":{"text/plain":["import spark.implicits._\n","columns: Seq[String] = List(Nombre, Apellido, Edad, Sexo)\n"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["import spark.implicits._\n","val columns = Seq(\"Nombre\",\"Apellido\", \"Edad\", \"Sexo\")"]},{"cell_type":"code","execution_count":null,"id":"255fb034","metadata":{"id":"255fb034","outputId":"6e249801-0248-4855-aba8-bc5575836b6f"},"outputs":[{"data":{"text/plain":["SpanishWritersRDD: org.apache.spark.rdd.RDD[(String, String, Int, String)] = ParallelCollectionRDD[151] at makeRDD at <console>:41\n"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersRDD = spark.sparkContext.makeRDD(SpanishWritersData)\n"]},{"cell_type":"code","execution_count":null,"id":"3b655191","metadata":{"id":"3b655191","outputId":"98d639c2-11fc-4cca-c71e-34c5f886890b"},"outputs":[{"data":{"text/plain":["SpanishWritersRDD_2: org.apache.spark.rdd.RDD[(String, String, Int, String)] = ParallelCollectionRDD[155] at parallelize at <console>:44\n"]},"execution_count":173,"metadata":{},"output_type":"execute_result"}],"source":["// Borrar\n","val SpanishWritersRDD_2 = spark.sparkContext.parallelize(SpanishWritersData)"]},{"cell_type":"code","execution_count":null,"id":"860600fd","metadata":{"id":"860600fd","outputId":"a0acd1be-05f3-4e2b-9c53-b5634ffd50b6"},"outputs":[{"data":{"text/plain":["dfFromRDD1: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 2 more fields]\n"]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["// Borrar\n","val dfFromRDD1 = SpanishWritersRDD_2.toDF()"]},{"cell_type":"code","execution_count":null,"id":"2e108fad","metadata":{"id":"2e108fad","outputId":"908e824d-1b36-42eb-fcee-07db8a67d2d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- _1: string (nullable = true)\n"," |-- _2: string (nullable = true)\n"," |-- _3: integer (nullable = false)\n"," |-- _4: string (nullable = true)\n","\n"]}],"source":["dfFromRDD1.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"3233639e","metadata":{"id":"3233639e","outputId":"67833d06-d368-42e8-b3de-90e464706419"},"outputs":[{"data":{"text/plain":["writersDF: org.apache.spark.sql.DataFrame = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":308,"metadata":{},"output_type":"execute_result"}],"source":["val writersDF = SpanishWritersRDD.toDF()"]},{"cell_type":"code","execution_count":null,"id":"753f9d90","metadata":{"id":"753f9d90","outputId":"c1a96d76-2df4-4ee0-a72c-ac7430169aec"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/10/22 17:28:26 ERROR Executor: Exception in task 2.0 in stage 108.0 (TID 403)\n","java.lang.ClassCastException: class $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters cannot be cast to class $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters ($line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; $line377.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n","\tat java.base/java.lang.Thread.run(Thread.java:833)\n","22/10/22 17:28:26 ERROR TaskSetManager: Task 2 in stage 108.0 failed 1 times; aborting job\n"]},{"ename":"org.apache.spark.SparkException","evalue":" Job aborted due to stage failure: Task 2 in stage 108.0 failed 1 times, most recent failure: Lost task 2.0 in stage 108.0 (TID 403) (192.168.51.14 executor driver): java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","output_type":"error","traceback":["org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 108.0 failed 1 times, most recent failure: Lost task 2.0 in stage 108.0 (TID 403) (192.168.51.14 executor driver): java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","\tat java.base/java.lang.Thread.run(Thread.java:833)","","Driver stacktrace:","  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)","  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)","  at scala.Option.foreach(Option.scala:407)","  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)","  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)","  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)","  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)","  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)","  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)","  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)","  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)","  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)","  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)","  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)","  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)","  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)","  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)","  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)","  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)","  at org.apache.spark.sql.Dataset.show(Dataset.scala:776)","  ... 65 elided","Caused by: java.lang.ClassCastException: class SpanishWriters cannot be cast to class SpanishWriters (SpanishWriters is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @43f4912e; SpanishWriters is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @751c2a8b)","  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","  at org.apache.spark.scheduler.Task.run(Task.scala:136)","  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","  ... 1 more",""]}],"source":["writersDF.show()"]},{"cell_type":"code","execution_count":null,"id":"0414e1dd","metadata":{"id":"0414e1dd","outputId":"2170f120-e2ef-487e-ac9d-fa23004c97a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Nombre: string (nullable = true)\n"," |-- Apellido: string (nullable = true)\n"," |-- Edad: integer (nullable = false)\n"," |-- Sexo: string (nullable = true)\n","\n"]},{"data":{"text/plain":["writersDF2: org.apache.spark.sql.DataFrame = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":178,"metadata":{},"output_type":"execute_result"}],"source":["\n","val writersDF2 = spark.createDataFrame(SpanishWritersRDD).toDF(columns:_*)\n","writersDF2.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"4cf6cabf","metadata":{"id":"4cf6cabf","outputId":"783051ff-1979-465d-cdc3-69e10dbd0ea0"},"outputs":[{"ename":"<console>","evalue":"44: error: value Edad is not a member of org.apache.spark.sql.Row","output_type":"error","traceback":["<console>:44: error: value Edad is not a member of org.apache.spark.sql.Row","       val writersDFResult = writersDF2.filter(writer => writer.Edad > 53)","                                                                ^",""]}],"source":["val writersDFResult = writersDF2.filter(writer => writer.Edad > 53)"]},{"cell_type":"code","execution_count":null,"id":"e56e5bd6","metadata":{"id":"e56e5bd6","outputId":"3466eb1d-310a-4d93-ecf9-2b2ae3ed7d60"},"outputs":[{"data":{"text/plain":["writersDFResult: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Nombre: string, Apellido: string ... 2 more fields]\n"]},"execution_count":180,"metadata":{},"output_type":"execute_result"}],"source":["val writersDFResult = writersDF2.filter(writer => writer.getAs[Int](\"Edad\") > 53)"]},{"cell_type":"code","execution_count":null,"id":"1d37905d","metadata":{"id":"1d37905d","outputId":"c87d51f0-7c3c-490a-c012-6c08077eac77"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------+----+----+\n","|  Nombre|Apellido|Edad|Sexo|\n","+--------+--------+----+----+\n","|Fancisco| Quevedo|  55|   M|\n","|    Luis| Góngora|  65|   M|\n","+--------+--------+----+----+\n","\n"]}],"source":["writersDFResult.show()"]},{"cell_type":"code","execution_count":null,"id":"f219e52e","metadata":{"id":"f219e52e"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f92e2c16","metadata":{"id":"f92e2c16"},"source":["**Querying on non existing column**"]},{"cell_type":"code","execution_count":null,"id":"a393f226","metadata":{"id":"a393f226","outputId":"2e564a05-2c32-48cc-f1b9-5d0ed6effb73"},"outputs":[{"ename":"org.apache.spark.sql.AnalysisException","evalue":" Column 'Birthday' does not exist. Did you mean one of the following? [Edad, Apellido, Nombre, Sexo];","output_type":"error","traceback":["org.apache.spark.sql.AnalysisException: Column 'Birthday' does not exist. Did you mean one of the following? [Edad, Apellido, Nombre, Sexo];","'Project ['Birthday]","+- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, SpanishWriters, true])).Nombre, true, false, true) AS Nombre#1456, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, SpanishWriters, true])).Apellido, true, false, true) AS Apellido#1457, knownnotnull(assertnotnull(input[0, SpanishWriters, true])).Edad AS Edad#1458, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, SpanishWriters, true])).Sexo, true, false, true) AS Sexo#1459]","   +- ExternalRDD [obj#1455]","","  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:199)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:192)","  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:192)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:192)","  at scala.collection.immutable.Stream.foreach(Stream.scala:533)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:192)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:101)","  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:101)","  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)","  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:187)","  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:210)","  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)","  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)","  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)","  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)","  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)","  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)","  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)","  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)","  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)","  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)","  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)","  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)","  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)","  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)","  at org.apache.spark.sql.Dataset.select(Dataset.scala:1519)","  at org.apache.spark.sql.Dataset.select(Dataset.scala:1536)","  ... 65 elided",""]}],"source":["// Now, let’s see how DataFrame and Dataset behave differently when querying on non-existing column.\n","// Let’s query on a salary column which is not present in DataFrame.\n","\n","val writersDFBirthday = writersDF.select(\"Birthday\")"]},{"cell_type":"code","execution_count":null,"id":"272b57a7","metadata":{"id":"272b57a7","outputId":"b7a1b918-043c-4f8a-ddd3-3305975bda04"},"outputs":[{"ename":"<console>","evalue":"75: error: value Birthday is not a member of SpanishWriters","output_type":"error","traceback":["<console>:75: error: value Birthday is not a member of SpanishWriters","       val writersDSBirthday = writersDS.map(writer => writer.Birthday)","                                                              ^",""]}],"source":["val writersDSBirthday = writersDS.map(writer => writer.Birthday)"]},{"cell_type":"markdown","id":"460a73ae","metadata":{"id":"460a73ae"},"source":["**Preserving schema or not when converted back to RDD**"]},{"cell_type":"code","execution_count":null,"id":"1cf0a516","metadata":{"id":"1cf0a516","outputId":"965ccba6-1a99-4ec5-d560-7f8ed49b26fc"},"outputs":[{"data":{"text/plain":["rddFromDF: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[249] at rdd at <console>:75\n"]},"execution_count":323,"metadata":{},"output_type":"execute_result"}],"source":["val rddFromDF = writersDF.rdd"]},{"cell_type":"code","execution_count":null,"id":"f6116746","metadata":{"id":"f6116746","outputId":"6fceced9-3eb0-41d2-f335-e19655628e5b"},"outputs":[{"ename":"<console>","evalue":"77: error: value Nombre is not a member of org.apache.spark.sql.Row","output_type":"error","traceback":["<console>:77: error: value Nombre is not a member of org.apache.spark.sql.Row","       rddFromDF.map(writer => writer.Nombre).foreach(println)","                                      ^",""]}],"source":["rddFromDF.map(writer => writer.Nombre).foreach(println)"]},{"cell_type":"code","execution_count":null,"id":"06c5c3db","metadata":{"id":"06c5c3db","outputId":"db4b6c7c-87b5-4f66-b9fd-dd5fad461259"},"outputs":[{"data":{"text/plain":["rddFromDS: org.apache.spark.rdd.RDD[SpanishWriters] = MapPartitionsRDD[252] at rdd at <console>:75\n"]},"execution_count":325,"metadata":{},"output_type":"execute_result"}],"source":["val rddFromDS = writersDS.rdd"]},{"cell_type":"code","execution_count":null,"id":"ff575a12","metadata":{"id":"ff575a12","outputId":"b47e32d9-8a22-4fb7-92e7-7014f8aa7969"},"outputs":[{"name":"stdout","output_type":"stream","text":["Luis\n","Fancisco\n","Miguel\n"]}],"source":["rddFromDS.map(writer => writer.Nombre).foreach(println)"]},{"cell_type":"markdown","id":"05b084fe","metadata":{"id":"05b084fe"},"source":["**Create a Spark DataFrame**"]},{"cell_type":"markdown","id":"7c5be91f","metadata":{"id":"7c5be91f"},"source":["**toDF()**"]},{"cell_type":"code","execution_count":null,"id":"9acacc26","metadata":{"id":"9acacc26","outputId":"19882bd8-e9fc-42fc-9754-41c6f93088dd"},"outputs":[{"data":{"text/plain":["carsData: Seq[(String, String, String, Int)] = List((USA,Chrysler,Chrysler 300,292), (Germany,BMW,BMW 8 Series,617), (Spain,Spania GTA,GTA Spano,925))\n"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["val carsData=Seq((\"USA\",\"Chrysler\",\"Chrysler 300\",292),(\"Germany\",\"BMW\",\"BMW 8 Series\",617),(\"Spain\", \"Spania GTA\", \"GTA Spano\",925))\n"]},{"cell_type":"code","execution_count":null,"id":"dd11923b","metadata":{"id":"dd11923b","outputId":"114389ad-6f67-40f4-f89c-36db57925d42"},"outputs":[{"data":{"text/plain":["carsRdd: org.apache.spark.rdd.RDD[(String, String, String, Int)] = ParallelCollectionRDD[6] at parallelize at <console>:31\n"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["val carsRdd = spark.sparkContext.parallelize(carsData)"]},{"cell_type":"code","execution_count":null,"id":"f284ced9","metadata":{"id":"f284ced9","outputId":"47b68b80-9a66-4699-d13d-a75ab5e197f1"},"outputs":[{"data":{"text/plain":["dfCars: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 2 more fields]\n"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["val dfCars = carsRdd.toDF()"]},{"cell_type":"code","execution_count":null,"id":"fc7022fc","metadata":{"id":"fc7022fc","outputId":"609f4f64-13f0-4f21-a8a6-b82aff03e99e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+------------+---+\n","|     _1|        _2|          _3| _4|\n","+-------+----------+------------+---+\n","|    USA|  Chrysler|Chrysler 300|292|\n","|Germany|       BMW|BMW 8 Series|617|\n","|  Spain|Spania GTA|   GTA Spano|925|\n","+-------+----------+------------+---+\n","\n"]}],"source":["dfCars.show()"]},{"cell_type":"code","execution_count":null,"id":"28917205","metadata":{"id":"28917205","outputId":"832ced15-b972-46f8-fc66-6bf67ca9f2f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- _1: string (nullable = true)\n"," |-- _2: string (nullable = true)\n"," |-- _3: string (nullable = true)\n"," |-- _4: integer (nullable = false)\n","\n"]}],"source":["dfCars.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"aafff722","metadata":{"id":"aafff722","outputId":"5715f380-d0eb-4093-db0b-6b565bf4bd34"},"outputs":[{"data":{"text/plain":["dfBrandedCars: org.apache.spark.sql.DataFrame = [Country: string, Manufacturer: string ... 2 more fields]\n"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["val dfBrandedCars = carsRdd.toDF(\"Country\",\"Manufacturer\",\"Model\",\"Power\")"]},{"cell_type":"code","execution_count":null,"id":"b91a3dec","metadata":{"id":"b91a3dec","outputId":"5b057ecb-9d03-4c9f-d2df-614a74bea6f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------+------------+-----+\n","|Country|Manufacturer|       Model|Power|\n","+-------+------------+------------+-----+\n","|    USA|    Chrysler|Chrysler 300|  292|\n","|Germany|         BMW|BMW 8 Series|  617|\n","|  Spain|  Spania GTA|   GTA Spano|  925|\n","+-------+------------+------------+-----+\n","\n"]}],"source":["dfBrandedCars.show()"]},{"cell_type":"markdown","id":"dc089bdb","metadata":{"id":"dc089bdb"},"source":["**createDataFrame(rowRDD,schema)**"]},{"cell_type":"code","execution_count":null,"id":"3f91675e","metadata":{"id":"3f91675e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"dd52ab0b","metadata":{"id":"dd52ab0b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1c509f30","metadata":{"id":"1c509f30"},"outputs":[],"source":["// var dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)"]},{"cell_type":"code","execution_count":null,"id":"82dd6163","metadata":{"id":"82dd6163","outputId":"d00e4a07-86aa-460a-8f43-33f150ad5893"},"outputs":[{"data":{"text/plain":["df2: org.apache.spark.sql.DataFrame = [Country: string, Manufacturer: string ... 2 more fields]\n"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["var df2 = spark.createDataFrame(carsData).toDF(\"Country\",\"Manufacturer\",\"Model\",\"Power\")"]},{"cell_type":"code","execution_count":null,"id":"4f3011c8","metadata":{"id":"4f3011c8","outputId":"f72d211a-f47c-49de-9037-43fc07034dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------+------------+-----+\n","|Country|Manufacturer|       Model|Power|\n","+-------+------------+------------+-----+\n","|    USA|    Chrysler|Chrysler 300|  292|\n","|Germany|         BMW|BMW 8 Series|  617|\n","|  Spain|  Spania GTA|   GTA Spano|  925|\n","+-------+------------+------------+-----+\n","\n"]}],"source":["df2.show()"]},{"cell_type":"code","execution_count":null,"id":"030ca27f","metadata":{"id":"030ca27f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d860801e","metadata":{"id":"d860801e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"77b7e033","metadata":{"id":"77b7e033","outputId":"d74b3a81-9313-4cec-e8cc-4cdcf5272386"},"outputs":[{"data":{"text/plain":["import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n","import org.apache.spark.sql.Row\n","Carschema: org.apache.spark.sql.types.StructType = StructType(StructField(Country,StringType,true),StructField(Manufacturer,StringType,true),StructField(Model,StringType,true),StructField(Power,IntegerType,true))\n","carsRowRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[18] at map at <console>:52\n","dfCarsFromRDD: org.apache.spark.sql.DataFrame = [Country: string, Manufacturer: string ... 2 more fields]\n"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.types.{IntegerType,StringType, StructField, StructType}\n","import org.apache.spark.sql.Row\n","\n","val Carschema = StructType( Array(\n","    StructField(\"Country\", StringType,true),\n","    StructField(\"Manufacturer\", StringType,true),\n","    StructField(\"Model\", StringType,true),\n","    StructField(\"Power\", IntegerType,true)\n","))\n","\n","val carsRowRdd = carsRdd.map(carSpecs => Row(carSpecs._1, carSpecs._2, carSpecs._3, carSpecs._4))\n","val dfCarsFromRDD = spark.createDataFrame(carsRowRdd,Carschema)"]},{"cell_type":"code","execution_count":null,"id":"c25cad12","metadata":{"id":"c25cad12","outputId":"0a0eaffd-7631-41e5-9b97-2462a9729ca3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------+------------+-----+\n","|Country|Manufacturer|       Model|Power|\n","+-------+------------+------------+-----+\n","|    USA|    Chrysler|Chrysler 300|  292|\n","|Germany|         BMW|BMW 8 Series|  617|\n","|  Spain|  Spania GTA|   GTA Spano|  925|\n","+-------+------------+------------+-----+\n","\n"]}],"source":["dfCarsFromRDD.show()"]},{"cell_type":"markdown","id":"52ef1bb0","metadata":{"id":"52ef1bb0"},"source":["**DataFrames. Generic Load/Save Functions**"]},{"cell_type":"code","execution_count":null,"id":"61ed510c","metadata":{"id":"61ed510c","outputId":"786a7607-c4fc-44b8-b4d3-ab39694205f8"},"outputs":[{"data":{"text/plain":["dfPersonas: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["// Generating a parquet file from a CSV\n","val dfPersonas = spark.read.option(\"header\", \"true\").csv(\"/Users/aantolinez/Downloads/personas.csv\")"]},{"cell_type":"code","execution_count":null,"id":"6109077d","metadata":{"id":"6109077d"},"outputs":[],"source":["// Saving the CSV file as parquet\n","dfPersonas.write.parquet(\"/Users/aantolinez/Downloads/personas.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"e72198c3","metadata":{"id":"e72198c3","outputId":"d8671558-1fa7-47b9-8148-6702d76e1834"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/10/29 17:13:16 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 50)\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n","\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n","\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n","\tat java.base/java.lang.Thread.run(Thread.java:833)\n","Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n","\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n","\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n","\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n","\tat scala.util.Success.map(Try.scala:213)\n","\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n","\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n","\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n","\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n","\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n","\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n","\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n","\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n","Caused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n","\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n","\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n","\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n","\t... 14 more\n","22/10/29 17:13:16 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n"]},{"ename":"org.apache.spark.SparkException","evalue":" Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 50) (192.168.0.14 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult:","output_type":"error","traceback":["org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 50) (192.168.0.14 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult:","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)","\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)","\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","\tat java.base/java.lang.Thread.run(Thread.java:833)","Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)","\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)","\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","\tat scala.util.Success.$anonfun$map$1(Try.scala:255)","\tat scala.util.Success.map(Try.scala:213)","\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","Caused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]","\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)","\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)","\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)","\t... 14 more","","Driver stacktrace:","  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)","  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)","  at scala.Option.foreach(Option.scala:407)","  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)","  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)","  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)","  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)","  at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)","  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:527)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:125)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:168)","  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)","  at scala.Option.orElse(Option.scala:447)","  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)","  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)","  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)","  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)","  at scala.Option.getOrElse(Option.scala:189)","  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)","  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)","  ... 40 elided","Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:","  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)","  at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)","  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)","  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)","  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)","  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)","  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)","  at org.apache.spark.scheduler.Task.run(Task.scala:136)","  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)","  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","  ... 1 more","Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}","  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)","  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)","  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","  at scala.util.Success.$anonfun$map$1(Try.scala:255)","  at scala.util.Success.map(Try.scala:213)","  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","  at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","Caused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]","  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)","  at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)","  at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)","  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)","  ... 14 more",""]}],"source":["// Spark SQL\n","// Runtime SQL Configuration\n","\n","val personasDF = spark.read.load(\"/Users/aantolinez/Downloads/personas.csv\")"]},{"cell_type":"code","execution_count":null,"id":"e67a2898","metadata":{"id":"e67a2898","outputId":"9f517227-d642-4f3e-c781-05dde710d733"},"outputs":[{"data":{"text/plain":["personasDF: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["val personasDF = spark.read.load(\"/Users/aantolinez/Downloads/personas.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"b401d51d","metadata":{"id":"b401d51d","outputId":"773b9d1b-a27d-42c9-b238-36635c133846"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]}],"source":["spark.sql(\"SELECT * FROM parquet.`/Users/aantolinez/Downloads/personas.parquet`\").show()"]},{"cell_type":"markdown","id":"3c5f74cc","metadata":{"id":"3c5f74cc"},"source":["**Ignore Corrupt Files**"]},{"cell_type":"code","execution_count":null,"id":"d102ad9b","metadata":{"id":"d102ad9b","outputId":"18cd72bb-4b92-45d0-e815-d72bcddafff3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]},{"data":{"text/plain":["corruptFiles: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["// enable ignore corrupt files\n","spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n","\n","// personas_corrupt.parquet is not real parquet file\n","val corruptFiles = spark.read.parquet(\n","    \"/Users/aantolinez/Downloads/personas.parquet\",\n","    \"/Users/aantolinez/Downloads/personas_corrupt.parquet\")\n","corruptFiles.show()"]},{"cell_type":"markdown","id":"3d0f2d32","metadata":{"id":"3d0f2d32"},"source":["**Modification Time Path Filters**\n"]},{"cell_type":"code","execution_count":null,"id":"44ac7966","metadata":{"id":"44ac7966","outputId":"fbba004d-006a-4ac6-fdca-f34896eaf56e"},"outputs":[{"ename":"org.apache.spark.sql.AnalysisException","evalue":" Unable to infer schema for CSV. It must be specified manually.","output_type":"error","traceback":["org.apache.spark.sql.AnalysisException: Unable to infer schema for CSV. It must be specified manually.","  at org.apache.spark.sql.errors.QueryCompilationErrors$.dataSchemaNotSpecifiedError(QueryCompilationErrors.scala:1007)","  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$12(DataSource.scala:212)","  at scala.Option.getOrElse(Option.scala:189)","  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:212)","  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)","  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)","  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)","  at scala.Option.getOrElse(Option.scala:189)","  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)","  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)","  ... 34 elided",""]}],"source":["val beforeFilterDF = spark.read.format(\"csv\")\n","  .option(\"header\", \"true\")\n","  .option(\"modifiedBefore\", \"2022-10-01T05:30:00\")\n","  .load(\"/Users/aantolinez/Downloads/Hands-On-Spark3\");\n","beforeFilterDF.show();"]},{"cell_type":"code","execution_count":null,"id":"32ec0f4e","metadata":{"id":"32ec0f4e","outputId":"c307cad4-5d2d-478c-a751-e1fcdd7b483a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]},{"data":{"text/plain":["modifiedAfterDF: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["val modifiedAfterDF = spark.read.format(\"csv\")\n","  .option(\"header\", \"true\")\n","  .option(\"modifiedAfter\", \"2022-10-30T05:30:00\")\n","  .load(\"/Users/aantolinez/Downloads/Hands-On-Spark3\");\n","\n","modifiedAfterDF.show();"]},{"cell_type":"markdown","id":"eeb4d780","metadata":{"id":"eeb4d780"},"source":["**Save Modes**"]},{"cell_type":"code","execution_count":null,"id":"b51c6a0d","metadata":{"id":"b51c6a0d","outputId":"b87b269f-d89a-4de6-b617-06ed33a0388b"},"outputs":[{"data":{"text/plain":["SpanishDf: org.apache.spark.sql.DataFrame = [Nombre: string, Primer_Apellido: string ... 3 more fields]\n"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishDf = spark.read.option(\"header\", \"true\")\n","     .option(\"inferSchema\", \"true\")\n","     .csv(\"/Users/aantolinez/Downloads/personas.csv\")"]},{"cell_type":"code","execution_count":null,"id":"9634ec86","metadata":{"id":"9634ec86","outputId":"9fc6412b-38a8-4eed-d084-a3eabee0d660"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------------+-------------------+----+----+\n","|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n","+--------+-----------------+-------------------+----+----+\n","|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n","|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n","|    Luis|       de Góngora|           y Argote|  65|   M|\n","|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n","+--------+-----------------+-------------------+----+----+\n","\n"]}],"source":["SpanishDf.show()"]},{"cell_type":"code","execution_count":null,"id":"f1105d15","metadata":{"id":"f1105d15"},"outputs":[],"source":["SpanishDf.write.format(\"csv\").mode(\"overwrite\")\n","    .option(\"header\", true)\n","    .save(\"/Users/aantolinez/Downloads/personas2.csv\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"224de7f0","metadata":{"id":"224de7f0","outputId":"68c0df7b-379d-49fe-e8fa-74887ca2ce84"},"outputs":[{"data":{"text/plain":["SpanishWritersData2: Seq[(String, String, Int, String)] = List((Miguel,de Unamuno,70,M))\n","SpanishWritersRdd: org.apache.spark.rdd.RDD[(String, String, Int, String)] = ParallelCollectionRDD[70] at parallelize at <console>:28\n"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["\n","val SpanishWritersData2 = Seq((\"Miguel\", \"de Unamuno\", 70, \"M\"))\n","\n","val SpanishWritersRdd = spark.sparkContext.parallelize(SpanishWritersData2)"]},{"cell_type":"code","execution_count":null,"id":"da9c07ba","metadata":{"id":"da9c07ba","outputId":"f205b216-8dfe-487a-9486-7704da9b8327"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+----------+---+---+\n","|    _1|        _2| _3| _4|\n","+------+----------+---+---+\n","|Miguel|de Unamuno| 70|  M|\n","+------+----------+---+---+\n","\n"]},{"data":{"text/plain":["SpanishWritersAppendDF: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 2 more fields]\n"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["val SpanishWritersAppendDF = SpanishWritersRdd.toDF()\n","\n","writersDF.show()"]},{"cell_type":"code","execution_count":null,"id":"97d70523","metadata":{"id":"97d70523"},"outputs":[],"source":["SpanishWritersAppendDF.write.format(\"csv\").mode(\"append\").save(\"/Users/aantolinez/Downloads/personas2.csv\")\n"]},{"cell_type":"markdown","id":"505d51be","metadata":{"id":"505d51be"},"source":["**Saving data into a single file**"]},{"cell_type":"code","execution_count":null,"id":"e3864d87","metadata":{"id":"e3864d87"},"outputs":[],"source":["// Using coalesce()\n","\n","SpanishWritersAppendDF.coalesce(1)\n",".write.csv(\"/Users/aantolinez/Downloads/personas_coalesce.csv\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0d1719c6","metadata":{"id":"0d1719c6"},"outputs":[],"source":["// Using repartition()\n","\n","df.repartition(1).write.csv(\"address\")"]},{"cell_type":"code","execution_count":null,"id":"5a941342","metadata":{"id":"5a941342"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0b081d74","metadata":{"id":"0b081d74"},"source":["**Chapter 4. Read and Write Apache Parquet Files**"]},{"cell_type":"code","execution_count":null,"id":"21ec4f47","metadata":{"id":"21ec4f47","outputId":"e960dc4b-ccf7-4ae0-eb2f-2d0334c353ba"},"outputs":[{"data":{"text/plain":["import org.apache.spark.sql.types.{StringType, StructType, IntegerType}\n","schemaWriters: org.apache.spark.sql.types.StructType = StructType(StructField(Name,StringType,true),StructField(Surname,StringType,true),StructField(Century,StringType,true),StructField(YearOfBirth,IntegerType,true))\n","SpanishWritersDf: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.types.{StringType, StructType, IntegerType} \n","\n","val schemaWriters = new StructType()\n","    .add(\"Name\",StringType,true)\n","    .add(\"Surname\",StringType,true)\n","    .add(\"Century\",StringType,true)\n","    .add(\"YearOfBirth\",IntegerType,true)\n","val SpanishWritersDf = spark.read.option(\"header\", \"true\")\n","    .schema(schemaWriters)\n","    .csv(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.csv\")"]},{"cell_type":"code","execution_count":null,"id":"e0731542","metadata":{"id":"e0731542","outputId":"49cd6679-cfdd-411c-efdf-9582413251f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------------+-------+-----------+\n","|     Name|        Surname|Century|YearOfBirth|\n","+---------+---------------+-------+-----------+\n","|  Gonzalo|      de Berceo|   XIII|       1196|\n","|    Juan |           Ruiz|    XIV|       1283|\n","| Fernando|       de Rojas|     XV|       1465|\n","|Garcilaso|     de la Vega|    XVI|       1539|\n","|   Miguel|   de Cervantes|    XVI|       1547|\n","|Francisco|     de Quevedo|    XVI|       1580|\n","|     Luis|     de Góngora|    XVI|       1561|\n","|     Lope|        de Vega|    XVI|       1562|\n","|    Tirso|      de Molina|    XVI|       1583|\n","| Calderón|    de la Barca|   XVII|       1600|\n","|   Adolfo|        Bécquer|    XIX|       1836|\n","|   Benito|   Pérez Galdós|    XIX|       1843|\n","|   Emilia|    Pardo Bazán|    XIX|       1851|\n","|     José|Ortega y Gasset|     XX|       1883|\n","+---------+---------------+-------+-----------+\n","\n"]}],"source":["SpanishWritersDf.show()"]},{"cell_type":"code","execution_count":null,"id":"eed1c379","metadata":{"id":"eed1c379","outputId":"0b17a5f4-48fa-4cfc-efb3-c26787f046a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Surname: string (nullable = true)\n"," |-- Century: string (nullable = true)\n"," |-- YearOfBirth: integer (nullable = true)\n","\n"]}],"source":["SpanishWritersDf.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"f613d2b1","metadata":{"id":"f613d2b1"},"outputs":[],"source":["// Saving data with default compression codec: snappy\n","SpanishWritersDf.write.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")\n"]},{"cell_type":"code","execution_count":null,"id":"ff2a5380","metadata":{"id":"ff2a5380"},"outputs":[],"source":["// Saving data with gzip compression codec\n","SpanishWritersDf.write.mode(\"append\").option(\"compression\", \"gzip\").parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")\n","\n"]},{"cell_type":"markdown","id":"101dd0ab","metadata":{"id":"101dd0ab"},"source":["*Using SQL queries on Parquet\n","\n","We can also create a temporary view on Parquet files and then use it in Spark SQL statements. This temporary table would be available until the SparkContext present."]},{"cell_type":"code","execution_count":null,"id":"429b8556","metadata":{"id":"429b8556","outputId":"195cc496-f010-4d5d-8713-2d8b630cf1ff"},"outputs":[{"data":{"text/plain":["parquetDF: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["val parquetDF = spark.read.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")\n"]},{"cell_type":"code","execution_count":null,"id":"1985d843","metadata":{"id":"1985d843","outputId":"bcbdeb58-ca1e-4765-fabd-f98bff1aaf1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Surname: string (nullable = true)\n"," |-- Century: string (nullable = true)\n"," |-- YearOfBirth: integer (nullable = true)\n","\n"]}],"source":["parquetDF.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"edef00f9","metadata":{"id":"edef00f9","outputId":"b50d0006-2c32-400b-8221-81f16823fb30"},"outputs":[{"data":{"text/plain":["sqlDf: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["parquetDF.createOrReplaceTempView(\"TempTable\")\n","\n","val sqlDf = spark.sql(\"select * from TempTable where YearOfBirth = 1600\")"]},{"cell_type":"code","execution_count":null,"id":"73695530","metadata":{"id":"73695530","outputId":"f55a1baa-cb75-4469-f928-48a3bf344de0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----------+-------+-----------+\n","|    Name|    Surname|Century|YearOfBirth|\n","+--------+-----------+-------+-----------+\n","|Calderón|de la Barca|   XVII|       1600|\n","+--------+-----------+-------+-----------+\n","\n"]}],"source":["sqlDf.show()"]},{"cell_type":"markdown","id":"c60cbb4e","metadata":{"id":"c60cbb4e"},"source":["**Parquet File Partitioning**"]},{"cell_type":"code","execution_count":null,"id":"1a782697","metadata":{"id":"1a782697","outputId":"bf42261a-6d38-402e-e86e-ce6828085b48"},"outputs":[{"data":{"text/plain":["import org.apache.spark.sql.types.{StringType, StructType, IntegerType}\n","schemaWriters: org.apache.spark.sql.types.StructType = StructType(StructField(Name,StringType,true),StructField(Surname,StringType,true),StructField(Century,StringType,true),StructField(YearOfBirth,IntegerType,true),StructField(Gender,StringType,true))\n","SpanishWritersDf: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 3 more fields]\n"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.types.{StringType, StructType, IntegerType} \n","\n","val schemaWriters = new StructType()\n","    .add(\"Name\",StringType,true)\n","    .add(\"Surname\",StringType,true)\n","    .add(\"Century\",StringType,true)\n","    .add(\"YearOfBirth\",IntegerType,true)\n","    .add(\"Gender\",StringType,true)\n","val SpanishWritersDf = spark.read.option(\"header\", \"true\")\n","    .schema(schemaWriters)\n","    .csv(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.csv\")"]},{"cell_type":"code","execution_count":null,"id":"aa72ea50","metadata":{"id":"aa72ea50","outputId":"9bde3549-fadc-4eaa-fad5-88da027c8840"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Surname: string (nullable = true)\n"," |-- Century: string (nullable = true)\n"," |-- YearOfBirth: integer (nullable = true)\n"," |-- Gender: string (nullable = true)\n","\n"]}],"source":["SpanishWritersDf.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"f2b52df2","metadata":{"id":"f2b52df2"},"outputs":[],"source":["SpanishWritersDf.write.partitionBy(\"Century\",\"Gender\")\n","    .parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.parquet\")"]},{"cell_type":"markdown","id":"529e639a","metadata":{"id":"529e639a"},"source":["Spanish_Writers_by_Gender.parquet\n","├── ._SUCCESS.crc\n","├── Century=XIII\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XIV\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XIX\n","│   ├── Gender=F\n","│   │   ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│   │   └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XV\n","│   ├── Gender=F\n","│   │   ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│   │   └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XVI\n","│   ├── Gender=F\n","│   │   ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│   │   └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XVII\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","├── Century=XX\n","│   └── Gender=M\n","│       ├── .part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet.crc\n","│       └── part-00000-aca04d79-8009-4b12-9e67-45d9f7f975e8.c000.snappy.parquet\n","└── _SUCCESS\n"]},{"cell_type":"markdown","id":"66596f4d","metadata":{"id":"66596f4d"},"source":["**Reading Parquet File Partitions**"]},{"cell_type":"code","execution_count":null,"id":"be818344","metadata":{"id":"be818344","outputId":"433e2791-427e-48a2-9300-4d67d863a730"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+---------------+-----------+------+\n","|Name|        Surname|YearOfBirth|Gender|\n","+----+---------------+-----------+------+\n","|José|Ortega y Gasset|       1883|     M|\n","+----+---------------+-----------+------+\n","\n"]},{"data":{"text/plain":["partitionDF: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","val partitionDF = spark.read.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.parquet/Century=XX\")\n","\n","partitionDF.show()\n"]},{"cell_type":"markdown","id":"b2d1f812","metadata":{"id":"b2d1f812"},"source":["**Spark JSON Read and Write operations**"]},{"cell_type":"markdown","id":"0acf5b9f","metadata":{"id":"0acf5b9f"},"source":["*Spark Read JSON Files into DataFrames*"]},{"cell_type":"code","execution_count":null,"id":"257050f3","metadata":{"id":"257050f3","outputId":"e9024a34-96da-49bf-a6d5-77456fa3acb1"},"outputs":[{"data":{"text/plain":["jsonDf: org.apache.spark.sql.DataFrame = [country: string, email: string ... 5 more fields]\n"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["//Loading a newline-delimited JSON strings file into a dataframe\n","\n","val jsonDf = spark.read.json(\"/Users/aantolinez/Downloads/Spaniards.json\")\n"]},{"cell_type":"code","execution_count":null,"id":"f544d2a5","metadata":{"id":"f544d2a5","outputId":"eafa5f97-6fe6-402e-fc36-1fcf0919e75c"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- country: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- id: long (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- registered: boolean (nullable = true)\n"," |-- updated: string (nullable = true)\n","\n"]}],"source":["jsonDf.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"17dea0ff","metadata":{"id":"17dea0ff","outputId":"95e460cc-b16f-4a29-a18e-c35e8aa940bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------+---+---------+----------+----------+\n","|country|email              |first_name|id |last_name|registered|updated   |\n","+-------+-------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n","+-------+-------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["jsonDf.show(4, false)"]},{"cell_type":"code","execution_count":null,"id":"5644f3a4","metadata":{"id":"5644f3a4","outputId":"e0b420da-6111-4b1e-84b8-1c6fe31b979c"},"outputs":[{"data":{"text/plain":["multilineJsonDf: org.apache.spark.sql.DataFrame = [country: string, email: string ... 5 more fields]\n"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["//Loading a multiline JSON strings file into a dataframe\n","\n","val multilineJsonDf = spark.read.option(\"multiline\",\"true\")\n",".json(\"/Users/aantolinez/Downloads/Spaniards_array.json\")\n"]},{"cell_type":"code","execution_count":null,"id":"3b4160f0","metadata":{"id":"3b4160f0","outputId":"698f4ccb-d367-4765-d73c-b7df440a6d86"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------+---+---------+----------+----------+\n","|country|email              |first_name|id |last_name|registered|updated   |\n","+-------+-------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n","+-------+-------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["multilineJsonDf.show(4, false) "]},{"cell_type":"markdown","id":"103599d9","metadata":{"id":"103599d9"},"source":["**Reading Multiple JSON Files at Once**"]},{"cell_type":"code","execution_count":null,"id":"bfbe3022","metadata":{"id":"bfbe3022","outputId":"2d196500-9ac9-44d0-f7af-fed490122392"},"outputs":[{"data":{"text/plain":["multipleJsonsDf: org.apache.spark.sql.DataFrame = [country: string, email: string ... 5 more fields]\n"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["//Loading a multiline JSON strings file into a dataframe at once\n","\n","val multipleJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n","    \"/Users/aantolinez/Downloads/Spaniards_array.json\",\n","    \"/Users/aantolinez/Downloads/Spaniards_array2.json\")"]},{"cell_type":"code","execution_count":null,"id":"6093fb5a","metadata":{"id":"6093fb5a","outputId":"edb69315-8597-4140-8b7b-5401d1b9218c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------+---+---------+----------+----------+\n","|country|email              |first_name|id |last_name|registered|updated   |\n","+-------+-------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n","|Spain  |luis.herrera@xyz.es|Luis      |1  |Herrera  |false     |2015-05-15|\n","|Spain  |mabad@opti.es      |Marcos    |2  |Abad     |true      |2015-03-21|\n","|Spain  |jabalos@redis.org  |Juan      |3  |Abalos   |true      |2015-02-14|\n","|Spain  |samo@terra.es      |Santiago  |4  |Amo      |false     |2014-10-21|\n","+-------+-------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["multipleJsonsDf.show(false)"]},{"cell_type":"markdown","id":"66659e25","metadata":{"id":"66659e25"},"source":["**Reading JSON Files Based On Patterns At Once**"]},{"cell_type":"code","execution_count":null,"id":"8abdcf59","metadata":{"id":"8abdcf59","outputId":"44a7b957-5cf2-466e-aad6-d281f39a2c30"},"outputs":[{"data":{"text/plain":["patternJsonsDf: org.apache.spark.sql.DataFrame = [country: string, email: string ... 5 more fields]\n"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["val patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n","    \"/Users/aantolinez/Downloads/Spaniards_array*.json\")"]},{"cell_type":"code","execution_count":null,"id":"1665e59f","metadata":{"id":"1665e59f","outputId":"737bc263-5257-4abe-fd85-e815ba609d42"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------------+----------+---+---------+----------+----------+\n","|country|email                    |first_name|id |last_name|registered|updated   |\n","+-------+-------------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.garcia@xyz.es       |Lucia     |9  |Garcia   |true      |2015-05-15|\n","|Spain  |maria.rodriguez@opti.es  |Maria     |10 |Rodriguez|true      |2015-03-21|\n","|Spain  |carmen.gonzalez@redis.org|Carmen    |11 |Gonzalez |true      |2015-02-14|\n","|Spain  |sara.fernandez@terra.es  |Sara      |12 |Fernandez|true      |2014-10-21|\n","|Spain  |luis.ortiz@mapy.cz       |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es       |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org           |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com      |Santiago  |4  |Sanchez  |false     |2014-10-31|\n","|Spain  |luis.herrera@xyz.es      |Luis      |1  |Herrera  |false     |2015-05-15|\n","|Spain  |mabad@opti.es            |Marcos    |2  |Abad     |true      |2015-03-21|\n","|Spain  |jabalos@redis.org        |Juan      |3  |Abalos   |true      |2015-02-14|\n","|Spain  |samo@terra.es            |Santiago  |4  |Amo      |false     |2014-10-21|\n","+-------+-------------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["patternJsonsDf.show(20, false)"]},{"cell_type":"code","execution_count":null,"id":"3e5144fe","metadata":{"id":"3e5144fe"},"outputs":[],"source":["//  Reading all the JSON files from a directory and only JSON files.\n","\n","val patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n","    \"/Users/aantolinez/Downloads/*.json\")"]},{"cell_type":"code","execution_count":null,"id":"e7f0b243","metadata":{"id":"e7f0b243"},"outputs":[],"source":["//  Reading ALL the files from a directory.\n","\n","val patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n","    \"/Users/aantolinez/Downloads/\")"]},{"cell_type":"markdown","id":"bb4de788","metadata":{"id":"bb4de788"},"source":["**Direct Queries on Parquet Files**"]},{"cell_type":"code","execution_count":null,"id":"bc0aa8a1","metadata":{"id":"bc0aa8a1","outputId":"b23fb804-40fd-455f-92d0-f10f801dbd1e"},"outputs":[{"data":{"text/plain":["res14: org.apache.spark.sql.DataFrame = []\n"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["// One way to do it linking a SQL query string\n","\n","spark.sqlContext.sql(\"CREATE TEMPORARY VIEW Spaniards USING json OPTIONS\" + \n","      \" (path '/Users/aantolinez/Downloads/Spaniards.json')\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"abf2c1ae","metadata":{"id":"abf2c1ae","outputId":"fe9f6801-b1d5-45f1-f264-85c644057a23"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------+---+---------+----------+----------+\n","|country|email              |first_name|id |last_name|registered|updated   |\n","+-------+-------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n","+-------+-------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["spark.sqlContext.sql(\"select * from Spaniards\").show(false)"]},{"cell_type":"code","execution_count":null,"id":"9abc361a","metadata":{"id":"9abc361a","outputId":"aa6f9267-9546-4a0a-a2cb-cd1ca43d2adb"},"outputs":[{"data":{"text/plain":["sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@74bdd79c\n"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["// Another way to do it, step-by-step\n","\n","val sqlContext = new org.apache.spark.sql.SQLContext(sc)"]},{"cell_type":"code","execution_count":null,"id":"d9979526","metadata":{"id":"d9979526","outputId":"670f6643-2528-4090-c2b4-51177d0a5eba"},"outputs":[{"data":{"text/plain":["Spaniards: org.apache.spark.sql.DataFrame = [country: string, email: string ... 5 more fields]\n"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["val Spaniards = sqlContext.jsonFile(\"/Users/aantolinez/Downloads/Spaniards.json\")"]},{"cell_type":"code","execution_count":null,"id":"46d2916e","metadata":{"id":"46d2916e"},"outputs":[],"source":["Spaniards.registerTempTable(\"Spaniards\")"]},{"cell_type":"code","execution_count":null,"id":"dcd7c254","metadata":{"id":"dcd7c254","outputId":"6b3e905e-90a9-4d6d-a81c-97296835e928"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------------+----------+---+---------+----------+----------+\n","|country|email              |first_name|id |last_name|registered|updated   |\n","+-------+-------------------+----------+---+---------+----------+----------+\n","|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n","|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n","|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n","|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n","+-------+-------------------+----------+---+---------+----------+----------+\n","\n"]}],"source":["sqlContext.sql(\"select * from Spaniards\").show(false)"]},{"cell_type":"markdown","id":"82402d90","metadata":{"id":"82402d90"},"source":["*How to work with Complex Nested JSON Files using Spark SQL*\n"]},{"cell_type":"code","execution_count":null,"id":"bfa37a96","metadata":{"id":"bfa37a96"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"557f73a6","metadata":{"id":"557f73a6"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2ec29ace","metadata":{"id":"2ec29ace"},"source":["**Saving a DataFrame To JSON File**"]},{"cell_type":"code","execution_count":null,"id":"cde45719","metadata":{"id":"cde45719"},"outputs":[],"source":["multipleJsonsDf.write\n"," .json(\"/Users/aantolinez/Downloads/Merged_Spaniards_array.json\")"]},{"cell_type":"code","execution_count":null,"id":"cc0b016b","metadata":{"id":"cc0b016b"},"outputs":[],"source":["multipleJsonsDf.write.mode(\"append\").json(\"/Users/aantolinez/Downloads/Merged_Spaniards_array.json\")"]},{"cell_type":"markdown","id":"51219e77","metadata":{"id":"51219e77"},"source":["**Load JSON Files Based On Customized Schemas**"]},{"cell_type":"code","execution_count":null,"id":"3b9a214c","metadata":{"id":"3b9a214c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a5bcf6bd","metadata":{"id":"a5bcf6bd","outputId":"906545b2-829f-4816-9bf3-d13e1a2a9f3a"},"outputs":[{"data":{"text/plain":["import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, BooleanType, DateType}\n","schemaSpaniards: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(email,StringType,true),StructField(country,StringType,true),StructField(updated,DateType,true),StructField(registered,BooleanType,true))\n"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.types.{StructType,StructField, StringType,IntegerType,BooleanType,DateType}\n","\n","val schemaSpaniards = StructType(Array(\n","    StructField(\"id\",StringType,nullable=true),\n","    StructField(\"first_name\",StringType,nullable=true),\n","    StructField(\"last_name\",StringType,nullable=true),\n","    StructField(\"email\", StringType,nullable=true),\n","    StructField(\"country\", StringType,nullable=true),\n","    StructField(\"updated\", DateType,nullable=true),\n","    StructField(\"registered\", BooleanType,nullable=true)\n","  ))"]},{"cell_type":"code","execution_count":null,"id":"1f7e4115","metadata":{"id":"1f7e4115","outputId":"3c773261-d72f-47d2-f918-c10db417b0f6"},"outputs":[{"data":{"text/plain":["schemaSpaniardsDf: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 5 more fields]\n"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["val schemaSpaniardsDf = spark.read.schema(schemaSpaniards).json(\"/Users/aantolinez/Downloads/Spaniards.json\")\n"]},{"cell_type":"code","execution_count":null,"id":"0b4f0dce","metadata":{"id":"0b4f0dce","outputId":"102419c6-57a8-4ec5-a6a5-37de4cafc578"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: string (nullable = true)\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- country: string (nullable = true)\n"," |-- updated: date (nullable = true)\n"," |-- registered: boolean (nullable = true)\n","\n"]}],"source":["schemaSpaniardsDf.printSchema()\n"]},{"cell_type":"code","execution_count":null,"id":"b6da2f86","metadata":{"id":"b6da2f86","outputId":"d50bdc20-797f-414c-f2e7-b3301db3bdd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------+---------+-------------------+-------+----------+----------+\n","|id |first_name|last_name|email              |country|updated   |registered|\n","+---+----------+---------+-------------------+-------+----------+----------+\n","|1  |Luis      |Ortiz    |luis.ortiz@mapy.cz |Spain  |2015-05-16|false     |\n","|2  |Alfonso   |Antolinez|aantolinez@optc.es |Spain  |2015-03-11|true      |\n","|3  |Juan      |Dominguez|jdomin@xyz.org     |Spain  |2015-02-15|true      |\n","|4  |Santiago  |Sanchez  |ssanchez@google.com|Spain  |2014-10-31|false     |\n","+---+----------+---------+-------------------+-------+----------+----------+\n","\n"]}],"source":["schemaSpaniardsDf.show(false)"]},{"cell_type":"markdown","id":"f30099f8","metadata":{"id":"f30099f8"},"source":["**Chapter 4. Work With Complex Nested JSON Structures using Spark**"]},{"cell_type":"code","execution_count":null,"id":"da79f3e3","metadata":{"id":"da79f3e3","outputId":"7150919b-eaf7-47a7-e3a5-d7c88fc8add7"},"outputs":[{"data":{"text/plain":["dfMlBooks: org.apache.spark.sql.DataFrame = [Book: struct<Authors: array<struct<firstname:string,lastname:string>>, DOI: string ... 7 more fields>]\n"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["val dfMlBooks = spark.read.option(\"multiline\", \"true\").json(\"file:///Users/aantolinez/Books_array.json\")"]},{"cell_type":"code","execution_count":null,"id":"009c69c5","metadata":{"id":"009c69c5","outputId":"40c84cbc-79e2-4de8-ad42-77ad019b5a3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Book: struct (nullable = true)\n"," |    |-- Authors: array (nullable = true)\n"," |    |    |-- element: struct (containsNull = true)\n"," |    |    |    |-- firstname: string (nullable = true)\n"," |    |    |    |-- lastname: string (nullable = true)\n"," |    |-- DOI: string (nullable = true)\n"," |    |-- Editors: array (nullable = true)\n"," |    |    |-- element: struct (containsNull = true)\n"," |    |    |    |-- firstname: string (nullable = true)\n"," |    |    |    |-- lastname: string (nullable = true)\n"," |    |-- ISBN: array (nullable = true)\n"," |    |    |-- element: struct (containsNull = true)\n"," |    |    |    |-- Hardcover ISBN: string (nullable = true)\n"," |    |    |    |-- Softcover ISBN: string (nullable = true)\n"," |    |    |    |-- eBook ISBN: string (nullable = true)\n"," |    |-- Id: long (nullable = true)\n"," |    |-- Publisher: string (nullable = true)\n"," |    |-- Title: struct (nullable = true)\n"," |    |    |-- Book Subtitle: string (nullable = true)\n"," |    |    |-- Book Title: string (nullable = true)\n"," |    |-- Topics: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- eBook Packages: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n","\n"]}],"source":["dfMlBooks.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"26ddab31","metadata":{"id":"26ddab31","outputId":"dfddf289-b950-465c-d6b4-f6265ca1ec39"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|Book                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|{[{Adam,  Freeman}], https://doi.org/10.1007/978-1-4842-7355-5, null, [{null, 978-1-4842-7354-8, 978-1-4842-7355-5}], 1, Apress Berkeley, CA, {The Complete Guide to Programming Reliable and Efficient Software Using Golang, Pro Go}, [Programming Language, Open Source, Programming Techniques], [Professional and Applied Computing, Professional and Applied Computing (R0), Apress Access Books]}                                |\n","|{[{Hien, Luu}], https://doi.org/10.1007/978-1-4842-3579-9, null, [{null, null, 978-1-4842-3579-9}], 2, Apress Berkeley, CA, {With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library, Beginning Apache Spark 2}, [Big Data, Java, Data Mining and Knowledge Discovery, Open Source], [Professional and Applied Computing, Professional and Applied Computing (R0), Apress Access Books]}|\n","|{null, https://doi.org/10.1007/978-981-13-0550-4, [{Valentina E., Mamta Mittal}, {Balas, Lalit Mohan Goyal}], [{978-981-13-0549-8, 978-981-13-4448-0, 978-981-13-0550-4}], 3, Springer Singapore, {null, Big Data Processing Using Spark in Cloud}, [Big Data, Data and Information Security, Data Analysis and Big Data], [ Engineering, Engineering (R0)]}                                                                            |\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["dfMlBooks.show(false)"]},{"cell_type":"code","execution_count":null,"id":"01ab9e0d","metadata":{"id":"01ab9e0d","outputId":"1185f938-f9d8-42d3-8d4a-1e80394b7fb5"},"outputs":[{"data":{"text/plain":["df2: org.apache.spark.sql.DataFrame = [Authors: array<struct<firstname:string,lastname:string>>, DOI: string ... 7 more fields]\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["val df2 =dfMlBooks.select(\"Book.*\")"]},{"cell_type":"code","execution_count":null,"id":"9c94d999","metadata":{"id":"9c94d999","outputId":"13377197-6d0c-411d-dd53-1b0a024adf9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- col: struct (nullable = true)\n"," |    |-- firstname: string (nullable = true)\n"," |    |-- lastname: string (nullable = true)\n"," |-- DOI: string (nullable = true)\n"," |-- Editors: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- firstname: string (nullable = true)\n"," |    |    |-- lastname: string (nullable = true)\n"," |-- ISBN: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- Hardcover ISBN: string (nullable = true)\n"," |    |    |-- Softcover ISBN: string (nullable = true)\n"," |    |    |-- eBook ISBN: string (nullable = true)\n"," |-- Id: long (nullable = true)\n"," |-- Publisher: string (nullable = true)\n"," |-- Title: struct (nullable = true)\n"," |    |-- Book Subtitle: string (nullable = true)\n"," |    |-- Book Title: string (nullable = true)\n"," |-- Topics: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- eBook Packages: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n","\n"]},{"data":{"text/plain":["df3: org.apache.spark.sql.DataFrame = [col: struct<firstname: string, lastname: string>, DOI: string ... 7 more fields]\n"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["val df3=df2.select(explode_outer($\"Authors\"), col(\"DOI\"), $\"Editors\", $\"ISBN\", col(\"Id\"), col(\"Publisher\"), $\"Title\", col(\"Topics\"), $\"eBook Packages\")\n","//df3.show(false)\n","df3.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"1033f886","metadata":{"id":"1033f886","outputId":"e3a8ce61-923c-4aae-ef44-7fc598f4035f"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- firstname: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- DOI: string (nullable = true)\n"," |-- col: struct (nullable = true)\n"," |    |-- firstname: string (nullable = true)\n"," |    |-- lastname: string (nullable = true)\n"," |-- ISBN: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- Hardcover ISBN: string (nullable = true)\n"," |    |    |-- Softcover ISBN: string (nullable = true)\n"," |    |    |-- eBook ISBN: string (nullable = true)\n"," |-- Id: long (nullable = true)\n"," |-- Publisher: string (nullable = true)\n"," |-- Title: struct (nullable = true)\n"," |    |-- Book Subtitle: string (nullable = true)\n"," |    |-- Book Title: string (nullable = true)\n"," |-- Topics: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- eBook Packages: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n","\n"]},{"data":{"text/plain":["df4: org.apache.spark.sql.DataFrame = [firstname: string, lastname: string ... 8 more fields]\n"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["val df4=df3.select(col(\"col.*\"),  col(\"DOI\"), explode_outer($\"Editors\"), $\"ISBN\", col(\"Id\"), col(\"Publisher\"), $\"Title\", col(\"Topics\"), $\"eBook Packages\")\n","//df4.show(false)\n","df4.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"f2b132c6","metadata":{"id":"f2b132c6","outputId":"a7da862c-64c3-453b-88c3-4b8b37250150"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Afirstname: string (nullable = true)\n"," |-- Alastname: string (nullable = true)\n"," |-- DOI: string (nullable = true)\n"," |-- firstname: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- col: struct (nullable = true)\n"," |    |-- Hardcover ISBN: string (nullable = true)\n"," |    |-- Softcover ISBN: string (nullable = true)\n"," |    |-- eBook ISBN: string (nullable = true)\n"," |-- Id: long (nullable = true)\n"," |-- Publisher: string (nullable = true)\n"," |-- Title: struct (nullable = true)\n"," |    |-- Book Subtitle: string (nullable = true)\n"," |    |-- Book Title: string (nullable = true)\n"," |-- Topics: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- eBook Packages: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n","\n"]},{"data":{"text/plain":["df5: org.apache.spark.sql.DataFrame = [Afirstname: string, Alastname: string ... 9 more fields]\n"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["val df5=df4.select(col(\"firstname\").alias(\"Afirstname\"),col(\"lastname\").alias(\"Alastname\"),col(\"DOI\"), col(\"col.*\"),explode_outer($\"ISBN\"), col(\"Id\"), col(\"Publisher\"), $\"Title\", col(\"Topics\"), $\"eBook Packages\")\n","//df5.show(false)\n","df5.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"6d2bc869","metadata":{"id":"6d2bc869","outputId":"f8a13d0e-0b6f-40ae-e12b-450316e45bd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Afirstname: string (nullable = true)\n"," |-- Alastname: string (nullable = true)\n"," |-- DOI: string (nullable = true)\n"," |-- Efirstname: string (nullable = true)\n"," |-- Elastname: string (nullable = true)\n"," |-- Hardcover ISBN: string (nullable = true)\n"," |-- Softcover ISBN: string (nullable = true)\n"," |-- eBook ISBN: string (nullable = true)\n"," |-- Id: long (nullable = true)\n"," |-- Publisher: string (nullable = true)\n"," |-- Book Subtitle: string (nullable = true)\n"," |-- Book Title: string (nullable = true)\n"," |-- Topics: string (nullable = true)\n"," |-- eBook Packages: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n","\n"]},{"data":{"text/plain":["df6: org.apache.spark.sql.DataFrame = [Afirstname: string, Alastname: string ... 12 more fields]\n"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["val df6=df5.select(col(\"Afirstname\"),col(\"Alastname\"),col(\"DOI\"),col(\"firstname\").alias(\"Efirstname\"),col(\"lastname\").alias(\"Elastname\"),col(\"col.Hardcover ISBN\").alias(\"Hardcover ISBN\"),col(\"col.Softcover ISBN\").alias(\"Softcover ISBN\"),col(\"col.eBook ISBN\").alias(\"eBook ISBN\"), col(\"Id\"), col(\"Publisher\"),col(\"Title.Book Subtitle\").alias(\"Book Subtitle\"),col(\"Title.Book Title\").alias(\"Book Title\") ,explode_outer($\"Topics\").alias(\"Topics\"), $\"eBook Packages\")\n","//df6.show(false)\n","df6.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"4e47cb3d","metadata":{"id":"4e47cb3d","outputId":"be910bc2-ab08-42d5-a689-42690c2d3d34"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+----------------------+---------------------------------------+\n","|Afirstname|Alastname|DOI                                      |Efirstname|Elastname|Hardcover ISBN|Softcover ISBN   |eBook ISBN       |Id |Publisher          |Book Subtitle                                                                                          |Book Title              |Topics                |eBook Packages                         |\n","+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+----------------------+---------------------------------------+\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language  |Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language  |Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language  |Apress Access Books                    |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source           |Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source           |Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source           |Apress Access Books                    |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques|Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques|Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques|Apress Access Books                    |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Big Data              |Professional and Applied Computing     |\n","+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+----------------------+---------------------------------------+\n","only showing top 10 rows\n","\n"]},{"data":{"text/plain":["df7: org.apache.spark.sql.DataFrame = [Afirstname: string, Alastname: string ... 12 more fields]\n"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["val df7=df6.select(col(\"Afirstname\"),col(\"Alastname\"),col(\"DOI\"),col(\"Efirstname\"), col(\"Elastname\"),col(\"Hardcover ISBN\"),col(\"Softcover ISBN\"),col(\"eBook ISBN\"),col(\"Id\"),col(\"Publisher\"),col(\"Book Subtitle\"),col(\"Book Title\"),col(\"Topics\"),explode_outer( $\"eBook Packages\").alias(\"eBook Packages\"))\n","df7.show(10, false)"]},{"cell_type":"code","execution_count":null,"id":"229614fe","metadata":{"id":"229614fe","outputId":"a74c8dc9-ad7f-43bd-9e31-681ecb94d7df"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Afirstname: string (nullable = true)\n"," |-- Alastname: string (nullable = true)\n"," |-- DOI: string (nullable = true)\n"," |-- Efirstname: string (nullable = true)\n"," |-- Elastname: string (nullable = true)\n"," |-- Hardcover ISBN: string (nullable = true)\n"," |-- Softcover ISBN: string (nullable = true)\n"," |-- eBook ISBN: string (nullable = true)\n"," |-- Id: long (nullable = true)\n"," |-- Publisher: string (nullable = true)\n"," |-- Book Subtitle: string (nullable = true)\n"," |-- Book Title: string (nullable = true)\n"," |-- Topics: string (nullable = true)\n"," |-- eBook Packages: string (nullable = true)\n","\n"]}],"source":["df7.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"a9021d80","metadata":{"id":"a9021d80","outputId":"b5c6dd5b-059f-423d-9709-6b61d5346e52"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+-----------------------------------+---------------------------------------+\n","|Afirstname|Alastname|DOI                                      |Efirstname|Elastname|Hardcover ISBN|Softcover ISBN   |eBook ISBN       |Id |Publisher          |Book Subtitle                                                                                          |Book Title              |Topics                             |eBook Packages                         |\n","+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+-----------------------------------+---------------------------------------+\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language               |Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language               |Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Language               |Apress Access Books                    |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source                        |Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source                        |Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Open Source                        |Apress Access Books                    |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques             |Professional and Applied Computing     |\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques             |Professional and Applied Computing (R0)|\n","|Adam      | Freeman |https://doi.org/10.1007/978-1-4842-7355-5|null      |null     |null          |978-1-4842-7354-8|978-1-4842-7355-5|1  |Apress Berkeley, CA|The Complete Guide to Programming Reliable and Efficient Software Using Golang                         |Pro Go                  |Programming Techniques             |Apress Access Books                    |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Big Data                           |Professional and Applied Computing     |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Big Data                           |Professional and Applied Computing (R0)|\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Big Data                           |Apress Access Books                    |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Java                               |Professional and Applied Computing     |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Java                               |Professional and Applied Computing (R0)|\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Java                               |Apress Access Books                    |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Data Mining and Knowledge Discovery|Professional and Applied Computing     |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Data Mining and Knowledge Discovery|Professional and Applied Computing (R0)|\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Data Mining and Knowledge Discovery|Apress Access Books                    |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Open Source                        |Professional and Applied Computing     |\n","|Hien      |Luu      |https://doi.org/10.1007/978-1-4842-3579-9|null      |null     |null          |null             |978-1-4842-3579-9|2  |Apress Berkeley, CA|With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library|Beginning Apache Spark 2|Open Source                        |Professional and Applied Computing (R0)|\n","+----------+---------+-----------------------------------------+----------+---------+--------------+-----------------+-----------------+---+-------------------+-------------------------------------------------------------------------------------------------------+------------------------+-----------------------------------+---------------------------------------+\n","only showing top 20 rows\n","\n"]},{"data":{"text/plain":["df8: org.apache.spark.sql.DataFrame = [Afirstname: string, Alastname: string ... 12 more fields]\n"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["val df8=df7.select(\"*\")\n","df8.show(false)"]},{"cell_type":"markdown","id":"00896081","metadata":{"id":"00896081"},"source":["**Chapter 4. Read and Write CSV Files with Spark**"]},{"cell_type":"code","execution_count":null,"id":"318813f1","metadata":{"id":"318813f1","outputId":"fae8a78d-0aae-404e-a5d8-8219e83eb733"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|                 _c0|\n","+--------------------+\n","|Name;Surname;Cent...|\n","|Gonzalo;de Berceo...|\n","| Juan ;Ruiz;XIV;1283|\n","|Fernando;de Rojas...|\n","|Garcilaso;de la V...|\n","+--------------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["PATH: String = Downloads/Spanish_Writers_by_Century_II.csv\n","df0: org.apache.spark.sql.DataFrame = [_c0: string]\n"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["// The default delimiter is \",\" for CSV files\n","val PATH =\"Downloads/Spanish_Writers_by_Century_II.csv\"\n","\n","val df0 = spark.read.csv(PATH)\n","df0.show(5)"]},{"cell_type":"code","execution_count":null,"id":"8d3707ad","metadata":{"id":"8d3707ad","outputId":"b36227e9-e700-4ecb-b738-fa785c444a52"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----------+-------+-----------+\n","|      _c0|       _c1|    _c2|        _c3|\n","+---------+----------+-------+-----------+\n","|     Name|   Surname|Century|YearOfBirth|\n","|  Gonzalo| de Berceo|   XIII|       1196|\n","|    Juan |      Ruiz|    XIV|       1283|\n","| Fernando|  de Rojas|     XV|       1465|\n","|Garcilaso|de la Vega|    XVI|       1539|\n","+---------+----------+-------+-----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["df1: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 2 more fields]\n"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["// We can take advantage of the option() function to specify a field's delimiter.\n","val df1 = spark.read.option(\"delimiter\", \";\").csv(PATH)\n","df1.show(5)"]},{"cell_type":"code","execution_count":null,"id":"cb2beef7","metadata":{"id":"cb2beef7","outputId":"8fc8a16f-7591-4a7e-dae5-239643a06391"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+------------+-------+-----------+\n","|     Name|     Surname|Century|YearOfBirth|\n","+---------+------------+-------+-----------+\n","|  Gonzalo|   de Berceo|   XIII|       1196|\n","|    Juan |        Ruiz|    XIV|       1283|\n","| Fernando|    de Rojas|     XV|       1465|\n","|Garcilaso|  de la Vega|    XVI|       1539|\n","|   Miguel|de Cervantes|    XVI|       1547|\n","+---------+------------+-------+-----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["df2: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["// To skip the first line and use it as column names we can use .option(\"header\", \"true\")\n","val df2 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(PATH)\n","df2.show(5)"]},{"cell_type":"code","execution_count":null,"id":"f9ccd080","metadata":{"id":"f9ccd080","outputId":"14991d09-2d63-441c-b7fe-cd2f1216a2c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+------------+-------+-----------+\n","|     Name|     Surname|Century|YearOfBirth|\n","+---------+------------+-------+-----------+\n","|  Gonzalo|   de Berceo|   XIII|       1196|\n","|    Juan |        Ruiz|    XIV|       1283|\n","| Fernando|    de Rojas|     XV|       1465|\n","|Garcilaso|  de la Vega|    XVI|       1539|\n","|   Miguel|de Cervantes|    XVI|       1547|\n","+---------+------------+-------+-----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["df3: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["// You can also use options() to use multiple options\n","val df3 = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\";\", \"header\"->\"true\")).csv(PATH)\n","df3.show(5)"]},{"cell_type":"code","execution_count":null,"id":"7d7f1d52","metadata":{"id":"7d7f1d52","outputId":"98a62cb3-e53f-4f36-c4da-1cc2984d91ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+------------+-------+-----------+\n","|     Name|     Surname|Century|YearOfBirth|\n","+---------+------------+-------+-----------+\n","|  Gonzalo|   de Berceo|   XIII|       1196|\n","|    Juan |        Ruiz|    XIV|       1283|\n","| Fernando|    de Rojas|     XV|       1465|\n","|Garcilaso|  de la Vega|    XVI|       1539|\n","|   Miguel|de Cervantes|    XVI|       1547|\n","+---------+------------+-------+-----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["GZIP_PATH: String = Downloads/Spanish_Writers_by_Century_II.csv.gz\n","df5: org.apache.spark.sql.DataFrame = [Name: string, Surname: string ... 2 more fields]\n"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["val GZIP_PATH = \"Downloads/Spanish_Writers_by_Century_II.csv.gz\"\n","\n","val df5 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").option(\"compression\", \"gzip\").csv(GZIP_PATH)\n","df5.show(5)"]},{"cell_type":"code","execution_count":null,"id":"38910cb2","metadata":{"id":"38910cb2"},"outputs":[],"source":["// To save a DataFrame to a CSV file\n","\n","OUTPUT_PATH=\"Downloads/\"\n","\n","df5.write.option(\"header\",\"true\").csv(OUTPUT_PATH)"]},{"cell_type":"markdown","id":"fda2bb35","metadata":{"id":"fda2bb35"},"source":["**Chapter 4. Hive Tables**"]},{"cell_type":"code","execution_count":null,"id":"b1de7a31","metadata":{"id":"b1de7a31"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a9919bef","metadata":{"id":"a9919bef"},"source":["**Chapter 4. Use of Spark DataFrames**"]},{"cell_type":"markdown","id":"97341f95","metadata":{"id":"97341f95"},"source":["*Data Sources*\n","* https://www.kaggle.com/datasets/abecklas/fifa-world-cup?select=WorldCups.csv\n"]},{"cell_type":"code","execution_count":null,"id":"69b1b6be","metadata":{"id":"69b1b6be","outputId":"6f57cf0f-2581-4268-eb15-37a682222acf"},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Spark Web UI available at http://192.168.2.38:4045\n","SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1669988580258)\n","SparkSession available as 'spark'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["dfWC: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 8 more fields]\n"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","val dfWC=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/WorldCups.csv\")"]},{"cell_type":"code","execution_count":null,"id":"1bb2178b","metadata":{"id":"1bb2178b","outputId":"da175960-3576-49b1-827f-a12f730ad366"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13|           18|   590.549|\n","|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16|           17|   363.000|\n","|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15|           18|   375.700|\n","|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|           22| 1.045.246|\n","|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16|           26|   768.607|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["// The show() function without parameters displays 20 rows and truncates the text length to 20 characters by default.\n","dfWC.show(5)\n"]},{"cell_type":"code","execution_count":null,"id":"03e21e3a","metadata":{"id":"03e21e3a","outputId":"7cea9ce2-29d2-43f3-cff9-8f8bb64a0ca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+--------+--------+----------+-------+--------+-----------+--------------+-------------+----------+\n","|Year| Country|  Winner|Runners-Up|  Third|  Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+--------+--------+----------+-------+--------+-----------+--------------+-------------+----------+\n","|1930| Uruguay| Uruguay|  Argen...|    USA|Yugos...|         70|            13|           18|   590.549|\n","|1934|   Italy|   Italy|  Czech...|Germany| Austria|         70|            16|           17|   363.000|\n","|1938|  France|   Italy|   Hungary| Brazil|  Sweden|         84|            15|           18|   375.700|\n","|1950|  Brazil| Uruguay|    Brazil| Sweden|   Spain|         88|            13|           22|  1.045...|\n","|1954|Switz...|Germa...|   Hungary|Austria| Uruguay|        140|            16|           26|   768.607|\n","+----+--------+--------+----------+-------+--------+-----------+--------------+-------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["dfWC.show(5,8)"]},{"cell_type":"code","execution_count":null,"id":"4a39e078","metadata":{"id":"4a39e078","outputId":"97f73f1c-4868-4c57-c848-8410b988022b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+-------+----------+-------+-------+-----------+--------------+-------------+----------+\n","|Year|Country| Winner|Runners-Up|  Third| Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-------+-------+----------+-------+-------+-----------+--------------+-------------+----------+\n","|1930|Uruguay|Uruguay|   Arge...|    USA|Yugo...|         70|            13|           18|   590.549|\n","|1934|  Italy|  Italy|   Czec...|Germany|Austria|         70|            16|           17|   363.000|\n","|1938| France|  Italy|   Hungary| Brazil| Sweden|         84|            15|           18|   375.700|\n","|1950| Brazil|Uruguay|    Brazil| Sweden|  Spain|         88|            13|           22|   1.04...|\n","|1954|Swit...|Germ...|   Hungary|Austria|Uruguay|        140|            16|           26|   768.607|\n","+----+-------+-------+----------+-------+-------+-----------+--------------+-------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["// Select all the columns from a DataFrame\n","dfWC.select(\"*\").show(5, 7)"]},{"cell_type":"code","execution_count":null,"id":"2149b4fb","metadata":{"scrolled":true,"id":"2149b4fb","outputId":"72bd6f29-cba1-4f59-cfa0-bd4c13591d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["// Fetch specific columns from a DataFrame using column names\n","\n","dfWC.select(\"Year\",\"Country\", \"Winner\", \"Runners-Up\", \"Third\",\"Fourth\").show(5, false)\n"]},{"cell_type":"code","execution_count":null,"id":"e820d000","metadata":{"id":"e820d000","outputId":"cd761923-6d77-4ef3-f024-8058226194db"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["//Fetch individual columns from a DataFrame using Dataframe object name\n","dfWC.select(dfWC(\"Year\"),dfWC(\"Country\"),dfWC(\"Winner\"),dfWC(\"Runners-Up\"),dfWC(\"Third\"),dfWC(\"Fourth\")).show(5, false)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d0bea887","metadata":{"id":"d0bea887","outputId":"da67334a-dd91-44f0-b330-ce800db87c27"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.functions.col\n"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["//Fetch individual columns from a DataFrame using col function.\n","\n","import org.apache.spark.sql.functions.col\n","dfWC.select(col(\"Year\"),col(\"Country\"),col(\"Winner\"),col(\"Runners-Up\"),col(\"Third\"),col(\"Fourth\")).show(5, false)"]},{"cell_type":"code","execution_count":null,"id":"a3e4d3c0","metadata":{"id":"a3e4d3c0","outputId":"92230ddd-1542-4644-fc70-b009695ad2a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+\n","|Year|Country    |Winner    |Runners-Up    |\n","+----+-----------+----------+--------------+\n","|1930|Uruguay    |Uruguay   |Argentina     |\n","|1934|Italy      |Italy     |Czechoslovakia|\n","|1938|France     |Italy     |Hungary       |\n","|1950|Brazil     |Uruguay   |Brazil        |\n","|1954|Switzerland|Germany FR|Hungary       |\n","+----+-----------+----------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["// Select columns from DataFrame based on column index\n","dfWC.select(dfWC.columns(0),dfWC.columns(1),dfWC.columns(2),dfWC.columns(3)).show(5, false)"]},{"cell_type":"markdown","id":"a1d76ef7","metadata":{"id":"a1d76ef7"},"source":["*Using a sequence of columns*"]},{"cell_type":"code","execution_count":null,"id":"7acca779","metadata":{"id":"7acca779","outputId":"639860d6-fb67-4899-c684-dcf59516e308"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["colIndex: Seq[Int] = List(0, 1, 2, 3, 4, 5)\n"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["// Select columns from DataFrame based on column index\n","val colIndex = Seq(0, 1, 2, 3, 4, 5)\n","dfWC.select(colIndex map dfWC.columns map col: _*).show(5, false)"]},{"cell_type":"code","execution_count":null,"id":"67505e65","metadata":{"id":"67505e65","outputId":"590f6448-a7ba-4f28-9453-b966f41071b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["seqColumnas: Seq[String] = List(Year, Country, Winner, Runners-Up, Third, Fourth)\n","result: Unit = ()\n"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["// Using a sequence plus the string column names:\n","val seqColumnas = Seq(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\")\n","val result = dfWC.select(seqColumnas.head, seqColumnas.tail: _*).show(5, false)"]},{"cell_type":"code","execution_count":null,"id":"c5096342","metadata":{"id":"c5096342","outputId":"6c571751-cc5e-480b-da61-bb781e8fcc5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+\n","|Year|Country    |Winner    |Runners-Up    |Third  |Fourth    |\n","+----+-----------+----------+--------------+-------+----------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA    |Yugoslavia|\n","|1934|Italy      |Italy     |Czechoslovakia|Germany|Austria   |\n","|1938|France     |Italy     |Hungary       |Brazil |Sweden    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden |Spain     |\n","|1954|Switzerland|Germany FR|Hungary       |Austria|Uruguay   |\n","+----+-----------+----------+--------------+-------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["// Using a sequence plus map Column name objects:\n","dfWC.select(seqColumnas.map(i => col(i)): _*).show(5,false)"]},{"cell_type":"markdown","id":"417cc92e","metadata":{"id":"417cc92e"},"source":["*Using a list of columns*"]},{"cell_type":"code","execution_count":null,"id":"6cadc166","metadata":{"id":"6cadc166","outputId":"e4ca43a9-fad6-4b38-89f8-1a23d8768bde"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+\n","|Year|Country    |Winner    |\n","+----+-----------+----------+\n","|1930|Uruguay    |Uruguay   |\n","|1934|Italy      |Italy     |\n","|1938|France     |Italy     |\n","|1950|Brazil     |Uruguay   |\n","|1954|Switzerland|Germany FR|\n","+----+-----------+----------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.Column\n","miColumnas: List[org.apache.spark.sql.Column] = List(Year, Country, Winner)\n"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.Column\n","\n","val miColumnas: List[Column] = List(new Column(\"Year\"), new Column(\"Country\"), new Column(\"Winner\"))\n","\n","dfWC.select(miColumnas: _*).show(5,false)\n"]},{"cell_type":"markdown","id":"0caa5fc2","metadata":{"id":"0caa5fc2"},"source":["*Select Columns Based on Name Patterns, startsWith(String prefix) and endsWith(String suffix)*"]},{"cell_type":"code","execution_count":null,"id":"adbb7d7a","metadata":{"id":"adbb7d7a","outputId":"f1f953cc-8d53-4dd1-eb60-340e6d1e5540"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+\n","|Year|\n","+----+\n","|1930|\n","|1934|\n","|1938|\n","|1950|\n","|1954|\n","+----+\n","only showing top 5 rows\n","\n"]}],"source":["dfWC.select(dfWC.columns.filter(s=>s.startsWith(\"Y\")).map(c=>col(c)):_*).show(5,false)"]},{"cell_type":"code","execution_count":null,"id":"4eecc2e8","metadata":{"id":"4eecc2e8","outputId":"389c039d-87fd-4bda-d844-a8081c7cdd34"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|Winner    |\n","+----------+\n","|Uruguay   |\n","|Italy     |\n","|Italy     |\n","|Uruguay   |\n","|Germany FR|\n","+----------+\n","only showing top 5 rows\n","\n"]}],"source":["dfWC.select(dfWC.columns.filter(s=>s.endsWith(\"ner\")).map(c=>col(c)):_*).show(5,false)"]},{"cell_type":"code","execution_count":null,"id":"ab5a7766","metadata":{"id":"ab5a7766","outputId":"bd73044e-5099-4604-de79-3c99bdd9e224"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+\n","|Year|     Country|Winner| Runners-Up|  Third| Fourth|\n","+----+------------+------+-----------+-------+-------+\n","|2010|South Africa| Spain|Netherlands|Germany|Uruguay|\n","+----+------------+------+-----------+-------+-------+\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.functions.col\n"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["// Filter all rows that contains string 'mes' in a 'name' column\n","import org.apache.spark.sql.functions.col\n","\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\").filter(col(\"Winner\").contains(\"S\")).show()\n"]},{"cell_type":"code","execution_count":null,"id":"ec195740","metadata":{"id":"ec195740","outputId":"a21dc0db-0a55-4e61-b79e-55f9cb85b00e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+\n","|Year|     Country|Winner| Runners-Up|  Third| Fourth|\n","+----+------------+------+-----------+-------+-------+\n","|2010|South Africa| Spain|Netherlands|Germany|Uruguay|\n","+----+------------+------+-----------+-------+-------+\n","\n"]}],"source":["// Use of filter with like to get the same result\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\").filter(col(\"Winner\").like(\"%S%\")).show()"]},{"cell_type":"code","execution_count":null,"id":"347188de","metadata":{"id":"347188de","outputId":"cbbcea2e-e9fa-46d0-a381-68d036858dbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+\n","|Year|     Country|Winner| Runners-Up|  Third| Fourth|\n","+----+------------+------+-----------+-------+-------+\n","|2010|South Africa| Spain|Netherlands|Germany|Uruguay|\n","+----+------------+------+-----------+-------+-------+\n","\n"]}],"source":["// We can also use SQL ANSI languege to filter rows\n","dfWC.createOrReplaceTempView(\"WorldCups\")\n","spark.sql(\"select Year,Country,Winner,`Runners-Up`,Third,Fourth from WorldCups where Winner like '%S%'\").show()"]},{"cell_type":"markdown","id":"3dd945c6","metadata":{"id":"3dd945c6"},"source":["*Use of Filter and Where to refine the results of query*"]},{"cell_type":"code","execution_count":null,"id":"e22cfa2e","metadata":{"id":"e22cfa2e","outputId":"c39a5428-fdda-4fd1-a4ac-557fefaf9ad3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+-------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|Year|Country|Winner |Runners-Up    |Third  |Fourth    |GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-------+-------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|1930|Uruguay|Uruguay|Argentina     |USA    |Yugoslavia|70         |13            |18           |590.549   |\n","|1934|Italy  |Italy  |Czechoslovakia|Germany|Austria   |70         |16            |17           |363.000   |\n","+----+-------+-------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","\n"]}],"source":["dfWC.filter(\"Year < 1938\").show(5,false)"]},{"cell_type":"code","execution_count":null,"id":"3cc1a405","metadata":{"id":"3cc1a405","outputId":"6011e3ff-e18c-4315-94a8-32e511f53708"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+-------+--------------+-------+----------+\n","|Year|Country|Winner |Runners-Up    |Third  |Fourth    |\n","+----+-------+-------+--------------+-------+----------+\n","|1930|Uruguay|Uruguay|Argentina     |USA    |Yugoslavia|\n","|1934|Italy  |Italy  |Czechoslovakia|Germany|Austria   |\n","+----+-------+-------+--------------+-------+----------+\n","\n"]}],"source":["dfWC.select(col(\"Year\"),col(\"Country\"),col(\"Winner\"),col(\"Runners-Up\"),col(\"Third\"),col(\"Fourth\")).filter(\"Year < 1938\").show(5,false)\n"]},{"cell_type":"code","execution_count":null,"id":"6bc16878","metadata":{"id":"6bc16878","outputId":"5631d135-9580-4ad9-f3d8-3c234c38bac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+------+--------------+\n","|Year|Country|Winner|Runners-Up    |\n","+----+-------+------+--------------+\n","|1934|Italy  |Italy |Czechoslovakia|\n","+----+-------+------+--------------+\n","\n"]}],"source":["dfWC.select(col(\"Year\"),col(\"Country\"),col(\"Winner\"),col(\"Runners-Up\")).filter(\"Year < 1938\").filter(\"Country = 'Italy'\").show(5,false)"]},{"cell_type":"code","execution_count":null,"id":"e6d396ba","metadata":{"id":"e6d396ba","outputId":"1c4d3402-4e1f-41e1-c737-9ad47fe32275"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+----------+--------------+-------+-------+-----------+--------------+-------------+----------+\n","|Year|Country|    Winner|    Runners-Up|  Third| Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-------+----------+--------------+-------+-------+-----------+--------------+-------------+----------+\n","|1934|  Italy|     Italy|Czechoslovakia|Germany|Austria|         70|            16|           17|   363.000|\n","|1990|  Italy|Germany FR|     Argentina|  Italy|England|        115|            24|           52| 2.516.215|\n","+----+-------+----------+--------------+-------+-------+-----------+--------------+-------------+----------+\n","\n"]}],"source":["dfWC.where(\"Country = 'Italy'\").show()"]},{"cell_type":"markdown","id":"8ffb4f7a","metadata":{"id":"8ffb4f7a"},"source":["Using `col(ColumName)`, `$\"ColumName\"` and `DataFrame(\"ColumName\")` to refer to column names while working with DataFrames and use of `“===”` for comparison."]},{"cell_type":"code","execution_count":null,"id":"c0922efb","metadata":{"id":"c0922efb","outputId":"1d307e44-c97d-4e58-9be8-b952024af985"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter('Winner === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"0d34dda4","metadata":{"id":"0d34dda4","outputId":"c1e13a6a-6a81-4b5d-e28c-ca12fbd00bfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter($\"Winner\" === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"4d5f6788","metadata":{"id":"4d5f6788","outputId":"b2069e27-5ae7-4fa9-d0e4-590b4969ddb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(col(\"Winner\") === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"ad9ca050","metadata":{"id":"ad9ca050","outputId":"b515c87f-032d-4115-eb63-ee1416652adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(dfWC(\"Winner\") === \"Spain\").show(false)\n"]},{"cell_type":"markdown","id":"1be0a150","metadata":{"id":"1be0a150"},"source":["*Use of where()*"]},{"cell_type":"code","execution_count":null,"id":"2e06cce5","metadata":{"id":"2e06cce5","outputId":"ca090936-4d2e-44d1-e0a2-3884d2350cc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").where('Winner === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"326b526e","metadata":{"id":"326b526e","outputId":"ec20a4ba-e332-4407-fa0f-98a1d1dc6847"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").where($\"Winner\" === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"5e79bf3e","metadata":{"id":"5e79bf3e","outputId":"2865274e-2db0-4ec0-969b-413251f22f0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").where(col(\"Winner\") === \"Spain\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"9cdbcb2d","metadata":{"id":"9cdbcb2d","outputId":"ca23dbf8-3317-4f71-f59a-9f899b944016"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").where(dfWC(\"Winner\") === \"Spain\").show(false)\n"]},{"cell_type":"markdown","id":"b087bae9","metadata":{"id":"b087bae9"},"source":["* filter() Spark DataFrame based on multiple conditions using AND, OR, and NOT. Equivalent &&\", \"||\" and \"!\" *"]},{"cell_type":"code","execution_count":null,"id":"fdeb0daf","metadata":{"id":"fdeb0daf","outputId":"d12d90ed-205e-4091-bab3-74495047d8bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["// Using AND, \"&&\" logical operator\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(dfWC(\"Winner\") === \"Spain\" and dfWC(\"Runners-Up\") === \"Netherlands\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"76701c06","metadata":{"id":"76701c06","outputId":"0710cd3e-6a47-4c77-b105-a6e33c0a87c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner|Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+------+-----------+-------+-------+-----------+\n","|2010|South Africa|Spain |Netherlands|Germany|Uruguay|145        |\n","+----+------------+------+-----------+-------+-------+-----------+\n","\n"]}],"source":["// Using AND, \"&&\" logical operator\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(dfWC(\"Winner\") === \"Spain\" && dfWC(\"Runners-Up\") === \"Netherlands\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"ab428f99","metadata":{"id":"ab428f99","outputId":"71272aa6-29e6-42ae-8e47-32d55e36d057"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+----------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner    |Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+----------+-----------+-------+-------+-----------+\n","|1974|Germany     |Germany FR|Netherlands|Poland |Brazil |97         |\n","|1978|Argentina   |Argentina |Netherlands|Brazil |Italy  |102        |\n","|2010|South Africa|Spain     |Netherlands|Germany|Uruguay|145        |\n","+----+------------+----------+-----------+-------+-------+-----------+\n","\n"]}],"source":["// Using OR, “||” logical operator\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(dfWC(\"Winner\") === \"Spain\" or dfWC(\"Runners-Up\") === \"Netherlands\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"a40a6b3a","metadata":{"id":"a40a6b3a","outputId":"c92776e8-a4d3-4f43-aa77-5fe5f3fb8879"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------------+----------+-----------+-------+-------+-----------+\n","|Year|Country     |Winner    |Runners-Up |Third  |Fourth |GoalsScored|\n","+----+------------+----------+-----------+-------+-------+-----------+\n","|1974|Germany     |Germany FR|Netherlands|Poland |Brazil |97         |\n","|1978|Argentina   |Argentina |Netherlands|Brazil |Italy  |102        |\n","|2010|South Africa|Spain     |Netherlands|Germany|Uruguay|145        |\n","+----+------------+----------+-----------+-------+-------+-----------+\n","\n"]}],"source":["// Using OR, “||” logical operator\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\",\"GoalsScored\").filter(dfWC(\"Winner\") === \"Spain\" || dfWC(\"Runners-Up\") === \"Netherlands\").show(false)\n"]},{"cell_type":"code","execution_count":null,"id":"dec4415d","metadata":{"id":"dec4415d","outputId":"2b00b71d-84ef-4b4a-e616-63744281a718"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-----------+--------------+\n","|Year|Country    |Winner    |Runners-Up    |Third      |Fourth        |\n","+----+-----------+----------+--------------+-----------+--------------+\n","|1930|Uruguay    |Uruguay   |Argentina     |USA        |Yugoslavia    |\n","|1950|Brazil     |Uruguay   |Brazil        |Sweden     |Spain         |\n","|1954|Switzerland|Germany FR|Hungary       |Austria    |Uruguay       |\n","|1958|Sweden     |Brazil    |Sweden        |France     |Germany FR    |\n","|1962|Chile      |Brazil    |Czechoslovakia|Chile      |Yugoslavia    |\n","|1966|England    |England   |Germany FR    |Portugal   |Soviet Union  |\n","|1970|Mexico     |Brazil    |Italy         |Germany FR |Uruguay       |\n","|1986|Mexico     |Argentina |Germany FR    |France     |Belgium       |\n","|1990|Italy      |Germany FR|Argentina     |Italy      |England       |\n","|1994|USA        |Brazil    |Italy         |Sweden     |Bulgaria      |\n","|1998|France     |France    |Brazil        |Croatia    |Netherlands   |\n","|2002|Korea/Japan|Brazil    |Germany       |Turkey     |Korea Republic|\n","|2014|Brazil     |Germany   |Argentina     |Netherlands|Brazil        |\n","+----+-----------+----------+--------------+-----------+--------------+\n","\n"]}],"source":["// Using NOT, “!” logical operator\n","dfWC.select(\"Year\",\"Country\",\"Winner\",\"Runners-Up\",\"Third\",\"Fourth\").filter(not (dfWC(\"Winner\") === \"Italy\" or dfWC(\"Runners-Up\") === \"Netherlands\")).show(false)\n"]},{"cell_type":"markdown","id":"491136a9","metadata":{"id":"491136a9"},"source":["**Chapter 4. Manipulating Spark DataFrame Columns**"]},{"cell_type":"code","execution_count":null,"id":"bc6274d7","metadata":{"id":"bc6274d7","outputId":"059f3b9a-d966-4f8c-ea5f-f049db2e7e6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13|           18|   590.549|\n","|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16|           17|   363.000|\n","|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15|           18|   375.700|\n","|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|           22| 1.045.246|\n","|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16|           26|   768.607|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["dfWC.select(\"*\").show(5)"]},{"cell_type":"code","execution_count":null,"id":"5aba4680","metadata":{"id":"5aba4680","outputId":"6fa15dfb-3696-4b4c-b222-9b222dee05ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Year: string (nullable = true)\n"," |-- Country: string (nullable = true)\n"," |-- Winner: string (nullable = true)\n"," |-- Runners-Up: string (nullable = true)\n"," |-- Third: string (nullable = true)\n"," |-- Fourth: string (nullable = true)\n"," |-- GoalsScored: string (nullable = true)\n"," |-- QualifiedTeams: string (nullable = true)\n"," |-- MatchesPlayed: string (nullable = true)\n"," |-- Attendance: string (nullable = true)\n","\n"]}],"source":["dfWC.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"5410abe8","metadata":{"id":"5410abe8","outputId":"45360525-fcbc-41a3-a673-ebf71d6e45d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n","|Year|Country| Winner|Runners-Up| Third|Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n","+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n","|1950| Brazil|Uruguay|    Brazil|Sweden| Spain|         88|            13|           22| 1.045.246|\n","+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n","\n"]}],"source":["dfWC.select(\"*\").filter(col(\"Attendance\") === \"1.045.246\").show()"]},{"cell_type":"code","execution_count":null,"id":"0ece1e34","metadata":{"id":"0ece1e34","outputId":"48405994-e9b4-4b3e-8e3d-163369444d90"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|Attendance|\n","+----------+\n","|    590549|\n","|    363000|\n","|    375700|\n","|   1045246|\n","|    768607|\n","|    819810|\n","|    893172|\n","|   1563135|\n","|   1603975|\n","|   1865753|\n","|   1545791|\n","|   2109723|\n","|   2394031|\n","|   2516215|\n","|   3587538|\n","|   2785100|\n","|   2705197|\n","|   3359439|\n","|   3178856|\n","|   3386810|\n","+----------+\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.functions.regexp_replace\n","import org.apache.spark.sql.functions.col\n","dfWC2: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 8 more fields]\n"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["// Modifying data format to transform it into numeric\n","\n","import org.apache.spark.sql.functions.regexp_replace\n","import org.apache.spark.sql.functions.col\n","\n","val dfWC2=dfWC.withColumn(\"Attendance\", regexp_replace($\"Attendance\", \"\\\\.\", \"\"))\n","dfWC2.select(\"Attendance\").show()"]},{"cell_type":"code","execution_count":null,"id":"7e342c72","metadata":{"id":"7e342c72","outputId":"7b413ff3-e0f1-4fd1-caa1-13f2ed3355d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Year: string (nullable = true)\n"," |-- Country: string (nullable = true)\n"," |-- Winner: string (nullable = true)\n"," |-- Runners-Up: string (nullable = true)\n"," |-- Third: string (nullable = true)\n"," |-- Fourth: string (nullable = true)\n"," |-- GoalsScored: integer (nullable = true)\n"," |-- QualifiedTeams: integer (nullable = true)\n"," |-- MatchesPlayed: integer (nullable = true)\n"," |-- Attendance: integer (nullable = true)\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.types.IntegerType\n","dfWC3: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 8 more fields]\n"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.types.IntegerType\n","\n","val dfWC3=dfWC2.withColumn(\"GoalsScored\", col(\"GoalsScored\").cast(IntegerType))\n",".withColumn(\"QualifiedTeams\", col(\"QualifiedTeams\").cast(IntegerType))\n",".withColumn(\"MatchesPlayed\", col(\"MatchesPlayed\").cast(IntegerType))\n",".withColumn(\"Attendance\", col(\"Attendance\").cast(IntegerType))\n","dfWC3.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"792853e3","metadata":{"id":"792853e3","outputId":"dc0d7904-04cd-48da-b95b-0ae1d7f80290"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-------------+------------------+\n","|Attendance|MatchesPlayed|AttendancePerMatch|\n","+----------+-------------+------------------+\n","|    590549|           18|         32808.278|\n","|    363000|           17|         21352.941|\n","|    375700|           18|         20872.222|\n","|   1045246|           22|         47511.182|\n","|    768607|           26|         29561.808|\n","+----------+-------------+------------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfWCExt: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 9 more fields]\n"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["// Adding a new column to the DataFrame fans attendance per match played\n","\n","val dfWCExt=dfWC3.withColumn(\"AttendancePerMatch\", round(col(\"Attendance\").cast(IntegerType)/col(\"MatchesPlayed\").cast(IntegerType), 3))\n","\n","dfWCExt.select(\"Attendance\",\"MatchesPlayed\",\"AttendancePerMatch\").show(5)\n"]},{"cell_type":"markdown","id":"5e0e7cb7","metadata":{"id":"5e0e7cb7"},"source":["**Chapter 4. Renaming DataFrame Columns**"]},{"cell_type":"code","execution_count":null,"id":"9da7a626","metadata":{"id":"9da7a626","outputId":"c4f98ced-b202-431f-a003-c470e4732368"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-------------+---------+\n","|Attendance|MatchesPlayed|  AxMatch|\n","+----------+-------------+---------+\n","|    590549|           18|32808.278|\n","|    363000|           17|21352.941|\n","|    375700|           18|20872.222|\n","|   1045246|           22|47511.182|\n","|    768607|           26|29561.808|\n","+----------+-------------+---------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfRenamed: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 9 more fields]\n"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["val dfRenamed = dfWCExt.withColumnRenamed(\"AttendancePerMatch\",\"AxMatch\")\n","dfRenamed.select(\"Attendance\", \"MatchesPlayed\",\"AxMatch\").show(5)"]},{"cell_type":"code","execution_count":null,"id":"e93e9bb8","metadata":{"id":"e93e9bb8","outputId":"9aecc1ae-c8fc-4093-fde5-aa16dd812043"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------------+----------------+\n","|    Att|MatchesPlayed|AttendancexMatch|\n","+-------+-------------+----------------+\n","| 590549|           18|       32808.278|\n","| 363000|           17|       21352.941|\n","| 375700|           18|       20872.222|\n","|1045246|           22|       47511.182|\n","| 768607|           26|       29561.808|\n","+-------+-------------+----------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfRenamed2: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 9 more fields]\n"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["// Renaming several columns at onces joining calls to withColumnRenamed()\n","\n","val dfRenamed2 = dfRenamed\n",".withColumnRenamed(\"Attendance\",\"Att\")\n",".withColumnRenamed(\"AxMatch\",\"AttendancexMatch\")\n","\n","dfRenamed2.select(\"Att\", \"MatchesPlayed\",\"AttendancexMatch\").show(5)"]},{"cell_type":"markdown","id":"e80cc9f0","metadata":{"id":"e80cc9f0"},"source":["**Chapter 4. Drop DataFrame column(s)**"]},{"cell_type":"markdown","id":"9d53c829","metadata":{"id":"9d53c829"},"source":["* Deleting a single column: dataframe.drop(\"column name\")\n","* Deleting multiple columns: dataframe.drop(*(\"column 1\",\"column 2\",\"column n\"))\n","* Deleting all columns of dataframe: dataframe.drop(*list_of_column names)"]},{"cell_type":"code","execution_count":null,"id":"d0c9887f","metadata":{"id":"d0c9887f","outputId":"ec4b582b-1cb2-4933-fba1-01d4ceb8b54e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n","|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|    Att|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n","|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13|           18| 590549|\n","|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16|           17| 363000|\n","|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15|           18| 375700|\n","|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|           22|1045246|\n","|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16|           26| 768607|\n","+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfFropOne: org.apache.spark.sql.DataFrame = [Year: string, Country: string ... 8 more fields]\n"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["val dfFropOne = dfRenamed2.drop(\"AttendancexMatch\")\n","dfFropOne.show(5)"]},{"cell_type":"code","execution_count":null,"id":"e946b4c6","metadata":{"id":"e946b4c6","outputId":"a9c999a2-572a-4c7e-a1dd-2a34fd86acaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["++\n","||\n","++\n","||\n","||\n","++\n","only showing top 2 rows\n","\n"]},{"data":{"text/plain":["allColumnsList: Array[String] = Array(Year, Country, Winner, Runners-Up, Third, Fourth, GoalsScored, QualifiedTeams, MatchesPlayed, Att, AttendancexMatch)\n","dfFropAll: org.apache.spark.sql.DataFrame = []\n"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["val allColumnsList = dfRenamed2.columns\n","val dfFropAll = dfRenamed2.drop(allColumnsList:_*)\n","dfFropAll.show(2)"]},{"cell_type":"markdown","id":"8d935bc8","metadata":{"id":"8d935bc8"},"source":["**Charter 4. Creating a New Dataframe Column Dependent on Another Column Condition**"]},{"cell_type":"code","execution_count":null,"id":"02c02677","metadata":{"id":"02c02677","outputId":"7f34eece-287c-4faf-c9eb-a4c777f787fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------+---+------+------+\n","|    name|surname|age|gender|salary|\n","+--------+-------+---+------+------+\n","|   Juan |  Bravo| 67|     M| 65000|\n","| Miguel |Rosales| 40|     M| 87000|\n","|Roberto |Morales|  7|     M|     0|\n","|  Maria |  Gomez| 12|     F|     0|\n","|  Vanesa|  Lopez| 25|     F| 72000|\n","+--------+-------+---+------+------+\n","\n"]},{"data":{"text/plain":["import org.apache.spark.sql.functions.{when, _}\n","import spark.sqlContext.implicits._\n","p: List[(String, String, Int, String, Int)] = List((\"Juan \",Bravo,67,M,65000), (\"Miguel \",Rosales,40,M,87000), (\"Roberto \",Morales,7,M,0), (\"Maria \",Gomez,12,F,0), (Vanesa,Lopez,25,F,72000))\n","c: Seq[String] = List(name, surname, age, gender, salary)\n","df: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 3 more fields]\n"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.functions.{when, _}\n","import spark.sqlContext.implicits._\n","\n","val p = List((\"Juan \",\"Bravo\",67,\"M\",65000),\n","                (\"Miguel \",\"Rosales\",40,\"M\",87000),\n","                (\"Roberto \",\"Morales\",7,\"M\",0),\n","                (\"Maria \",\"Gomez\",12,\"F\",0),\n","                (\"Vanesa\",\"Lopez\",25,\"F\",72000))\n","val c = Seq(\"name\",\"surname\",\"age\",\"gender\",\"salary\")\n","\n","val df = spark.createDataFrame(p).toDF(c:_*)\n","df.show(5)"]},{"cell_type":"code","execution_count":null,"id":"3f028fce","metadata":{"id":"3f028fce","outputId":"9b22babb-d5af-41d9-b727-d61be37d93af"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------+---+------+------+--------+\n","|    name|surname|age|gender|salary|   stage|\n","+--------+-------+---+------+------+--------+\n","|   Juan |  Bravo| 67|     M| 65000|   Adult|\n","| Miguel |Rosales| 40|     M| 87000|   Adult|\n","|Roberto |Morales|  7|     M|     0|   Child|\n","|  Maria |  Gomez| 12|     F|     0|Teenager|\n","|  Vanesa|  Lopez| 25|     F| 72000|   Adult|\n","+--------+-------+---+------+------+--------+\n","\n"]},{"data":{"text/plain":["df2: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 4 more fields]\n"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["val df2 = df\n","    .withColumn(\"stage\", when(col(\"age\") < 10,\"Child\")\n","    .when(col(\"age\") >= 10 && col(\"age\") < 18,\"Teenager\")\n","    .otherwise(\"Adult\"))\n","df2.show(5)"]},{"cell_type":"code","execution_count":null,"id":"b72951cb","metadata":{"id":"b72951cb","outputId":"6ede5740-ed19-4537-f1b7-82cd100cd67a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------+---+------+------+--------+\n","|    name|surname|age|gender|salary|   stage|\n","+--------+-------+---+------+------+--------+\n","|   Juan |  Bravo| 67|     M| 65000|   Adult|\n","| Miguel |Rosales| 40|     M| 87000|   Adult|\n","|Roberto |Morales|  7|     M|     0|   Child|\n","|  Maria |  Gomez| 12|     F|     0|Teenager|\n","|  Vanesa|  Lopez| 25|     F| 72000|   Adult|\n","+--------+-------+---+------+------+--------+\n","\n"]},{"data":{"text/plain":["df3: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 4 more fields]\n"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["// Using when() as part of a SQL select statement.\n","val df3 = df.select(col(\"*\"),\n","      expr(\"case when age < '10' then 'Child' \" +\n","                       \"when age >= '10' and age <= '18' then 'Teenager' \" +\n","                       \"else 'Adult' end\").alias(\"stage\"))\n","df3.show()\n"]},{"cell_type":"markdown","id":"0e50e3d2","metadata":{"id":"0e50e3d2"},"source":["*Using when() with null values*"]},{"cell_type":"code","execution_count":null,"id":"9fb9b3be","metadata":{"id":"9fb9b3be","outputId":"3ad88e6c-4435-4a8d-8490-421ac431278f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------+---+------+------+------+\n","|    name|surname|age|gender|salary|ppower|\n","+--------+-------+---+------+------+------+\n","|   Juan |  Bravo| 67|     M| 65000|Medium|\n","| Miguel |Rosales| 40|     M| 87000|  High|\n","|Roberto |Morales|  7|     M|  null|      |\n","|  Maria |  Gomez| 12|     F|  null|      |\n","|  Vanesa|  Lopez| 25|     F| 32000|   Low|\n","+--------+-------+---+------+------+------+\n","\n"]},{"data":{"text/plain":["personas: org.apache.spark.rdd.RDD[(String, String, Int, String, Integer)] = ParallelCollectionRDD[159] at parallelize at <console>:40\n","dfp: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 3 more fields]\n"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["// Using the when() with null values\n","\n","val personas = sc.parallelize(Seq(\n","    (\"Juan \",\"Bravo\",67,\"M\",new Integer(65000) ),\n","    (\"Miguel \",\"Rosales\",40,\"M\",new Integer(87000) ),\n","    (\"Roberto \",\"Morales\",7,\"M\",null.asInstanceOf[Integer] ),\n","    (\"Maria \",\"Gomez\",12,\"F\",null.asInstanceOf[Integer] ),\n","    (\"Vanesa\",\"Lopez\",25,\"F\",new Integer(32000) ))\n",")\n","val dfp = personas.toDF(\"name\",\"surname\",\"age\",\"gender\",\"salary\")\n","\n","// ppower --> purchasing power\n","dfp.withColumn(\"ppower\", when(col(\"salary\") < 40000,\"Low\")\n","               .when(col(\"salary\") >= 40000 && col(\"Salary\") < 70000,\"Medium\")\n","               .when(col(\"salary\").isNull ,\"\")\n","               .otherwise(\"High\")).show()"]},{"cell_type":"markdown","id":"5c5fed6b","metadata":{"id":"5c5fed6b"},"source":["**Chapter 4. User Defined Functions (UDF)**"]},{"cell_type":"code","execution_count":null,"id":"c3c7ae4a","metadata":{"id":"c3c7ae4a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"21ddb7cb","metadata":{"id":"21ddb7cb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"551f0d1f","metadata":{"id":"551f0d1f","outputId":"ed640cbb-6cae-4b25-951d-df4201096d9e"},"outputs":[{"data":{"text/plain":["isAdult: Integer => String\n","isAdultUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4389/0x0000000801d46878@290c47fb,StringType,List(Some(class[value[0]: int])),Some(class[value[0]: string]),None,true,true)\n"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["// Writting the UDF\n","def  isAdult= (age: Integer) => {\n","  if(age >= 18){\n","      \"Adult\"\n","  }\n","    else{\n","        \"No adult\"\n","    }\n","}\n","val isAdultUDF = udf(isAdult)"]},{"cell_type":"code","execution_count":null,"id":"b1507b80","metadata":{"id":"b1507b80","outputId":"7adca2e1-cd54-47a5-de08-75364a2b110a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------+---+------+------+--------+\n","|    name|surname|age|gender|salary|is_adult|\n","+--------+-------+---+------+------+--------+\n","|   Juan |  Bravo| 67|     M| 65000|   Adult|\n","| Miguel |Rosales| 40|     M| 87000|   Adult|\n","|Roberto |Morales|  7|     M|     0|No adult|\n","|  Maria |  Gomez| 12|     F|     0|No adult|\n","|  Vanesa|  Lopez| 25|     F| 72000|   Adult|\n","+--------+-------+---+------+------+--------+\n","\n"]},{"data":{"text/plain":["finalDF: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 4 more fields]\n"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["val finalDF=df.withColumn(\"is_adult\",isAdultUDF(col(\"age\")))\n","\n","finalDF.show()"]},{"cell_type":"markdown","id":"0eda4cda","metadata":{"id":"0eda4cda"},"source":["**Charter 4. Use of Union and UnionByName with Data Frames**"]},{"cell_type":"markdown","id":"ad2afdca","metadata":{"id":"ad2afdca"},"source":["Crude Oil Production (Thousand Barrels)"]},{"cell_type":"code","execution_count":null,"id":"cc8c0564","metadata":{"id":"cc8c0564","outputId":"24feee0c-26b9-4e14-d536-4ab5584140ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+\n","|Year|   Jan|   Feb|   Mar|   Apr|   May|   Jun|\n","+----+------+------+------+------+------+------+\n","|2015|290891|266154|297091|289755|293711|280734|\n","|2016|285262|262902|282132|266219|273875|260284|\n","|2017|275117|255081|284146|273041|284727|273321|\n","|2018|310032|287870|324467|314996|323491|319216|\n","|2019|367924|326845|369292|364458|376763|366546|\n","+----+------+------+------+------+------+------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfCOP1: org.apache.spark.sql.DataFrame = [Year: string, Jan: string ... 5 more fields]\n"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["\n","val dfCOP1=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/Crude_Oil_Production_1.csv\")\n","\n","dfCOP1.show(5)"]},{"cell_type":"code","execution_count":null,"id":"4691dc3a","metadata":{"id":"4691dc3a","outputId":"c6b85d54-7376-4cf6-eb70-f452f3cc5f01"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+\n","|Year|   Jul|   Aug|   Sep|   Oct|   Nov|   Dec|\n","+----+------+------+------+------+------+------+\n","|2015|292807|291702|284406|291419|279982|287533|\n","|2016|268526|269386|256317|272918|267097|273288|\n","|2017|286657|286759|285499|299726|302564|309486|\n","|2018|337814|353154|343298|356767|356583|370284|\n","|2019|368965|387073|377710|397094|390010|402314|\n","+----+------+------+------+------+------+------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfCOP2: org.apache.spark.sql.DataFrame = [Year: string, Jul: string ... 5 more fields]\n"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["\n","val dfCOP2=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/Crude_Oil_Production_2.csv\")\n","\n","dfCOP2.show(5)"]},{"cell_type":"code","execution_count":null,"id":"9657eb6b","metadata":{"id":"9657eb6b","outputId":"8289d5be-9abc-473d-be4d-b1df5625c514"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|Year|   Jan|   Feb|   Mar|   Apr|   May|   Jun|   Jul|   Aug|   Sep|   Oct|   Nov|   Dec|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|2010|167529|155496|170976|161769|167427|161385|164234|168867|168473|174547|167272|173831|\n","|2011|170393|151354|174158|166858|174363|167673|168635|175618|168411|182977|181157|189487|\n","|2012|191472|181783|196138|189601|197456|188262|199368|196867|197942|216057|212472|220282|\n","|2013|219601|200383|223683|221242|227139|218355|233210|233599|235177|240600|237597|245937|\n","|2014|250430|228396|257225|255822|268025|262291|274273|276909|272623|287256|279821|296518|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","\n"]},{"data":{"text/plain":["dfCOP3: org.apache.spark.sql.DataFrame = [Year: string, Jan: string ... 11 more fields]\n"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["val dfCOP3=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/Crude_Oil_Production_3.csv\")\n","\n","dfCOP3.show(5)"]},{"cell_type":"code","execution_count":null,"id":"f16b9aae","metadata":{"id":"f16b9aae","outputId":"1b5f6d40-13d7-4cdb-8c77-64561b8bb181"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|Year|   Jan|   Feb|   Mar|   Apr|   May|   Jun|   Jul|   Aug|   Sep|   Oct|   Nov|   Dec|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|2015|290891|266154|297091|289755|293711|280734|292807|291702|284406|291419|279982|287533|\n","|2016|285262|262902|282132|266219|273875|260284|268526|269386|256317|272918|267097|273288|\n","|2017|275117|255081|284146|273041|284727|273321|286657|286759|285499|299726|302564|309486|\n","|2018|310032|287870|324467|314996|323491|319216|337814|353154|343298|356767|356583|370284|\n","|2019|367924|326845|369292|364458|376763|366546|368965|387073|377710|397094|390010|402314|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","\n"]},{"data":{"text/plain":["dfCOP4: org.apache.spark.sql.DataFrame = [Year: string, Jan: string ... 11 more fields]\n"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["val dfCOP4=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/Crude_Oil_Production_4.csv\")\n","\n","dfCOP4.show(5)"]},{"cell_type":"code","execution_count":null,"id":"f4dba080","metadata":{"id":"f4dba080"},"outputs":[],"source":["// Combining DataFrames with Duplicates and Removing Duplicates\n"]},{"cell_type":"code","execution_count":null,"id":"760b381b","metadata":{"id":"760b381b","outputId":"eba8d034-59ac-4fe8-c40e-016b8a95aa78"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|Year|   Jan|   Feb|   Mar|   Apr|   May|   Jun|   Jul|   Aug|   Sep|   Oct|   Nov|   Dec|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|2010|167529|155496|170976|161769|167427|161385|164234|168867|168473|174547|167272|173831|\n","|2011|170393|151354|174158|166858|174363|167673|168635|175618|168411|182977|181157|189487|\n","|2012|191472|181783|196138|189601|197456|188262|199368|196867|197942|216057|212472|220282|\n","|2013|219601|200383|223683|221242|227139|218355|233210|233599|235177|240600|237597|245937|\n","|2014|250430|228396|257225|255822|268025|262291|274273|276909|272623|287256|279821|296518|\n","|2015|290891|266154|297091|289755|293711|280734|292807|291702|284406|291419|279982|287533|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","\n"]},{"data":{"text/plain":["dfCOP5: org.apache.spark.sql.DataFrame = [Year: string, Jan: string ... 11 more fields]\n"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["val dfCOP5=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/Crude_Oil_Production_5.csv\")\n","\n","dfCOP5.show(10)"]},{"cell_type":"code","execution_count":null,"id":"bc058a15","metadata":{"id":"bc058a15","outputId":"db01eae7-ee0f-48a3-da99-bc688b2948e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|Year|Jan   |Feb   |Mar   |Apr   |May   |Jun   |Jul   |Aug   |Sep   |Oct   |Nov   |Dec   |\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|2019|367924|326845|369292|364458|376763|366546|368965|387073|377710|397094|390010|402314|\n","|2016|285262|262902|282132|266219|273875|260284|268526|269386|256317|272918|267097|273288|\n","|2015|290891|266154|297091|289755|293711|280734|292807|291702|284406|291419|279982|287533|\n","|2018|310032|287870|324467|314996|323491|319216|337814|353154|343298|356767|356583|370284|\n","|2017|275117|255081|284146|273041|284727|273321|286657|286759|285499|299726|302564|309486|\n","|2012|191472|181783|196138|189601|197456|188262|199368|196867|197942|216057|212472|220282|\n","|2011|170393|151354|174158|166858|174363|167673|168635|175618|168411|182977|181157|189487|\n","|2014|250430|228396|257225|255822|268025|262291|274273|276909|272623|287256|279821|296518|\n","|2010|167529|155496|170976|161769|167427|161385|164234|168867|168473|174547|167272|173831|\n","|2013|219601|200383|223683|221242|227139|218355|233210|233599|235177|240600|237597|245937|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","\n"]},{"data":{"text/plain":["cleanDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: string, Jan: string ... 11 more fields]\n"]},"execution_count":112,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["22/12/02 22:07:04 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 979524 ms exceeds timeout 120000 ms\n","22/12/02 22:07:04 WARN SparkContext: Killing executors is not supported by current scheduler.\n"]}],"source":["val cleanDf = dfCOP4.union(dfCOP5).distinct()\n","\n","cleanDf.show(false)"]},{"cell_type":"code","execution_count":null,"id":"ea52d0ef","metadata":{"id":"ea52d0ef","outputId":"85b09957-0a8b-4b8a-f164-110c7cff0b85"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|Year|   Jan|   Feb|   Mar|   Apr|   May|   Jun|   Jul|   Aug|   Sep|   Oct|   Nov|   Dec|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","|2015|290891|266154|297091|289755|293711|280734|  null|  null|  null|  null|  null|  null|\n","|2016|285262|262902|282132|266219|273875|260284|  null|  null|  null|  null|  null|  null|\n","|2017|275117|255081|284146|273041|284727|273321|  null|  null|  null|  null|  null|  null|\n","|2018|310032|287870|324467|314996|323491|319216|  null|  null|  null|  null|  null|  null|\n","|2019|367924|326845|369292|364458|376763|366546|  null|  null|  null|  null|  null|  null|\n","|2020|398420|372419|396693|357412|301105|313275|  null|  null|  null|  null|  null|  null|\n","|2015|  null|  null|  null|  null|  null|  null|292807|291702|284406|291419|279982|287533|\n","|2016|  null|  null|  null|  null|  null|  null|268526|269386|256317|272918|267097|273288|\n","|2017|  null|  null|  null|  null|  null|  null|286657|286759|285499|299726|302564|309486|\n","|2018|  null|  null|  null|  null|  null|  null|337814|353154|343298|356767|356583|370284|\n","|2019|  null|  null|  null|  null|  null|  null|368965|387073|377710|397094|390010|402314|\n","|2020|  null|  null|  null|  null|  null|  null|341184|327875|327623|324180|335867|346223|\n","+----+------+------+------+------+------+------+------+------+------+------+------+------+\n","\n"]},{"data":{"text/plain":["missingColumnsDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: string, Jan: string ... 11 more fields]\n"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["// Using allowMissingColumns=true\n","\n","val missingColumnsDf=dfCOP1.unionByName(dfCOP2, allowMissingColumns=true)\n","\n","missingColumnsDf.show()"]},{"cell_type":"markdown","id":"8f556a45","metadata":{"id":"8f556a45"},"source":["**Charter 4. Joining DataFrames with Join**"]},{"cell_type":"code","execution_count":null,"id":"d3c2d3d8","metadata":{"id":"d3c2d3d8","outputId":"583c32cd-8ef7-4b5a-a937-ff4ffeb90ca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+------+--------+-----------+------+\n","|firstName|lastName|gender|language|ISO3166Code|salary|\n","+---------+--------+------+--------+-----------+------+\n","|  Liselda|   Rojas|Female| Spanish|        484| 62000|\n","| Leopoldo|   Galán|  Male| Spanish|        604| 47000|\n","|  William|   Adams|  Male| English|        826| 99000|\n","|    James|   Allen|  Male| English|        124| 55000|\n","|   Andrea|   López|Female| Spanish|        724| 95000|\n","+---------+--------+------+--------+-----------+------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfUByL: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 4 more fields]\n"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["val dfUByL=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/User_by_language.csv\")\n","\n","dfUByL.show(5)"]},{"cell_type":"code","execution_count":null,"id":"c70e7fb0","metadata":{"id":"c70e7fb0","outputId":"9486164e-56f9-4ee8-8a5b-b1cfaf32ec56"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+--------------+\n","|ISO3166Code|   CountryName|\n","+-----------+--------------+\n","|        484|        Mexico|\n","|        826|United Kingdom|\n","|        250|        France|\n","|        124|        Canada|\n","|        724|         Spain|\n","+-----------+--------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["dfCCodes: org.apache.spark.sql.DataFrame = [ISO3166Code: string, CountryName: string]\n"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["val dfCCodes=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/ISO_3166_country_codes.csv\")\n","\n","dfCCodes.show(5)\n"]},{"cell_type":"code","execution_count":null,"id":"88991dad","metadata":{"id":"88991dad","outputId":"5850ae92-943e-43df-ebce-bbdd21a2c4ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"inner\").show()"]},{"cell_type":"code","execution_count":null,"id":"44160436","metadata":{"id":"44160436","outputId":"190b78fd-8767-448f-a805-857eb69cad92"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+------+--------+-----------+------+-----------+--------------+\n","|firstName|lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+--------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|   Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|   Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|  William|   Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|    James|   Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|   López|Female| Spanish|        724| 95000|        724|         Spain|\n","+---------+--------+------+--------+-----------+------+-----------+--------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["joinedDf: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 6 more fields]\n"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["val joinedDf=dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"inner\")\n","joinedDf.show(5)"]},{"cell_type":"code","execution_count":null,"id":"6ae20b17","metadata":{"id":"6ae20b17","outputId":"ca046b2b-ff65-458d-c4ce-98b7dfc88e05"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+--------------+\n","|firstName|lastName |gender|language|ISO3166Code|salary|CountryName   |\n","+---------+---------+------+--------+-----------+------+--------------+\n","|Liselda  |Rojas    |Female|Spanish |484        |62000 |Mexico        |\n","|Leopoldo |Galán    |Male  |Spanish |604        |47000 |Peru          |\n","|William  |Adams    |Male  |English |826        |99000 |United Kingdom|\n","|James    |Allen    |Male  |English |124        |55000 |Canada        |\n","|Andrea   |López    |Female|Spanish |724        |95000 |Spain         |\n","|Sophia   |Rochefort|Female|French  |250        |49000 |France        |\n","|Ben      |Müller   |Female|German  |276        |47000 |Germany       |\n","+---------+---------+------+--------+-----------+------+--------------+\n","\n"]}],"source":["// Create a temporary view\n","dfUByL.createOrReplaceTempView(\"UByL\")\n","dfCCodes.createOrReplaceTempView(\"CCodes\")\n","\n","// Now you can run a SQL query as you would do in a RDBMS\n","spark.sql(\"SELECT u.*, c.CountryName FROM UByL u INNER JOIN CCodes c ON u.ISO3166Code == c.ISO3166Code\").show(truncate=false)\n"]},{"cell_type":"code","execution_count":null,"id":"4fcc34b3","metadata":{"id":"4fcc34b3","outputId":"ba59a68a-b7b0-4e82-a511-bff4028473d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+------+--------+-----------+------+--------------+\n","|firstName|lastName|gender|language|ISO3166Code|salary|   CountryName|\n","+---------+--------+------+--------+-----------+------+--------------+\n","|  Liselda|   Rojas|Female| Spanish|        484| 62000|        Mexico|\n","| Leopoldo|   Galán|  Male| Spanish|        604| 47000|          Peru|\n","|  William|   Adams|  Male| English|        826| 99000|United Kingdom|\n","|    James|   Allen|  Male| English|        124| 55000|        Canada|\n","|   Andrea|   López|Female| Spanish|        724| 95000|         Spain|\n","+---------+--------+------+--------+-----------+------+--------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["cleanDf: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 5 more fields]\n"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["// Now you can run a SQL query as you would do in a RDBMS\n","\n","val cleanDf=spark.sql(\"SELECT u.*, c.CountryName FROM UByL u INNER JOIN CCodes c ON u.ISO3166Code == c.ISO3166Code\")\n","cleanDf.show(5)"]},{"cell_type":"code","execution_count":null,"id":"df92fa2d","metadata":{"id":"df92fa2d","outputId":"ff326ffc-a10b-4115-acd3-aecd35b99cd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Agnete|   Jensen|Female|  Danish|        208| 80000|       null|          null|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","|   Amelie| Hoffmann|Female|  German|         40| 45000|       null|          null|\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Fullouter, Full and Outer join\n","\n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"fullouter\").show()\n"]},{"cell_type":"code","execution_count":null,"id":"7c8670fc","metadata":{"id":"7c8670fc","outputId":"711bddcc-2ff5-4795-c310-1b4f02e972b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Agnete|   Jensen|Female|  Danish|        208| 80000|       null|          null|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","|   Amelie| Hoffmann|Female|  German|         40| 45000|       null|          null|\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Fullouter, Full and Outer join\n","\n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"full\").show()"]},{"cell_type":"code","execution_count":null,"id":"1122d1ee","metadata":{"id":"1122d1ee","outputId":"953508db-378b-40bd-8bee-5379b9ea953b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Agnete|   Jensen|Female|  Danish|        208| 80000|       null|          null|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","|   Amelie| Hoffmann|Female|  German|         40| 45000|       null|          null|\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Fullouter, Full and Outer join\n","\n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"outer\").show()\n"]},{"cell_type":"code","execution_count":null,"id":"9efb74b2","metadata":{"id":"9efb74b2","outputId":"79897cf5-ad8f-48cd-92c6-39b9dc14d719"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","|   Agnete|   Jensen|Female|  Danish|        208| 80000|       null|          null|\n","|   Amelie| Hoffmann|Female|  German|         40| 45000|       null|          null|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Leftouter Left join\n","\n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"left\").show()"]},{"cell_type":"code","execution_count":null,"id":"07322eac","metadata":{"id":"07322eac","outputId":"bd1e0334-e6c1-469e-c867-b0fdb5efadec"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","|   Agnete|   Jensen|Female|  Danish|        208| 80000|       null|          null|\n","|   Amelie| Hoffmann|Female|  German|         40| 45000|       null|          null|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"leftouter\").show()\n"]},{"cell_type":"code","execution_count":null,"id":"6d4f5aec","metadata":{"id":"6d4f5aec","outputId":"5a4e7195-36fd-4b15-cf8a-c39b05d34134"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Right or Rightouter join \n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"right\").show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"83bc8b1c","metadata":{"id":"83bc8b1c","outputId":"7f67f10d-ba88-46b1-a8da-9973f5430380"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|firstName| lastName|gender|language|ISO3166Code|salary|ISO3166Code|   CountryName|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","|  Liselda|    Rojas|Female| Spanish|        484| 62000|        484|        Mexico|\n","|  William|    Adams|  Male| English|        826| 99000|        826|United Kingdom|\n","|   Sophia|Rochefort|Female|  French|        250| 49000|        250|        France|\n","|    James|    Allen|  Male| English|        124| 55000|        124|        Canada|\n","|   Andrea|    López|Female| Spanish|        724| 95000|        724|         Spain|\n","| Leopoldo|    Galán|  Male| Spanish|        604| 47000|        604|          Peru|\n","|      Ben|   Müller|Female|  German|        276| 47000|        276|       Germany|\n","+---------+---------+------+--------+-----------+------+-----------+--------------+\n","\n"]}],"source":["// Right or Rightouter join \n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"rightouter\").show()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f8bd64cf","metadata":{"id":"f8bd64cf","outputId":"53920420-3cdb-41e6-fa24-ca05e5fd421f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------+------+--------+-----------+------+\n","|firstName|lastName|gender|language|ISO3166Code|salary|\n","+---------+--------+------+--------+-----------+------+\n","|   Agnete|  Jensen|Female|  Danish|        208| 80000|\n","|   Amelie|Hoffmann|Female|  German|         40| 45000|\n","+---------+--------+------+--------+-----------+------+\n","\n"]}],"source":["// Anti join\n","dfUByL.join(dfCCodes, dfUByL(\"ISO3166Code\") === dfCCodes(\"ISO3166Code\"), \"anti\").show()\n"]},{"cell_type":"markdown","id":"daebd2af","metadata":{"id":"daebd2af"},"source":["**Chapter 4. cache()**"]},{"cell_type":"code","execution_count":null,"id":"efdfdb9c","metadata":{"id":"efdfdb9c","outputId":"4dfc9f2d-12b2-4256-8f89-3001e2bed11f"},"outputs":[{"data":{"text/plain":["df1: org.apache.spark.sql.DataFrame = [id: bigint, square: bigint]\n"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["val dfcache = spark.range(1, 1000000000).toDF(\"base\").withColumn(\"square\", $\"base\" * $\"base\")\n"]},{"cell_type":"code","execution_count":null,"id":"a814ad80","metadata":{"id":"a814ad80","outputId":"22c09f09-0caa-4efa-f660-8aacb0da2c9a"},"outputs":[{"data":{"text/plain":["res80: df1.type = [id: bigint, square: bigint]\n"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["dfcache.cache() // Cache the data\n"]},{"cell_type":"code","execution_count":null,"id":"8753526c","metadata":{"id":"8753526c","outputId":"59c3d064-c8c2-4db9-a94e-0dfa81b031d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 57407 ms\n"]},{"data":{"text/plain":["res81: Long = 1000000000\n"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["spark.time(dfcache.count()) // Materialize the cache\n"]},{"cell_type":"code","execution_count":null,"id":"861ce56a","metadata":{"id":"861ce56a","outputId":"142c02b9-4338-41db-dd2e-dc239b85dbbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 2358 ms\n"]},{"data":{"text/plain":["res83: Long = 1000000000\n"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["spark.time(dfcache.count())"]},{"cell_type":"markdown","id":"e4b64522","metadata":{"id":"e4b64522"},"source":["**Chapter 4. persist()**"]},{"cell_type":"code","execution_count":null,"id":"21ce937e","metadata":{"id":"21ce937e","outputId":"87b05fbc-5689-44a6-8661-feb9fd6e2c4a"},"outputs":[{"data":{"text/plain":["import org.apache.spark.storage.StorageLevel\n","dfpersist: org.apache.spark.sql.DataFrame = [base: bigint, square: bigint]\n","res89: dfpersist.type = [base: bigint, square: bigint]\n"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.storage.StorageLevel\n","\n","val dfpersist = spark.range(1, 1000000000).toDF(\"base\").withColumn(\"square\", $\"base\" * $\"base\")\n","dfpersist.persist(StorageLevel.DISK_ONLY) // Data is serialized and cached it on disk"]},{"cell_type":"code","execution_count":null,"id":"4ecd953e","metadata":{"id":"4ecd953e","outputId":"2f9a0b52-dcca-4cbb-fe21-40f3b1aee9b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 54762 ms\n"]},{"data":{"text/plain":["res90: Long = 999999999\n"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["spark.time(dfpersist.count()) // Materialize the cache"]},{"cell_type":"code","execution_count":null,"id":"1d476cb8","metadata":{"id":"1d476cb8","outputId":"b5007bd2-1dfd-460d-9b5d-f825a9cd67c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 2045 ms\n"]},{"data":{"text/plain":["res91: Long = 999999999\n"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["spark.time(dfpersist.count()) // Taking advantage of cached data"]},{"cell_type":"markdown","id":"64ca6c41","metadata":{"id":"64ca6c41"},"source":["**Chapter 4. Cache tables or views derived from DataFrames**"]},{"cell_type":"code","execution_count":null,"id":"b0d47d82","metadata":{"id":"b0d47d82","outputId":"66f470c0-377b-4be4-c552-7db0b31806d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 23 ms\n","+--------+\n","|count(1)|\n","+--------+\n","| 9999999|\n","+--------+\n","\n"]},{"data":{"text/plain":["import org.apache.spark.storage.StorageLevel\n","dfWithQuery: org.apache.spark.sql.DataFrame = [base: bigint, square: bigint]\n"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.storage.StorageLevel\n","\n","val dfWithQuery = spark.range(1, 10000000).toDF(\"base\").withColumn(\"square\", $\"base\" * $\"base\")\n","\n","dfWithQuery.createOrReplaceTempView(\"TableWithQuery\")\n","spark.sql(\"CACHE TABLE TableWithQuery\")\n","spark.time(spark.sql(\"SELECT count(*) FROM TableWithQuery\")).show()"]},{"cell_type":"code","execution_count":null,"id":"bbdc7c56","metadata":{"id":"bbdc7c56","outputId":"6c22be53-7c3f-468b-d5a3-e48711620f0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 3 ms\n","+--------+\n","|count(1)|\n","+--------+\n","| 9999999|\n","+--------+\n","\n"]}],"source":["// Using already cached data\n","spark.time(spark.sql(\"SELECT count(*) FROM TableWithQuery\")).show()"]},{"cell_type":"markdown","id":"71009a11","metadata":{"id":"71009a11"},"source":["**Chapter 4. Uncache tables or views derived from DataFrames**"]},{"cell_type":"code","execution_count":null,"id":"ad2f3c22","metadata":{"id":"ad2f3c22","outputId":"2a5eec27-f752-434b-e1b5-707b3e80eda5"},"outputs":[{"data":{"text/plain":["dfUnpersist: dfWithQuery.type = [base: bigint, square: bigint]\n"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["val dfUnpersist = dfWithQuery.unpersist()"]},{"cell_type":"code","execution_count":null,"id":"34f0f4d6","metadata":{"id":"34f0f4d6","outputId":"d0cd02bf-d908-4999-88fc-a67699743080"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken: 99 ms\n"]},{"data":{"text/plain":["res2: Long = 9999999\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["22/12/07 01:37:35 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 990721 ms exceeds timeout 120000 ms\n","22/12/07 01:37:35 WARN SparkContext: Killing executors is not supported by current scheduler.\n"]}],"source":["spark.time(dfUnpersist.count())"]},{"cell_type":"code","execution_count":null,"id":"0beb3056","metadata":{"id":"0beb3056"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}