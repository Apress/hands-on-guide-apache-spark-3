{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03245d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 11:53:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/11/26 11:53:06 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"Hands-On Spark 3\") \\\n",
    "      .getOrCreate() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9d2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea160f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Hands-On Spark 3>\n",
      "Spark App Name : Hands-On Spark 3\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb1ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "currencyList = [\"USD;Euro;GBP;CHF\",\"CHF;JPY;CNY;KRW\",\"CNY;KRW;Euro;USD\",\"CAD;NZD;SEK;MXN\"]\n",
    "\n",
    "currencyListRdd = spark.sparkContext.parallelize(currencyList, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f784f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "currenciesRdd = currencyListRdd.flatMap(lambda x: x.split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027c67e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S\n",
      "E, u\n",
      "G, B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampleData = currenciesRdd.take(3)\n",
    "\n",
    "for f in sampleData:\n",
    "    print(str(f[0]) +\", \"+str(f[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "842fa391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD = currenciesRdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9d724464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(USD,1)\n",
      "(Euro,1)\n",
      "(GBP,1)\n",
      "(CHF,1)\n",
      "(CHF,1)\n"
     ]
    }
   ],
   "source": [
    "sampleData = pairRDD.take(5)\n",
    "\n",
    "for f in sampleData:\n",
    "    print(str(\"(\"+f[0]) +\",\"+str(f[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2969909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(USD,1)\n",
      "(Euro,1)\n",
      "(GBP,1)\n",
      "(CHF,1)\n",
      "(CHF,1)\n",
      "(JPY,1)\n",
      "(CNY,1)\n",
      "(KRW,1)\n",
      "(CNY,1)\n",
      "(KRW,1)\n",
      "(Euro,1)\n",
      "(USD,1)\n",
      "(CAD,1)\n",
      "(NZD,1)\n",
      "(SEK,1)\n",
      "(MXN,1)\n"
     ]
    }
   ],
   "source": [
    "sampleData = pairRDD.collect()\n",
    "\n",
    "for f in sampleData:\n",
    "    print(str(\"(\"+f[0]) +\",\"+str(f[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9b063da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CAD,1)\n",
      "(CHF,1)\n",
      "(CNY,1)\n",
      "(Euro,1)\n",
      "(GBP,1)\n",
      "(JPY,1)\n",
      "(KRW,1)\n",
      "(MXN,1)\n",
      "(NZD,1)\n",
      "(SEK,1)\n",
      "(USD,1)\n"
     ]
    }
   ],
   "source": [
    "# distinct – Returns distinct keys.\n",
    "sampleData = sorted(pairRDD.distinct().collect())\n",
    "for f in sampleData:\n",
    "    print(str(\"(\"+f[0]) +\",\"+str(f[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "93d1811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CAD,1)\n",
      "(CHF,1)\n",
      "(CHF,1)\n",
      "(CNY,1)\n",
      "(CNY,1)\n",
      "(Euro,1)\n",
      "(Euro,1)\n",
      "(GBP,1)\n",
      "(JPY,1)\n",
      "(KRW,1)\n",
      "(KRW,1)\n",
      "(MXN,1)\n",
      "(NZD,1)\n",
      "(SEK,1)\n",
      "(USD,1)\n",
      "(USD,1)\n"
     ]
    }
   ],
   "source": [
    "# sortByKey – Transformation returns an RDD after sorting by key\n",
    "# https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/\n",
    "\n",
    "\n",
    "sampleData = pairRDD.sortByKey().collect()\n",
    "for f in sampleData:\n",
    "    print(str(\"(\"+f[0]) +\",\"+str(f[1])+\")\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7965967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('CAD', 1), 1)\n",
      "(('CHF', 1), 2)\n",
      "(('CNY', 1), 2)\n",
      "(('Euro', 1), 2)\n",
      "(('GBP', 1), 1)\n",
      "(('JPY', 1), 1)\n",
      "(('KRW', 1), 2)\n",
      "(('MXN', 1), 1)\n",
      "(('NZD', 1), 1)\n",
      "(('SEK', 1), 1)\n",
      "(('USD', 1), 2)\n"
     ]
    }
   ],
   "source": [
    "# pyspark.RDD.countByValue\n",
    "#sorted(pairRDD.countByValue().items())\n",
    "\n",
    "sampleData = sorted(pairRDD.countByValue().items())\n",
    "for f in sampleData:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8fd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14e73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb6eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from parallelize    \n",
    "dataList = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(dataList, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8898184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d01f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    }
   ],
   "source": [
    "alphabetList = ['a','b','c','d','e','f','g','h','i','j','k','l']\n",
    "rdd = spark.sparkContext.parallelize(dataList, 4)\n",
    "print(\"Number of partitions: \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76a2a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    }
   ],
   "source": [
    "DonQuixote = [\"The Ingenious Gentleman Don Quixote of La Mancha\", \"El Ingenioso Hidalgo Don Quijote de La Mancha\"]\n",
    "\n",
    "DonQuixoteRdd = spark.sparkContext.parallelize(DonQuixote, 4)\n",
    "\n",
    "print(\"Number of partitions: \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4daa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairDonQuixoteRdd = DonQuixoteRdd.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee0055aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = DonQuixoteRdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "694f5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T,h,e, \n",
      "E,l, ,I\n"
     ]
    }
   ],
   "source": [
    "pairDonQuixoteRddColl=DonQuixoteRdd.collect()\n",
    "\n",
    "for i in pairDonQuixoteRddColl:\n",
    "    print(i[0] + \",\" +str(i[1])+ \",\" +str(i[2])+ \",\" +str(i[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08240373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data3 Key:T, Value:h\n",
      "data3 Key:I, Value:n\n",
      "data3 Key:G, Value:e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Action - take\n",
    "data3 = rdd2.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc7b01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DonQuixoteRdd = spark.sparkContext.textFile(\"DonQuixote.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "28d7bdec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [150]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m DonQuixoteRdd:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "for row in DonQuixoteRdd:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f39b2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "DonQuixoteRdd2 = DonQuixoteRdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9481b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "DonQuixoteRdd3 = DonQuixoteRdd2.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59f307c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key - Value\n",
      "(In,1)\n",
      "(a,1)\n",
      "(village,1)\n",
      "(of,1)\n",
      "(La,1)\n",
      "(Mancha,,1)\n"
     ]
    }
   ],
   "source": [
    "# Action - take\n",
    "DonQuixotePairedRdd = DonQuixoteRdd3.take(6)\n",
    "print(\"Key - Value\")\n",
    "for f in DonQuixotePairedRdd:\n",
    "    print(\"(\" + str(f[0]) +\",\"+ str(f[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5fc85c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In', 1)\n",
      "('a', 1)\n",
      "('village', 1)\n",
      "('La', 1)\n",
      "('Mancha,', 1)\n",
      "('the', 1)\n",
      "('which', 1)\n",
      "('I', 1)\n",
      "('desire', 1)\n",
      "('to', 1)\n",
      "('mind,', 1)\n",
      "('not', 1)\n",
      "('since', 1)\n",
      "('one', 1)\n",
      "('those', 1)\n",
      "('that', 1)\n",
      "('keep', 1)\n",
      "('lance-rack,', 1)\n",
      "('old', 1)\n",
      "('buckler,', 1)\n",
      "('lean', 1)\n",
      "('hack,', 1)\n",
      "('and', 1)\n",
      "('for', 1)\n",
      "('coursing.', 1)\n",
      "('An', 1)\n",
      "('olla', 1)\n",
      "('on', 1)\n",
      "('most', 1)\n",
      "('scraps', 1)\n",
      "('Saturdays,', 1)\n",
      "('or', 1)\n",
      "('so', 1)\n",
      "('extra', 1)\n",
      "('Sundays,', 1)\n",
      "('made', 1)\n",
      "('with', 1)\n",
      "('three-quarters', 1)\n",
      "('income.', 1)\n",
      "('it', 1)\n",
      "('went', 1)\n",
      "('cloth', 1)\n",
      "('while', 1)\n",
      "('homespun.', 1)\n",
      "('He', 1)\n",
      "('had', 1)\n",
      "('housekeeper', 1)\n",
      "('under', 1)\n",
      "('lad', 1)\n",
      "('who', 1)\n",
      "('hack', 1)\n",
      "('well', 1)\n",
      "('bill-hook.', 1)\n",
      "('age', 1)\n",
      "('ours', 1)\n",
      "('habit,', 1)\n",
      "('gaunt-featured,', 1)\n",
      "('early', 1)\n",
      "('great', 1)\n",
      "('will', 1)\n",
      "('Quixada', 1)\n",
      "('here', 1)\n",
      "('some', 1)\n",
      "('difference', 1)\n",
      "('among', 1)\n",
      "('authors', 1)\n",
      "('write', 1)\n",
      "('subject),', 1)\n",
      "('from', 1)\n",
      "('reasonable', 1)\n",
      "('seems', 1)\n",
      "('plain', 1)\n",
      "('called', 1)\n",
      "('little', 1)\n",
      "('importance', 1)\n",
      "('our', 1)\n",
      "('tale;', 1)\n",
      "('be', 1)\n",
      "('enough', 1)\n",
      "('hair’s', 1)\n",
      "('truth', 1)\n",
      "('telling', 1)\n",
      "('of', 1)\n",
      "('name', 1)\n",
      "('have', 1)\n",
      "('no', 1)\n",
      "('call', 1)\n",
      "('there', 1)\n",
      "('lived', 1)\n",
      "('long', 1)\n",
      "('gentlemen', 1)\n",
      "('lance', 1)\n",
      "('in', 1)\n",
      "('an', 1)\n",
      "('greyhound', 1)\n",
      "('rather', 1)\n",
      "('more', 1)\n",
      "('beef', 1)\n",
      "('than', 1)\n",
      "('mutton,', 1)\n",
      "('salad', 1)\n",
      "('nights,', 1)\n",
      "('lentils', 1)\n",
      "('Fridays,', 1)\n",
      "('pigeon', 1)\n",
      "('away', 1)\n",
      "('his', 1)\n",
      "('The', 1)\n",
      "('rest', 1)\n",
      "('doublet', 1)\n",
      "('fine', 1)\n",
      "('velvet', 1)\n",
      "('breeches', 1)\n",
      "('shoes', 1)\n",
      "('match', 1)\n",
      "('holidays,', 1)\n",
      "('week-days', 1)\n",
      "('he', 1)\n",
      "('brave', 1)\n",
      "('figure', 1)\n",
      "('best', 1)\n",
      "('house', 1)\n",
      "('past', 1)\n",
      "('forty,', 1)\n",
      "('niece', 1)\n",
      "('twenty,', 1)\n",
      "('field', 1)\n",
      "('market-place,', 1)\n",
      "('used', 1)\n",
      "('saddle', 1)\n",
      "('as', 1)\n",
      "('handle', 1)\n",
      "('this', 1)\n",
      "('gentleman', 1)\n",
      "('was', 1)\n",
      "('bordering', 1)\n",
      "('fifty;', 1)\n",
      "('hardy', 1)\n",
      "('spare,', 1)\n",
      "('very', 1)\n",
      "('riser', 1)\n",
      "('sportsman.', 1)\n",
      "('They', 1)\n",
      "('surname', 1)\n",
      "('Quesada', 1)\n",
      "('(for', 1)\n",
      "('is', 1)\n",
      "('opinion', 1)\n",
      "('although', 1)\n",
      "('conjectures', 1)\n",
      "('Quexana.', 1)\n",
      "('This,', 1)\n",
      "('however,', 1)\n",
      "('but', 1)\n",
      "('stray', 1)\n",
      "('breadth', 1)\n",
      "('it.', 1)\n"
     ]
    }
   ],
   "source": [
    "# Distinct keys\n",
    "DonQuixoteRdd4 = DonQuixoteRdd3.distinct().collect()\n",
    "\n",
    "for pair in DonQuixoteRdd4:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "effcaa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "name\n",
      "have\n",
      "no\n",
      "call\n",
      "there\n",
      "lived\n",
      "long\n",
      "gentlemen\n",
      "lance\n",
      "in\n",
      "an\n",
      "greyhound\n",
      "rather\n",
      "more\n",
      "beef\n",
      "than\n",
      "mutton,\n",
      "salad\n",
      "nights,\n",
      "lentils\n",
      "Fridays,\n",
      "pigeon\n",
      "away\n",
      "his\n",
      "The\n",
      "rest\n",
      "doublet\n",
      "fine\n",
      "velvet\n",
      "breeches\n",
      "shoes\n",
      "match\n",
      "holidays,\n",
      "week-days\n",
      "he\n",
      "brave\n",
      "figure\n",
      "best\n",
      "house\n",
      "past\n",
      "forty,\n",
      "niece\n",
      "twenty,\n",
      "field\n",
      "market-place,\n",
      "used\n",
      "saddle\n",
      "as\n",
      "handle\n",
      "this\n",
      "gentleman\n",
      "was\n",
      "bordering\n",
      "fifty;\n",
      "hardy\n",
      "spare,\n",
      "very\n",
      "riser\n",
      "sportsman.\n",
      "They\n",
      "surname\n",
      "Quesada\n",
      "(for\n",
      "is\n",
      "opinion\n",
      "although\n",
      "conjectures\n",
      "Quexana.\n",
      "This,\n",
      "however,\n",
      "but\n",
      "stray\n",
      "breadth\n",
      "it.\n",
      "In\n",
      "a\n",
      "village\n",
      "La\n",
      "Mancha,\n",
      "the\n",
      "which\n",
      "I\n",
      "desire\n",
      "to\n",
      "mind,\n",
      "not\n",
      "since\n",
      "one\n",
      "those\n",
      "that\n",
      "keep\n",
      "lance-rack,\n",
      "old\n",
      "buckler,\n",
      "lean\n",
      "hack,\n",
      "and\n",
      "for\n",
      "coursing.\n",
      "An\n",
      "olla\n",
      "on\n",
      "most\n",
      "scraps\n",
      "Saturdays,\n",
      "or\n",
      "so\n",
      "extra\n",
      "Sundays,\n",
      "made\n",
      "with\n",
      "three-quarters\n",
      "income.\n",
      "it\n",
      "went\n",
      "cloth\n",
      "while\n",
      "homespun.\n",
      "He\n",
      "had\n",
      "housekeeper\n",
      "under\n",
      "lad\n",
      "who\n",
      "hack\n",
      "well\n",
      "bill-hook.\n",
      "age\n",
      "ours\n",
      "habit,\n",
      "gaunt-featured,\n",
      "early\n",
      "great\n",
      "will\n",
      "Quixada\n",
      "here\n",
      "some\n",
      "difference\n",
      "among\n",
      "authors\n",
      "write\n",
      "subject),\n",
      "from\n",
      "reasonable\n",
      "seems\n",
      "plain\n",
      "called\n",
      "little\n",
      "importance\n",
      "our\n",
      "tale;\n",
      "be\n",
      "enough\n",
      "hair’s\n",
      "truth\n",
      "telling\n"
     ]
    }
   ],
   "source": [
    "# Distinct keys II\n",
    "DonQuixoteRdd4 = DonQuixoteRdd2.distinct().collect()\n",
    "\n",
    "for pair in DonQuixoteRdd4:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3884b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In', 1)\n",
      "('a', 15)\n",
      "('village', 1)\n",
      "('of', 13)\n",
      "('La', 1)\n",
      "('Mancha,', 1)\n",
      "('the', 9)\n",
      "('name', 1)\n",
      "('which', 1)\n",
      "('I', 1)\n",
      "('have', 2)\n",
      "('no', 1)\n",
      "('desire', 1)\n",
      "('to', 6)\n",
      "('call', 1)\n",
      "('mind,', 1)\n",
      "('there', 2)\n",
      "('lived', 1)\n",
      "('not', 2)\n",
      "('long', 1)\n",
      "('since', 1)\n",
      "('one', 1)\n",
      "('those', 1)\n",
      "('gentlemen', 1)\n",
      "('that', 2)\n",
      "('keep', 1)\n",
      "('lance', 1)\n",
      "('in', 5)\n",
      "('lance-rack,', 1)\n",
      "('an', 1)\n",
      "('old', 1)\n",
      "('buckler,', 1)\n",
      "('lean', 1)\n",
      "('hack,', 1)\n",
      "('and', 7)\n",
      "('greyhound', 1)\n",
      "('for', 3)\n",
      "('coursing.', 1)\n",
      "('An', 1)\n",
      "('olla', 1)\n",
      "('rather', 1)\n",
      "('more', 1)\n",
      "('beef', 1)\n",
      "('than', 1)\n",
      "('mutton,', 1)\n",
      "('salad', 1)\n",
      "('on', 7)\n",
      "('most', 1)\n",
      "('nights,', 1)\n",
      "('scraps', 1)\n",
      "('Saturdays,', 1)\n",
      "('lentils', 1)\n",
      "('Fridays,', 1)\n",
      "('pigeon', 1)\n",
      "('or', 2)\n",
      "('so', 1)\n",
      "('extra', 1)\n",
      "('Sundays,', 1)\n",
      "('made', 2)\n",
      "('away', 1)\n",
      "('with', 1)\n",
      "('three-quarters', 1)\n",
      "('his', 4)\n",
      "('income.', 1)\n",
      "('The', 2)\n",
      "('rest', 1)\n",
      "('it', 4)\n",
      "('went', 1)\n",
      "('doublet', 1)\n",
      "('fine', 1)\n",
      "('cloth', 1)\n",
      "('velvet', 1)\n",
      "('breeches', 1)\n",
      "('shoes', 1)\n",
      "('match', 1)\n",
      "('holidays,', 1)\n",
      "('while', 1)\n",
      "('week-days', 1)\n",
      "('he', 3)\n",
      "('brave', 1)\n",
      "('figure', 1)\n",
      "('best', 1)\n",
      "('homespun.', 1)\n",
      "('He', 1)\n",
      "('had', 1)\n",
      "('house', 1)\n",
      "('housekeeper', 1)\n",
      "('past', 1)\n",
      "('forty,', 1)\n",
      "('niece', 1)\n",
      "('under', 1)\n",
      "('twenty,', 1)\n",
      "('lad', 1)\n",
      "('field', 1)\n",
      "('market-place,', 1)\n",
      "('who', 2)\n",
      "('used', 1)\n",
      "('saddle', 1)\n",
      "('hack', 1)\n",
      "('as', 2)\n",
      "('well', 1)\n",
      "('handle', 1)\n",
      "('bill-hook.', 1)\n",
      "('age', 1)\n",
      "('this', 1)\n",
      "('gentleman', 1)\n",
      "('ours', 1)\n",
      "('was', 4)\n",
      "('bordering', 1)\n",
      "('fifty;', 1)\n",
      "('hardy', 1)\n",
      "('habit,', 1)\n",
      "('spare,', 1)\n",
      "('gaunt-featured,', 1)\n",
      "('very', 1)\n",
      "('early', 1)\n",
      "('riser', 1)\n",
      "('great', 1)\n",
      "('sportsman.', 1)\n",
      "('They', 1)\n",
      "('will', 2)\n",
      "('surname', 1)\n",
      "('Quixada', 1)\n",
      "('Quesada', 1)\n",
      "('(for', 1)\n",
      "('here', 1)\n",
      "('is', 2)\n",
      "('some', 1)\n",
      "('difference', 1)\n",
      "('opinion', 1)\n",
      "('among', 1)\n",
      "('authors', 1)\n",
      "('write', 1)\n",
      "('subject),', 1)\n",
      "('although', 1)\n",
      "('from', 2)\n",
      "('reasonable', 1)\n",
      "('conjectures', 1)\n",
      "('seems', 1)\n",
      "('plain', 1)\n",
      "('called', 1)\n",
      "('Quexana.', 1)\n",
      "('This,', 1)\n",
      "('however,', 1)\n",
      "('but', 1)\n",
      "('little', 1)\n",
      "('importance', 1)\n",
      "('our', 1)\n",
      "('tale;', 1)\n",
      "('be', 1)\n",
      "('enough', 1)\n",
      "('stray', 1)\n",
      "('hair’s', 1)\n",
      "('breadth', 1)\n",
      "('truth', 1)\n",
      "('telling', 1)\n",
      "('it.', 1)\n"
     ]
    }
   ],
   "source": [
    "# pyspark.RDD.countByValue\n",
    "# Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "DonQuixoteRdd5 = DonQuixoteRdd2.countByValue().items()\n",
    "\n",
    "for pair in DonQuixoteRdd5:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "17865531",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [149]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m DonQuixoteRddReduceByKey \u001b[38;5;241m=\u001b[39m DonQuixoteRdd3\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x,y: x\u001b[38;5;241m+\u001b[39my)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m DonQuixoteRddReduceByKey:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pair)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not iterable"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/25 03:00:24 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1074325 ms exceeds timeout 120000 ms\n",
      "22/09/25 03:00:24 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "DonQuixoteRddReduceByKey = DonQuixoteRdd3.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "for pair in DonQuixoteRddReduceByKey:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b79c7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 157\n"
     ]
    }
   ],
   "source": [
    "# Action - count\n",
    "print(\"Count : \"+str(DonQuixoteRddReduceByKey.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c01bc",
   "metadata": {},
   "source": [
    "Saving RDDs as text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tempfile\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "tempfile.tempdir = \"./\"\n",
    "\n",
    "RDDDonQuixote = NamedTemporaryFile(delete=True)\n",
    "\n",
    "RDDDonQuixote.close()\n",
    "\n",
    "DonQuixoteRdd3.saveAsTextFile(RDDDonQuixote.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5caa27c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('(for', 1)\\n('An', 1)\\n('Fridays,', 1)\\n('He', 1)\\n('I', 1)\\n('In', 1)\\n('La', 1)\\n('Mancha,', 1)\\n('Quesada', 1)\\n('Quexana.', 1)\\n('Quixada', 1)\\n('Saturdays,', 1)\\n('Sundays,', 1)\\n('The', 1)\\n('The', 1)\\n('They', 1)\\n('This,', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('a', 1)\\n('age', 1)\\n('although', 1)\\n('among', 1)\\n('an', 1)\\n('and', 1)\\n('and', 1)\\n('and', 1)\\n('and', 1)\\n('and', 1)\\n('and', 1)\\n('and', 1)\\n('as', 1)\\n('as', 1)\\n('authors', 1)\\n('away', 1)\\n('be', 1)\\n('beef', 1)\\n('best', 1)\\n('bill-hook.', 1)\\n('bordering', 1)\\n('brave', 1)\\n('breadth', 1)\\n('breeches', 1)\\n('buckler,', 1)\\n('but', 1)\\n('call', 1)\\n('called', 1)\\n('cloth', 1)\\n('conjectures', 1)\\n('coursing.', 1)\\n('desire', 1)\\n('difference', 1)\\n('doublet', 1)\\n('early', 1)\\n('enough', 1)\\n('extra', 1)\\n('field', 1)\\n('fifty;', 1)\\n('figure', 1)\\n('fine', 1)\\n('for', 1)\\n('for', 1)\\n('for', 1)\\n('forty,', 1)\\n('from', 1)\\n('from', 1)\\n('gaunt-featured,', 1)\\n('gentleman', 1)\\n('gentlemen', 1)\\n('great', 1)\\n('greyhound', 1)\\n('habit,', 1)\\n('hack', 1)\\n('hack,', 1)\\n('had', 1)\\n('hair’s', 1)\\n('handle', 1)\\n('hardy', 1)\\n('have', 1)\\n('have', 1)\\n('he', 1)\\n('he', 1)\\n('he', 1)\\n('here', 1)\\n('his', 1)\\n('his', 1)\\n('his', 1)\\n('his', 1)\\n('holidays,', 1)\\n('homespun.', 1)\\n('house', 1)\\n('housekeeper', 1)\\n('however,', 1)\\n('importance', 1)\\n('in', 1)\\n('in', 1)\\n('in', 1)\\n('in', 1)\\n('in', 1)\\n('income.', 1)\\n('is', 1)\\n('is', 1)\\n('it', 1)\\n('it', 1)\\n('it', 1)\\n('it', 1)\\n('it.', 1)\\n('keep', 1)\\n('lad', 1)\\n('lance', 1)\\n('lance-rack,', 1)\\n('lean', 1)\\n('lentils', 1)\\n('little', 1)\\n('lived', 1)\\n('long', 1)\\n('made', 1)\\n('made', 1)\\n('market-place,', 1)\\n('match', 1)\\n('mind,', 1)\\n('more', 1)\\n('most', 1)\\n('mutton,', 1)\\n('name', 1)\\n('niece', 1)\\n('nights,', 1)\\n('no', 1)\\n('not', 1)\\n('not', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('of', 1)\\n('old', 1)\\n('olla', 1)\\n('on', 1)\\n('on', 1)\\n('on', 1)\\n('on', 1)\\n('on', 1)\\n('on', 1)\\n('on', 1)\\n('one', 1)\\n('opinion', 1)\\n('or', 1)\\n('or', 1)\\n('our', 1)\\n('ours', 1)\\n('past', 1)\\n('pigeon', 1)\\n('plain', 1)\\n('rather', 1)\\n('reasonable', 1)\\n('rest', 1)\\n('riser', 1)\\n('saddle', 1)\\n('salad', 1)\\n('scraps', 1)\\n('seems', 1)\\n('shoes', 1)\\n('since', 1)\\n('so', 1)\\n('some', 1)\\n('spare,', 1)\\n('sportsman.', 1)\\n('stray', 1)\\n('subject),', 1)\\n('surname', 1)\\n('tale;', 1)\\n('telling', 1)\\n('than', 1)\\n('that', 1)\\n('that', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('the', 1)\\n('there', 1)\\n('there', 1)\\n('this', 1)\\n('those', 1)\\n('three-quarters', 1)\\n('to', 1)\\n('to', 1)\\n('to', 1)\\n('to', 1)\\n('to', 1)\\n('to', 1)\\n('truth', 1)\\n('twenty,', 1)\\n('under', 1)\\n('used', 1)\\n('velvet', 1)\\n('very', 1)\\n('village', 1)\\n('was', 1)\\n('was', 1)\\n('was', 1)\\n('was', 1)\\n('week-days', 1)\\n('well', 1)\\n('went', 1)\\n('which', 1)\\n('while', 1)\\n('who', 1)\\n('who', 1)\\n('will', 1)\\n('will', 1)\\n('with', 1)\\n('write', 1)\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fileinput import input\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "''.join(sorted(input(glob(RDDDonQuixote.name + \"/part-0000*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8b171840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tempfile._TemporaryFileWrapper object at 0x7f9ed1e65040>\n",
      "/Users/aantolinez/tmp906w7eoy\n"
     ]
    }
   ],
   "source": [
    "print(RDDDonQuixote)\n",
    "print(RDDDonQuixote.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1d01dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n"
     ]
    }
   ],
   "source": [
    "print(tempfile.tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f71c7346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "018ab812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 3, 5, 7]), ('b', [2, 4, 6])]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "pairs = sc.parallelize([(\"a\", 1), (\"a\", 3), (\"b\", 2), (\"a\", 5), (\"b\", 4), (\"a\", 7), (\"b\", 6)])\n",
    "\n",
    "def to_list(x):\n",
    "    return [x]\n",
    "\n",
    "def append(x, y):\n",
    "    x.append(y) # The append() method adds the y element to the x list.\n",
    "    return x\n",
    "\n",
    "def extend(x, y):\n",
    "    x.extend(y) # The extend() method adds the y list elements to the end of the x list.\n",
    "    return x\n",
    "\n",
    "\n",
    "sorted(pairs.combineByKey(to_list, append, extend).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7a9591e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "def to_list(y):\n",
    "\n",
    "    return [y]\n",
    "\n",
    "\n",
    "def append(a, b):\n",
    "\n",
    "    a.append(b)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def extend(a, b):\n",
    "\n",
    "    a.extend(b)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "sorted(x.combineByKey(to_list, append, extend).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097af902",
   "metadata": {},
   "source": [
    "JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ecb9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a62ce2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Scala', (15, 11)), ('Scala', (15, 20)), ('PySpark', (10, 75)), ('PySpark', (10, 35))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([(\"PySpark\",10),(\"Scala\",15),(\"R\",100)])\n",
    "rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "\n",
    "joinedRDD = rdd1.join(rdd2)\n",
    "\n",
    "print(joinedRDD.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a29e5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('R', (100, None)), ('Scala', (15, 11)), ('Scala', (15, 20)), ('PySpark', (10, 75)), ('PySpark', (10, 35))]\n"
     ]
    }
   ],
   "source": [
    "# ------ leftOuterJoin\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize([(\"PySpark\",10),(\"Scala\",15),(\"R\",100)])\n",
    "rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "\n",
    "joinedRDD = rdd1.leftOuterJoin(rdd2)\n",
    "\n",
    "print(joinedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58518471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Scala', (15, 11)), ('Scala', (15, 20)), ('PySpark', (10, 75)), ('PySpark', (10, 35))]\n"
     ]
    }
   ],
   "source": [
    "# ------ rightOuterJoin\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize([(\"PySpark\",10),(\"Scala\",15),(\"R\",100)])\n",
    "rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "\n",
    "joinedRDD = rdd1.rightOuterJoin(rdd2)\n",
    "\n",
    "print(joinedRDD.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a127818",
   "metadata": {},
   "source": [
    "PySpark RDD | countByKey method\n",
    "* https://skytowner.com/explore/pyspark_rdd_countbykey_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2872c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Scala', 11), ('Scala', 20), ('PySpark', 75), ('PySpark', 35)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "rdd2.collect()\n",
    "\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f96ab62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Scala': 2, 'PySpark': 2})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "491343ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Scala': 2, 'PySpark': 2})\n"
     ]
    }
   ],
   "source": [
    "# Getting the count of each group in PySpark Pair RDD\n",
    "# To group by the key, and get the count of each group:\n",
    "                   \n",
    "elementsCount = rdd2.countByKey()\n",
    "\n",
    "print(elementsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09014a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementsCount['Scala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9cf3115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to access of elementsCount that do not exist will return 0:\n",
    "elementsCount['SQL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5bf5d",
   "metadata": {},
   "source": [
    "**countByValue()**\n",
    " \n",
    "RDD.countByValue() → Dict[K, int][source]\n",
    " \n",
    "*Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41551672",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbb678e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('PySpark', 35), 1),\n",
       " (('PySpark', 75), 1),\n",
       " (('Scala', 11), 1),\n",
       " (('Scala', 20), 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd2.countByValue().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdfb18",
   "metadata": {},
   "source": [
    "**PySpark RDD | collectAsMap method**\n",
    "\n",
    "* RDD's collectAsMap(~) method collects all the elements of a pair RDD in the driver node and converts the RDD into a dictionary.\n",
    " \n",
    "* NOTE\n",
    "\n",
    "* A pair RDD is a RDD that contains a list of tuples.\n",
    "* Parameters. This method does not take in any parameters.\n",
    "* Return Value. A dictionary.\n",
    " \n",
    "Example. Consider the following PySpark pair RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6359f8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PySpark': 10, 'Scala': 15, 'R': 100}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd = sc.parallelize([(\"a\",5),(\"b\",2),(\"c\",3)]) \n",
    "# rdd.collect()\n",
    "# https://skytowner.com/explore/pyspark_rdd_collectasmap_method\n",
    "\n",
    "# rdd1 = spark.sparkContext.parallelize([(\"PySpark\",10),(\"Scala\",15),(\"R\",100)])\n",
    "# rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "\n",
    "rdd1.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92db67",
   "metadata": {},
   "source": [
    "In case of duplicate keys\n",
    "\n",
    "When we have duplicate keys, the latter key-value pair will overwrite the former ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d852da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scala': 20, 'PySpark': 35}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, the tuple (\"Scala\",20) has overwritten (\"Scala\",11).\n",
    "rdd2.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a251bcf",
   "metadata": {},
   "source": [
    "PySpark RDD – lookup()\n",
    "\n",
    "lookup() is an action in pair RDD, which is used to return all the values that are associated with a key in a list. It is performed on single pair RDD. It takes a key as a parameter.\n",
    "\n",
    "Syntax:\n",
    "RDD_data.lookup(key)\n",
    "* https://linuxhint.com/pyspark-rdd-lookup-collectasmap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b81eb43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 35]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd2 = spark.sparkContext.parallelize([(\"Scala\",11),(\"Scala\",20),(\"PySpark\",75), (\"PySpark\",35)])\n",
    "\n",
    "rdd2.lookup(\"PySpark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe78c0",
   "metadata": {},
   "source": [
    "**Broadcast Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8592a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bVariable =  spark.sparkContext.broadcast([1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e65eacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bVariable.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c2442",
   "metadata": {},
   "source": [
    "**Accumulators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ceadf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc value:  15\n"
     ]
    }
   ],
   "source": [
    "acc = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:acc.add(x))\n",
    "print(\"Acc value: \", acc.value) #Accessed by driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f7b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51eb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c58c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23dc6ab7",
   "metadata": {},
   "source": [
    "**DataFrames. Generic Load/Save Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786c36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 18:00:06 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n",
      "\t... 14 more\n",
      "22/10/29 18:00:06 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.47.224 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:527)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:168)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m personasDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/aantolinez/Downloads/personas.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/Applications/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Applications/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Applications/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.47.224 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:527)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:168)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/Users/aantolinez/Downloads/personas.csv; isDirectory=false; length=200; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: java.lang.RuntimeException: file:/Users/aantolinez/Downloads/personas.csv is not a Parquet file. Expected magic number at tail, but found [48, 44, 70, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "personasDF = spark.read.load(\"/Users/aantolinez/Downloads/personas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d604ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------------+----+----+\n",
      "|Nombre|Primer_Apellido|Segundo_Apellido|Edad|Sexo|\n",
      "+------+---------------+----------------+----+----+\n",
      "|Miguel|   de Cervantes|        Saavedra|  50|   M|\n",
      "+------+---------------+----------------+----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personasDF = spark.read.load(\"/Users/aantolinez/Downloads/personas.parquet\")\n",
    "personasDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b771b",
   "metadata": {},
   "source": [
    "**Querying Files Using SQL language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "219e3aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+----+----+\n",
      "|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n",
      "+--------+-----------------+-------------------+----+----+\n",
      "|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n",
      "|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n",
      "|    Luis|       de Góngora|           y Argote|  65|   M|\n",
      "|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n",
      "+--------+-----------------+-------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`/Users/aantolinez/Downloads/personas.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4b84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55502c6b",
   "metadata": {},
   "source": [
    "**Modification Time Path Filters**\n",
    "* https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#recursive-file-lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdb7812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+----+----+\n",
      "|  Nombre|  Primer_Apellido|   Segundo_Apellido|Edad|Sexo|\n",
      "+--------+-----------------+-------------------+----+----+\n",
      "|  Miguel|     de Cervantes|           Saavedra|  50|   M|\n",
      "|Fancisco|          Quevedo|Santibáñez Villegas|  55|   M|\n",
      "|    Luis|       de Góngora|           y Argote|  65|   M|\n",
      "|  Teresa|Sánchez de Cepeda|          y Ahumada|  70|   F|\n",
      "+--------+-----------------+-------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modifiedAfterDF = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"modifiedAfter\", \"2022-10-30T05:30:00\") \\\n",
    "  .load(\"/Users/aantolinez/Downloads/Hands-On-Spark3\");\n",
    "\n",
    "modifiedAfterDF.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2664ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, StructType, IntegerType\n",
    "\n",
    "schemaWriters = StructType([ \n",
    "    StructField(\"Name\",StringType(),True), \n",
    "    StructField(\"Surname\",StringType(),True), \n",
    "    StructField(\"Century\",StringType(),True), \n",
    "    StructField(\"YearOfBirth\", IntegerType(), True) \n",
    "  ])\n",
    "\n",
    "SpanishWritersDf = spark.read.option(\"header\", \"true\") \\\n",
    "    .schema(schemaWriters) \\\n",
    "    .csv(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0651376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------+-----------+\n",
      "|     Name|        Surname|Century|YearOfBirth|\n",
      "+---------+---------------+-------+-----------+\n",
      "|  Gonzalo|      de Berceo|   XIII|       1196|\n",
      "|    Juan |           Ruiz|    XIV|       1283|\n",
      "| Fernando|       de Rojas|     XV|       1465|\n",
      "|Garcilaso|     de la Vega|    XVI|       1539|\n",
      "|   Miguel|   de Cervantes|    XVI|       1547|\n",
      "|Francisco|     de Quevedo|    XVI|       1580|\n",
      "|     Luis|     de Góngora|    XVI|       1561|\n",
      "|     Lope|        de Vega|    XVI|       1562|\n",
      "|    Tirso|      de Molina|    XVI|       1583|\n",
      "| Calderón|    de la Barca|   XVII|       1600|\n",
      "|   Adolfo|        Bécquer|    XIX|       1836|\n",
      "|   Benito|   Pérez Galdós|    XIX|       1843|\n",
      "|   Emilia|    Pardo Bazán|    XIX|       1851|\n",
      "|     José|Ortega y Gasset|     XX|       1883|\n",
      "+---------+---------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SpanishWritersDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0167382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- Century: string (nullable = true)\n",
      " |-- YearOfBirth: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SpanishWritersDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b823afb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Saving data with default compression codec: snappy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mSpanishWritersDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Applications/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet already exists."
     ]
    }
   ],
   "source": [
    "# Saving data with default compression codec: snappy\n",
    "SpanishWritersDf.write.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data with gzip compression codec\n",
    "SpanishWritersDf.write.mode(\"append\").option(\"compression\", \"gzip\").parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78aa5830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-----------+\n",
      "|    Name|    Surname|Century|YearOfBirth|\n",
      "+--------+-----------+-------+-----------+\n",
      "|Calderón|de la Barca|   XVII|       1600|\n",
      "+--------+-----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF = spark.read.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Century.parquet\")\n",
    "\n",
    "parquetDF.createOrReplaceTempView(\"TempTable\")\n",
    "\n",
    "sqlDf = spark.sql(\"select * from TempTable where YearOfBirth = 1600\")\n",
    "\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20280374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6f1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92896887",
   "metadata": {},
   "source": [
    "**Parquet File Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d11da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, StructType, IntegerType\n",
    "\n",
    "schemaWriters = StructType([ \n",
    "    StructField(\"Name\",StringType(),True), \n",
    "    StructField(\"Surname\",StringType(),True), \n",
    "    StructField(\"Century\",StringType(),True), \n",
    "    StructField(\"YearOfBirth\", IntegerType(),True),\n",
    "    StructField(\"Gender\",StringType(),True),\n",
    "  ])\n",
    "\n",
    "SpanishWritersDf = spark.read.option(\"header\", \"true\") \\\n",
    "    .schema(schemaWriters) \\\n",
    "    .csv(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df3444ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------+-----------+------+\n",
      "|Name     |Surname     |Century|YearOfBirth|Gender|\n",
      "+---------+------------+-------+-----------+------+\n",
      "|Gonzalo  |de Berceo   |XIII   |1196       |M     |\n",
      "|Juan     |Ruiz        |XIV    |1283       |M     |\n",
      "|Fernando |de Rojas    |XV     |1465       |M     |\n",
      "|Garcilaso|de la Vega  |XVI    |1539       |M     |\n",
      "|Miguel   |de Cervantes|XVI    |1547       |M     |\n",
      "+---------+------------+-------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SpanishWritersDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "108a7e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- Century: string (nullable = true)\n",
      " |-- YearOfBirth: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SpanishWritersDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10199bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SpanishWritersDf.write.partitionBy(\"Century\",\"Gender\") \\\n",
    "    .parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95205c14",
   "metadata": {},
   "source": [
    "**Reading Parquet File Partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81dece2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----------+------+\n",
      "|Name|        Surname|YearOfBirth|Gender|\n",
      "+----+---------------+-----------+------+\n",
      "|José|Ortega y Gasset|       1883|     M|\n",
      "+----+---------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partitionDF = spark.read.parquet(\"/Users/aantolinez/Downloads/Spanish_Writers_by_Gender.parquet/Century=XX\")\n",
    "\n",
    "partitionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4027e60",
   "metadata": {},
   "source": [
    "**Spark JSON Read and Write operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473aa9b6",
   "metadata": {},
   "source": [
    "*Spark Read JSON File into DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458ebd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDf = spark.read.json(\"/Users/aantolinez/Downloads/Spaniards.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea69640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- registered: boolean (nullable = true)\n",
      " |-- updated: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01959693",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilineJsonDf = spark.read.option(\"multiline\",\"true\") \\\n",
    ".json(\"/Users/aantolinez/Downloads/Spaniards_array.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55001255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|country|              email|first_name| id|last_name|registered|   updated|\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|  Spain| luis.ortiz@mapy.cz|      Luis|  1|    Ortiz|     false|2015-05-16|\n",
      "|  Spain| aantolinez@optc.es|   Alfonso|  2|Antolinez|      true|2015-03-11|\n",
      "|  Spain|     jdomin@xyz.org|      Juan|  3|Dominguez|      true|2015-02-15|\n",
      "|  Spain|ssanchez@google.com|  Santiago|  4|  Sanchez|     false|2014-10-31|\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multilineJsonDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762f49b",
   "metadata": {},
   "source": [
    "**Reading Multiple JSON Files at Once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257359ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a multiline JSON strings file into a dataframe at once\n",
    "\n",
    "multipleJsonsDf = spark.read.option(\"multiline\",\"true\") \\\n",
    ".json([\"/Users/aantolinez/Downloads/Spaniards_array.json\", \\\n",
    "      \"/Users/aantolinez/Downloads/Spaniards_array2.json\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39565c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|country|email              |first_name|id |last_name|registered|updated   |\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n",
      "|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n",
      "|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n",
      "|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n",
      "|Spain  |luis.herrera@xyz.es|Luis      |1  |Herrera  |false     |2015-05-15|\n",
      "|Spain  |mabad@opti.es      |Marcos    |2  |Abad     |true      |2015-03-21|\n",
      "|Spain  |jabalos@redis.org  |Juan      |3  |Abalos   |true      |2015-02-14|\n",
      "|Spain  |samo@terra.es      |Santiago  |4  |Amo      |false     |2014-10-21|\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multipleJsonsDf.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aaf91",
   "metadata": {},
   "source": [
    "**Reading JSON Files Based On Patterns At Once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf285c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n",
    "    \"/Users/aantolinez/Downloads/Spaniards_array*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d5231e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+----------+---+---------+----------+----------+\n",
      "|country|email                    |first_name|id |last_name|registered|updated   |\n",
      "+-------+-------------------------+----------+---+---------+----------+----------+\n",
      "|Spain  |luis.garcia@xyz.es       |Lucia     |9  |Garcia   |true      |2015-05-15|\n",
      "|Spain  |maria.rodriguez@opti.es  |Maria     |10 |Rodriguez|true      |2015-03-21|\n",
      "|Spain  |carmen.gonzalez@redis.org|Carmen    |11 |Gonzalez |true      |2015-02-14|\n",
      "|Spain  |sara.fernandez@terra.es  |Sara      |12 |Fernandez|true      |2014-10-21|\n",
      "|Spain  |luis.ortiz@mapy.cz       |Luis      |1  |Ortiz    |false     |2015-05-16|\n",
      "|Spain  |aantolinez@optc.es       |Alfonso   |2  |Antolinez|true      |2015-03-11|\n",
      "|Spain  |jdomin@xyz.org           |Juan      |3  |Dominguez|true      |2015-02-15|\n",
      "|Spain  |ssanchez@google.com      |Santiago  |4  |Sanchez  |false     |2014-10-31|\n",
      "|Spain  |luis.herrera@xyz.es      |Luis      |1  |Herrera  |false     |2015-05-15|\n",
      "|Spain  |mabad@opti.es            |Marcos    |2  |Abad     |true      |2015-03-21|\n",
      "|Spain  |jabalos@redis.org        |Juan      |3  |Abalos   |true      |2015-02-14|\n",
      "|Spain  |samo@terra.es            |Santiago  |4  |Amo      |false     |2014-10-21|\n",
      "+-------+-------------------------+----------+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patternJsonsDf.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the JSON files from a directory and only JSON files.\n",
    "\n",
    "patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n",
    "    \"/Users/aantolinez/Downloads/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea339bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading ALL the files from a directory.\n",
    "\n",
    "patternJsonsDf = spark.read.option(\"multiline\",\"true\").json(\n",
    "    \"/Users/aantolinez/Downloads/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46a0d2",
   "metadata": {},
   "source": [
    "**Direct Queries on Parquet Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d7f2094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode USING json OPTIONS\" + \" (path 'resources/zipcodes.json')\")\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW Spaniards USING json OPTIONS\" + \n",
    "      \" (path '/Users/aantolinez/Downloads/Spaniards.json')\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c797ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|country|email              |first_name|id |last_name|registered|updated   |\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "|Spain  |luis.ortiz@mapy.cz |Luis      |1  |Ortiz    |false     |2015-05-16|\n",
      "|Spain  |aantolinez@optc.es |Alfonso   |2  |Antolinez|true      |2015-03-11|\n",
      "|Spain  |jdomin@xyz.org     |Juan      |3  |Dominguez|true      |2015-02-15|\n",
      "|Spain  |ssanchez@google.com|Santiago  |4  |Sanchez  |false     |2014-10-31|\n",
      "+-------+-------------------+----------+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select * from Spaniards\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aab281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d2bfa5",
   "metadata": {},
   "source": [
    "**Saving a DataFrame To JSON File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleJsonsDf.write\n",
    " .json(\"/Users/aantolinez/Downloads/Merged_Spaniards_array.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00f3e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleJsonsDf.write.mode(\"append\").json(\"/Users/aantolinez/Downloads/Merged_Spaniards_array.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cf291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "305c376f",
   "metadata": {},
   "source": [
    "**Load JSON Files Based On Customized Schemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad314094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,BooleanType,DateType\n",
    "\n",
    "schemaSpaniards = StructType([ \\\n",
    "                              StructField(\"id\",IntegerType(),nullable=True), \\\n",
    "                              StructField(\"first_name\",StringType(),nullable=True), \\\n",
    "                              StructField(\"last_name\",StringType(),nullable=True), \\\n",
    "                              StructField(\"email\",StringType(),nullable=True), \\\n",
    "                              StructField(\"country\",StringType(),nullable=True), \\\n",
    "                              StructField(\"updated\",DateType(),nullable=True), \\\n",
    "                              StructField(\"registered\",BooleanType(),nullable=True) \\\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11227380",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaSpaniardsDf = spark.read.schema(schemaSpaniards).json(\"/Users/aantolinez/Downloads/Spaniards.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85ff8a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- updated: date (nullable = true)\n",
      " |-- registered: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaSpaniardsDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4116e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-------------------+-------+----------+----------+\n",
      "|id |first_name|last_name|email              |country|updated   |registered|\n",
      "+---+----------+---------+-------------------+-------+----------+----------+\n",
      "|1  |Luis      |Ortiz    |luis.ortiz@mapy.cz |Spain  |2015-05-16|false     |\n",
      "|2  |Alfonso   |Antolinez|aantolinez@optc.es |Spain  |2015-03-11|true      |\n",
      "|3  |Juan      |Dominguez|jdomin@xyz.org     |Spain  |2015-02-15|true      |\n",
      "|4  |Santiago  |Sanchez  |ssanchez@google.com|Spain  |2014-10-31|false     |\n",
      "+---+----------+---------+-------------------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaSpaniardsDf.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c2cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31f8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86064a9",
   "metadata": {},
   "source": [
    "**Chapter 4. Read and Write CSV Files with Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e123f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|Name;Surname;Cent...|\n",
      "|Gonzalo;de Berceo...|\n",
      "| Juan ;Ruiz;XIV;1283|\n",
      "|Fernando;de Rojas...|\n",
      "|Garcilaso;de la V...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The default delimiter is \",\" for CSV files\n",
    "PATH =\"Downloads/Spanish_Writers_by_Century_II.csv\"\n",
    "\n",
    "df0 = spark.read.csv(PATH)\n",
    "df0.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42692696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97695afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac2b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edcee327",
   "metadata": {},
   "source": [
    "**Chapter 4. Drop DataFrame column(s)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e7bb4",
   "metadata": {},
   "source": [
    "* Deleting a single column: dataframe.drop(\"column name\")\n",
    "* Deleting multiple columns: dataframe.drop(*(\"column 1\",\"column 2\",\"column n\"))\n",
    "* Deleting all columns of dataframe: dataframe.drop(*list_of_column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413cba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWC=spark.read.option(\"header\", \"true\").csv(\"file:///Users/aantolinez/Downloads/WorldCups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9aef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n",
      "|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n",
      "|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13|           18|   590.549|\n",
      "|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16|           17|   363.000|\n",
      "|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15|           18|   375.700|\n",
      "|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|           22| 1.045.246|\n",
      "|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16|           26|   768.607|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The show() function without parameters displays 20 rows and truncates the text length to 20 characters by default.\n",
    "dfWC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a3b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n",
      "|Year|Country| Winner|Runners-Up| Third|Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|Attendance|\n",
      "+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n",
      "|1950| Brazil|Uruguay|    Brazil|Sweden| Spain|         88|            13|           22| 1.045.246|\n",
      "+----+-------+-------+----------+------+------+-----------+--------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dfWC.select(\"*\").filter(F.col(\"Attendance\") == \"1.045.246\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6ec04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Attendance|\n",
      "+----------+\n",
      "|    590549|\n",
      "|    363000|\n",
      "|    375700|\n",
      "|   1045246|\n",
      "|    768607|\n",
      "|    819810|\n",
      "|    893172|\n",
      "|   1563135|\n",
      "|   1603975|\n",
      "|   1865753|\n",
      "|   1545791|\n",
      "|   2109723|\n",
      "|   2394031|\n",
      "|   2516215|\n",
      "|   3587538|\n",
      "|   2785100|\n",
      "|   2705197|\n",
      "|   3359439|\n",
      "|   3178856|\n",
      "|   3386810|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying transformations to the data in the Attendance column to transform it into numeric (I)\n",
    "\n",
    "dfWC2=dfWC.select(F.regexp_replace(\"Attendance\", \"\\\\.\", \"\").alias(\"Attendance\"))\n",
    "dfWC2.select(\"Attendance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e5b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Attendance|\n",
      "+----------+\n",
      "|    590549|\n",
      "|    363000|\n",
      "|    375700|\n",
      "|   1045246|\n",
      "|    768607|\n",
      "|    819810|\n",
      "|    893172|\n",
      "|   1563135|\n",
      "|   1603975|\n",
      "|   1865753|\n",
      "|   1545791|\n",
      "|   2109723|\n",
      "|   2394031|\n",
      "|   2516215|\n",
      "|   3587538|\n",
      "|   2785100|\n",
      "|   2705197|\n",
      "|   3359439|\n",
      "|   3178856|\n",
      "|   3386810|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying transformations to the data in the Attendance column to transform it into numeric (II)\n",
    "\n",
    "# IMPORTANT\n",
    "# Do not mix solutions (II) with (I) otherwise next won't work\n",
    "\n",
    "dfWC2=dfWC.withColumn(\"Attendance\", F.regexp_replace(\"Attendance\", \"\\\\.\", \"\"))\n",
    "dfWC2.select(\"Attendance\").show()\n",
    "\n",
    "\n",
    "# Use an alias to assign a new name to the returned column\n",
    "#df.select(F.regexp_replace('name', 'le', 'LE').alias('new_name')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed59ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Winner: string (nullable = true)\n",
      " |-- Runners-Up: string (nullable = true)\n",
      " |-- Third: string (nullable = true)\n",
      " |-- Fourth: string (nullable = true)\n",
      " |-- GoalsScored: integer (nullable = true)\n",
      " |-- QualifiedTeams: integer (nullable = true)\n",
      " |-- MatchesPlayed: integer (nullable = true)\n",
      " |-- Attendance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ones the data format is ready, we can transform it to numeric, integer in this case\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "dfWC3=dfWC2.withColumn(\"GoalsScored\", F.col(\"GoalsScored\").cast(IntegerType())) \\\n",
    ".withColumn(\"QualifiedTeams\", F.col(\"QualifiedTeams\").cast(IntegerType())) \\\n",
    ".withColumn(\"MatchesPlayed\", F.col(\"MatchesPlayed\").cast(IntegerType())) \\\n",
    ".withColumn(\"Attendance\", F.col(\"Attendance\").cast(IntegerType()))\n",
    "\n",
    "dfWC3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a0773c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+\n",
      "|Attendance|MatchesPlayed|AttendancePerMatch|\n",
      "+----------+-------------+------------------+\n",
      "|    590549|           18|         32808.278|\n",
      "|    363000|           17|         21352.941|\n",
      "|    375700|           18|         20872.222|\n",
      "|   1045246|           22|         47511.182|\n",
      "|    768607|           26|         29561.808|\n",
      "+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding a new column to the DataFrame fans attendance per match played\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "dfWCExt=dfWC3.withColumn(\"AttendancePerMatch\", round(F.col(\"Attendance\").cast(IntegerType())/F.col(\"MatchesPlayed\").cast(IntegerType()), 3))\n",
    "\n",
    "dfWCExt.select(\"Attendance\",\"MatchesPlayed\",\"AttendancePerMatch\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba441f",
   "metadata": {},
   "source": [
    "**Chapter 4. Renaming DataFrame Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4908c3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+\n",
      "|Attendance|MatchesPlayed|  AxMatch|\n",
      "+----------+-------------+---------+\n",
      "|    590549|           18|32808.278|\n",
      "|    363000|           17|21352.941|\n",
      "|    375700|           18|20872.222|\n",
      "|   1045246|           22|47511.182|\n",
      "|    768607|           26|29561.808|\n",
      "+----------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRenamed = dfWCExt.withColumnRenamed(\"AttendancePerMatch\",\"AxMatch\")\n",
    "dfRenamed.select(\"Attendance\", \"MatchesPlayed\",\"AxMatch\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44a51fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------------+\n",
      "|    Att|MatchesPlayed|AttendancexMatch|\n",
      "+-------+-------------+----------------+\n",
      "| 590549|           18|       32808.278|\n",
      "| 363000|           17|       21352.941|\n",
      "| 375700|           18|       20872.222|\n",
      "|1045246|           22|       47511.182|\n",
      "| 768607|           26|       29561.808|\n",
      "+-------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming several columns at onces joining calls to withColumnRenamed()\n",
    "\n",
    "dfRenamed2 = dfRenamed \\\n",
    ".withColumnRenamed(\"Attendance\",\"Att\") \\\n",
    ".withColumnRenamed(\"AxMatch\",\"AttendancexMatch\")\n",
    "\n",
    "dfRenamed2.select(\"Att\", \"MatchesPlayed\",\"AttendancexMatch\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82d4f8",
   "metadata": {},
   "source": [
    "**Chapter 4. Drop DataFrame column(s)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca600616",
   "metadata": {},
   "source": [
    "* Deleting a single column: dataframe.drop(\"column name\")\n",
    "* Deleting multiple columns: dataframe.drop(*(\"column 1\",\"column 2\",\"column n\"))\n",
    "* Deleting all columns of dataframe: dataframe.drop(*list_of_column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8351dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n",
      "|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|MatchesPlayed|    Att|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n",
      "|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13|           18| 590549|\n",
      "|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16|           17| 363000|\n",
      "|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15|           18| 375700|\n",
      "|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|           22|1045246|\n",
      "|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16|           26| 768607|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFropOne = dfRenamed2.drop(\"AttendancexMatch\")\n",
    "dfFropOne.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33f02389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------+\n",
      "|Year|    Country|    Winner|    Runners-Up|  Third|    Fourth|GoalsScored|QualifiedTeams|    Att|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------+\n",
      "|1930|    Uruguay|   Uruguay|     Argentina|    USA|Yugoslavia|         70|            13| 590549|\n",
      "|1934|      Italy|     Italy|Czechoslovakia|Germany|   Austria|         70|            16| 363000|\n",
      "|1938|     France|     Italy|       Hungary| Brazil|    Sweden|         84|            15| 375700|\n",
      "|1950|     Brazil|   Uruguay|        Brazil| Sweden|     Spain|         88|            13|1045246|\n",
      "|1954|Switzerland|Germany FR|       Hungary|Austria|   Uruguay|        140|            16| 768607|\n",
      "+----+-----------+----------+--------------+-------+----------+-----------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFropTwo = dfRenamed2.drop(*(\"MatchesPlayed\",\"AttendancexMatch\"))\n",
    "dfFropTwo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4b7a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allColumnsList = dfRenamed2.columns\n",
    "dfFropAll = dfRenamed2.drop(*allColumnsList)\n",
    "dfFropAll.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e53ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c5943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f1fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e96c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725adc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65770411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------- CUIDADO ---------------\n",
    "# Este ejercicio usa dataframe, no RDD\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f238b0",
   "metadata": {},
   "source": [
    "3. PySpark Inner Join DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94085d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639432f",
   "metadata": {},
   "source": [
    "4. PySpark Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45558fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93b0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1e2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e1d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d082b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cc75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
