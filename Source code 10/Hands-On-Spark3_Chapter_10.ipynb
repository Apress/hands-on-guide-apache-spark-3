{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0609c1",
   "metadata": {},
   "source": [
    "#### Spark ML + Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType,LongType}\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, VectorAssembler, MinMaxScaler, StringIndexer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val schema = new StructType()\n",
    "      .add(\"age\",LongType,true)\n",
    "      .add(\"sex\",LongType,true)\n",
    "      .add(\"cp\",LongType,true)\n",
    "      .add(\"trtbps\",LongType,true)\n",
    "      .add(\"chol\",LongType,true)\n",
    "      .add(\"fbs\",LongType,true)\n",
    "      .add(\"restecg\",LongType,true)\n",
    "      .add(\"thalachh\",LongType,true)\n",
    "      .add(\"exng\",LongType,true)\n",
    "      .add(\"oldpeak\",LongType,true)\n",
    "      .add(\"slp\",LongType,true)\n",
    "      .add(\"caa\",LongType,true)\n",
    "      .add(\"thall\",LongType,true)\n",
    "      .add(\"output\",LongType,true)\n",
    "      \n",
    "val heartdF = spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .load(\"file:///tmp/spark_ml\")\n",
    "      .withColumnRenamed(\"output\",\"label\")\n",
    "\n",
    "println(heartdF.count)\n",
    "heartdF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d29660",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartdF.filter(\"oldpeak is null\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainDF, testDF) = heartdF.randomSplit(weights=Array(.8, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c050ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "\n",
    "val oneHotEnc = new OneHotEncoder()\n",
    ".setInputCols(Array(\"sex\", \"cp\", \"fbs\", \"restecg\", \"exng\", \"slp\", \"caa\",\"thall\"))\n",
    ".setOutputCols(Array(\"SexOHE\", \"cpOHE\", \"fbsOHE\", \"restecgOHE\", \"exngOHE\", \"slpOHE\", \"caaOHE\",\"thallOHE\"))\n",
    "\n",
    "val assemblerA = new VectorAssembler()\n",
    "  .setInputCols(Array(\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"))\n",
    "  .setOutputCol(\"features_scaled1\")\n",
    "  .setHandleInvalid(\"skip\")\n",
    " \n",
    "val scaler = new MinMaxScaler()\n",
    "  .setInputCol(\"features_scaled1\")\n",
    "  .setOutputCol(\"features_scaled\")\n",
    " \n",
    "val assemblerB = new VectorAssembler()\n",
    "  .setInputCols(Array(\"SexOHE\", \"cpOHE\", \"fbsOHE\", \"restecgOHE\", \"exngOHE\", \"slpOHE\", \"caaOHE\",\"thallOHE\", \"features_scaled\"))\n",
    "  .setOutputCol(\"features\")\n",
    "  .setHandleInvalid(\"skip\")\n",
    " \n",
    "val modelStages = Array(assemblerA, scaler, oneHotEnc, assemblerB, lr)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(modelStages)\n",
    "\n",
    "val PipelineModel = pipeline.fit(trainDF)\n",
    " \n",
    "val trainingPred = PipelineModel.transform(trainDF)\n",
    "\n",
    "trainingPred.select(\"label\",\"probability\",\"prediction\").show(truncate=false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91399de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.repartition(10)\n",
    "    .write.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"file:///tmp/spark_ml_streaming/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8889d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val streamingSource=spark\n",
    "    .readStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",true)\n",
    "    .schema(schema)\n",
    "    .option(\"ignoreLeadingWhiteSpace\",true)\n",
    "    .option(\"mode\",\"dropMalformed\")\n",
    "    .option(\"maxFilesPerTrigger\",1)\n",
    "    .load(\"file:///tmp/HeartTest/\")\n",
    "    .withColumnRenamed(\"output\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b621804",
   "metadata": {},
   "outputs": [],
   "source": [
    "val streamingHeart = PipelineModel.transform(streamingSource).select(\"label\",\"probability\",\"prediction\")\n",
    "\n",
    "streamingHeart.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"truncate\", false)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76662f64",
   "metadata": {},
   "source": [
    "#### Model validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad403dad",
   "metadata": {},
   "source": [
    "Calculate the true positive and true negative rates (sensitivity and specificity of the model respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{count, sum, when}\n",
    "\n",
    "val streamingRates = PipelineModel.transform(streamingSource)\n",
    "    .groupBy('label)\n",
    "    .agg(\n",
    "        (sum(when('prediction === 'label, 1)) / count('label)).alias(\"true prediction rate\"),\n",
    "        count('label).alias(\"count\")\n",
    "        )\n",
    "\n",
    "streamingRates.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"truncate\", false)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
